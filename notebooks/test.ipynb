{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bbb63e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tail8: b'\\xaa\\x05\\x00\\x00PAR1' footer_len: 1450 magic: b'PAR1'\n",
      "footer_start_offset: 287650\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import struct\n",
    "\n",
    "p = Path(\"/mnt/d/community-detection/data/processed/_runs/20251214_183903/lbsn2vec/edges_final.parquet\")\n",
    "\n",
    "with p.open(\"rb\") as f:\n",
    "    f.seek(-8, 2)\n",
    "    tail8 = f.read(8)\n",
    "\n",
    "footer_len = struct.unpack(\"<I\", tail8[:4])[0]\n",
    "magic = tail8[4:]\n",
    "print(\"tail8:\", tail8, \"footer_len:\", footer_len, \"magic:\", magic)\n",
    "print(\"footer_start_offset:\", p.stat().st_size - 8 - footer_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6cfc025e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK 54084 1\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "\n",
    "p = Path(\"/mnt/d/community-detection/data/processed/_runs/20251214_165847/lbsn2vec/edges_final.parquet\")\n",
    "try:\n",
    "    pf = pq.ParquetFile(p)\n",
    "    print(\"OK\", pf.metadata.num_rows, pf.metadata.num_row_groups)\n",
    "except Exception as e:\n",
    "    print(\"FAIL:\", type(e).__name__, e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ca825695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "READ FULL FAIL: OSError Corrupt snappy compressed data.\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "\n",
    "p = Path(\"/mnt/d/community-detection/data/processed/_runs/20251214_165847/lbsn2vec/edges_final.parquet\")\n",
    "\n",
    "try:\n",
    "    t = pq.read_table(p)\n",
    "    df = t.to_pandas()\n",
    "    print(\"READ FULL OK:\", df.shape, \"cols:\", df.columns.tolist())\n",
    "    print(df.head(3))\n",
    "except Exception as e:\n",
    "    print(\"READ FULL FAIL:\", type(e).__name__, e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ac0c724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRY READ: /mnt/d/community-detection/data/processed/_runs/20251214_165847/lbsn2vec/edges_final.parquet\n"
     ]
    }
   ],
   "source": [
    "print(\"TRY READ:\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1ad9ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--project-root PROJECT_ROOT] --dataset\n",
      "                             {brightkite,lbsn2vec} [--run-id RUN_ID]\n",
      "                             [--strict]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --dataset\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/d/community-detection/.venv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3709: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def _ok(msg): print(f\"[OK]  {msg}\")\n",
    "def _warn(msg): print(f\"[WARN]{msg}\")\n",
    "def _fail(msg): print(f\"[FAIL]{msg}\")\n",
    "\n",
    "\n",
    "def read_manifest(manifest_path: Path):\n",
    "    if not manifest_path.exists():\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(manifest_path.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def file_stat(p: Path):\n",
    "    if not p.exists():\n",
    "        return False, \"missing\"\n",
    "    sz = p.stat().st_size\n",
    "    if sz <= 0:\n",
    "        return False, \"empty\"\n",
    "    return True, f\"{sz/1024/1024:.2f} MB\"\n",
    "\n",
    "\n",
    "def check_parquet(p: Path, required_cols=None, head=5):\n",
    "    try:\n",
    "        df = pd.read_parquet(p)\n",
    "    except Exception as e:\n",
    "        return False, f\"read_parquet error: {e}\", None\n",
    "\n",
    "    if required_cols:\n",
    "        missing = [c for c in required_cols if c not in df.columns]\n",
    "        if missing:\n",
    "            return False, f\"missing cols: {missing}\", df\n",
    "\n",
    "    if len(df) == 0:\n",
    "        return False, \"dataframe is empty\", df\n",
    "\n",
    "    # quick sanity: null ratio\n",
    "    null_ratio = df.isna().mean(numeric_only=False).sort_values(ascending=False).head(3).to_dict()\n",
    "    return True, f\"rows={len(df):,} cols={len(df.columns)} null_top3={null_ratio}\", df.head(head)\n",
    "\n",
    "\n",
    "def check_npy(p: Path):\n",
    "    try:\n",
    "        Z = np.load(p, mmap_mode=\"r\")\n",
    "    except Exception as e:\n",
    "        return False, f\"np.load error: {e}\", None\n",
    "\n",
    "    if Z is None or len(Z.shape) != 2:\n",
    "        return False, f\"unexpected shape: {None if Z is None else Z.shape}\", None\n",
    "\n",
    "    n, d = Z.shape\n",
    "    if n <= 0 or d <= 0:\n",
    "        return False, f\"invalid shape: {Z.shape}\", None\n",
    "\n",
    "    # quick numeric sanity\n",
    "    try:\n",
    "        sample = np.asarray(Z[: min(2000, n)])\n",
    "        finite_ratio = np.isfinite(sample).mean()\n",
    "    except Exception:\n",
    "        finite_ratio = None\n",
    "\n",
    "    return True, f\"shape={Z.shape} finite_ratio(sample)={finite_ratio}\", None\n",
    "\n",
    "\n",
    "def main():\n",
    "    ap = argparse.ArgumentParser()\n",
    "    ap.add_argument(\"--project-root\", type=str, default=\".\")\n",
    "    ap.add_argument(\"--dataset\", type=str, choices=[\"brightkite\", \"lbsn2vec\"], required=True)\n",
    "    ap.add_argument(\"--run-id\", type=str, default=None)\n",
    "    ap.add_argument(\"--strict\", action=\"store_true\", help=\"fail fast if any required file fails\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    root = Path(args.project_root).resolve()\n",
    "    processed = root / \"data\" / \"processed\"\n",
    "    data_cleared = processed / \"data_cleared\" / args.dataset\n",
    "    runs_root = processed / \"_runs\"\n",
    "\n",
    "    manifest = read_manifest(processed / \"viz_runs.json\")\n",
    "    run_id = args.run_id\n",
    "    if run_id is None and manifest and args.dataset in manifest:\n",
    "        run_id = manifest[args.dataset].get(\"run_id\")\n",
    "\n",
    "    if run_id is None:\n",
    "        _fail(\"run_id not provided and viz_runs.json missing/does not contain dataset\")\n",
    "        return 2\n",
    "\n",
    "    run_ds = runs_root / run_id / args.dataset\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"PROJECT_ROOT : {root}\")\n",
    "    print(f\"DATASET      : {args.dataset}\")\n",
    "    print(f\"DATA_CLEARED : {data_cleared}\")\n",
    "    print(f\"RUN_DIR      : {run_ds}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # ---- Required base files (data_cleared)\n",
    "    base_files = {\n",
    "        \"edges_final\": (data_cleared / \"edges_final.parquet\", [\"u\", \"v\"]),\n",
    "        \"users_final\": (data_cleared / \"users_final.parquet\", [\"user_id\"]),\n",
    "    }\n",
    "\n",
    "    # ---- Optional base files\n",
    "    optional_base = {\n",
    "        \"feat_df\": (data_cleared / \"feat_df.parquet\", [\"user_id\"]),\n",
    "        \"checkins_clean\": (data_cleared / \"checkins_clean.parquet\", None),\n",
    "        \"meta\": (data_cleared / \"data_cleared_meta.json\", None),\n",
    "    }\n",
    "\n",
    "    # ---- Required run artifacts\n",
    "    comm_df_rep = run_ds / \"comm_df.repaired.parquet\"\n",
    "    comm_df_raw = run_ds / \"comm_df.parquet\"\n",
    "    comm_df_path = comm_df_rep if comm_df_rep.exists() else comm_df_raw\n",
    "\n",
    "    run_files = {\n",
    "        \"comm_df\": (comm_df_path, [\"user_id\", \"community_id\"]),\n",
    "        \"Z\": (run_ds / \"Z.npy\", None),\n",
    "    }\n",
    "\n",
    "    # ---- Optional run artifacts\n",
    "    optional_run = {\n",
    "        \"comm_metrics\": (run_ds / \"comm_metrics.parquet\", None),\n",
    "        \"metrics_global\": (run_ds / \"metrics_global.json\", None),\n",
    "    }\n",
    "\n",
    "    # ---- Check existence + load\n",
    "    failures = 0\n",
    "\n",
    "    print(\"\\n[BASE REQUIRED]\")\n",
    "    for k, (p, req_cols) in base_files.items():\n",
    "        alive, stat = file_stat(p)\n",
    "        if not alive:\n",
    "            _fail(f\"{k}: {p} -> {stat}\")\n",
    "            failures += 1\n",
    "            if args.strict: return 2\n",
    "            continue\n",
    "        ok, msg, head = check_parquet(p, req_cols)\n",
    "        if ok:\n",
    "            _ok(f\"{k}: {p.name} ({stat}) | {msg}\")\n",
    "            print(head)\n",
    "        else:\n",
    "            _fail(f\"{k}: {p.name} ({stat}) | {msg}\")\n",
    "            failures += 1\n",
    "            if args.strict: return 2\n",
    "\n",
    "    print(\"\\n[RUN REQUIRED]\")\n",
    "    # comm_df\n",
    "    p, req_cols = run_files[\"comm_df\"]\n",
    "    alive, stat = file_stat(p)\n",
    "    if not alive:\n",
    "        _fail(f\"comm_df: {p} -> {stat}\")\n",
    "        failures += 1\n",
    "        if args.strict: return 2\n",
    "        comm_df = None\n",
    "    else:\n",
    "        ok, msg, head = check_parquet(p, req_cols)\n",
    "        if ok:\n",
    "            _ok(f\"comm_df: {p.name} ({stat}) | {msg}\")\n",
    "            print(head)\n",
    "            comm_df = pd.read_parquet(p)[[\"user_id\", \"community_id\"]].copy()\n",
    "            comm_df[\"user_id\"] = comm_df[\"user_id\"].astype(str)\n",
    "        else:\n",
    "            _fail(f\"comm_df: {p.name} ({stat}) | {msg}\")\n",
    "            failures += 1\n",
    "            if args.strict: return 2\n",
    "            comm_df = None\n",
    "\n",
    "    # Z.npy\n",
    "    pz, _ = run_files[\"Z\"]\n",
    "    alive, stat = file_stat(pz)\n",
    "    if not alive:\n",
    "        _fail(f\"Z.npy: {pz} -> {stat}\")\n",
    "        failures += 1\n",
    "        if args.strict: return 2\n",
    "        Z = None\n",
    "    else:\n",
    "        ok, msg, _ = check_npy(pz)\n",
    "        if ok:\n",
    "            _ok(f\"Z.npy: {pz.name} ({stat}) | {msg}\")\n",
    "            Z = np.load(pz, mmap_mode=\"r\")\n",
    "        else:\n",
    "            _fail(f\"Z.npy: {pz.name} ({stat}) | {msg}\")\n",
    "            failures += 1\n",
    "            if args.strict: return 2\n",
    "            Z = None\n",
    "\n",
    "    # Alignment check (critical for embedding view)\n",
    "    if comm_df is not None and Z is not None:\n",
    "        if len(comm_df) != Z.shape[0]:\n",
    "            _warn(\n",
    "                f\"Alignment: len(comm_df)={len(comm_df):,} != Z.shape[0]={Z.shape[0]:,}. \"\n",
    "                \"Embedding scatter có thể lệch màu. Nên lưu thêm users_in_run.parquet theo đúng thứ tự Z.\"\n",
    "            )\n",
    "        else:\n",
    "            _ok(f\"Alignment: len(comm_df) matches Z rows ({len(comm_df):,})\")\n",
    "\n",
    "        # community sanity\n",
    "        n_comm = comm_df[\"community_id\"].nunique()\n",
    "        largest = comm_df[\"community_id\"].value_counts().max()\n",
    "        _ok(f\"Community sanity: n_communities={n_comm} | largest_comm={largest}\")\n",
    "\n",
    "    print(\"\\n[OPTIONAL FILES]\")\n",
    "    for k, (p, req_cols) in {**optional_base, **optional_run}.items():\n",
    "        alive, stat = file_stat(p)\n",
    "        if not alive:\n",
    "            _warn(f\"{k}: {p} -> {stat} (optional)\")\n",
    "            continue\n",
    "\n",
    "        if p.suffix == \".parquet\":\n",
    "            ok, msg, head = check_parquet(p, req_cols)\n",
    "            if ok:\n",
    "                _ok(f\"{k}: {p.name} ({stat}) | {msg}\")\n",
    "                print(head)\n",
    "            else:\n",
    "                _warn(f\"{k}: {p.name} ({stat}) | {msg}\")\n",
    "        elif p.suffix == \".json\":\n",
    "            try:\n",
    "                obj = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "                _ok(f\"{k}: {p.name} ({stat}) | json keys={list(obj)[:10]}\")\n",
    "            except Exception as e:\n",
    "                _warn(f\"{k}: {p.name} ({stat}) | json read error: {e}\")\n",
    "        else:\n",
    "            _ok(f\"{k}: {p.name} ({stat})\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    if failures == 0:\n",
    "        _ok(\"PRECHECK PASSED ✅ Bạn có thể bắt đầu visualization (Streamlit).\")\n",
    "        return 0\n",
    "    _fail(f\"PRECHECK FAILED ❌ failures={failures}\")\n",
    "    return 2\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    raise SystemExit(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
