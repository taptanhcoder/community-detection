{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a18e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /mnt/d/community-detection\n",
      "DATA_DIR: /mnt/d/community-detection/data\n",
      "DATA_CLEARED_DIR: /mnt/d/community-detection/data/processed/data_cleared\n",
      "LINUX_TMP_DIR: /home/dtnghia/cd_parquet_tmp\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os, json, time, math, shutil\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any, Tuple, Set\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# ====== Repo root của bạn (đang chạy ở /mnt/d/community-detection) ======\n",
    "PROJECT_ROOT = Path(\"/mnt/d/community-detection\").resolve()\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "PROCESSED_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "# Thư mục output \"sạch\" dùng về sau\n",
    "DATA_CLEARED_DIR = PROCESSED_DIR / \"data_cleared\"\n",
    "\n",
    "# Thư mục tạm trên filesystem Linux (quan trọng để tránh corrupt trên /mnt/d)\n",
    "LINUX_TMP_DIR = Path.home() / \"cd_parquet_tmp\"\n",
    "LINUX_TMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"DATA_CLEARED_DIR:\", DATA_CLEARED_DIR)\n",
    "print(\"LINUX_TMP_DIR:\", LINUX_TMP_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76b80d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parquet_validate(path: Path, read_rows: int = 5) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Validate Parquet bằng cách:\n",
    "    1) đọc metadata (ParquetFile)\n",
    "    2) đọc thử vài dòng bằng pandas\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pf = pq.ParquetFile(path)\n",
    "        _ = pf.metadata.num_rows\n",
    "    except Exception as e:\n",
    "        return False, f\"metadata fail: {type(e).__name__}: {e}\"\n",
    "\n",
    "    try:\n",
    "        df_head = pd.read_parquet(path, engine=\"pyarrow\").head(read_rows)\n",
    "        _ = df_head.shape\n",
    "    except Exception as e:\n",
    "        return False, f\"read fail: {type(e).__name__}: {e}\"\n",
    "\n",
    "    return True, \"ok\"\n",
    "\n",
    "\n",
    "def parquet_write_safe(\n",
    "    df: pd.DataFrame,\n",
    "    final_path: Path,\n",
    "    tmp_root: Path = LINUX_TMP_DIR,\n",
    "    compression: str = \"zstd\",   # đổi sang zstd để tránh snappy-related pain khi file bị gián đoạn\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    1) ghi vào tmp (Linux FS) trước\n",
    "    2) validate tmp\n",
    "    3) copy sang final_path (/mnt/d/...)\n",
    "    4) validate final_path\n",
    "    \"\"\"\n",
    "    final_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    tmp_path = tmp_root / f\"{final_path.name}.tmp_{int(time.time())}.parquet\"\n",
    "\n",
    "    # Ghi tmp\n",
    "    df.to_parquet(tmp_path, engine=\"pyarrow\", index=False, compression=compression)\n",
    "\n",
    "    ok, msg = parquet_validate(tmp_path)\n",
    "    if not ok:\n",
    "        raise OSError(f\"[TMP PARQUET CORRUPT] {tmp_path} | {msg}\")\n",
    "\n",
    "    # Copy sang đích\n",
    "    shutil.copyfile(tmp_path, final_path)\n",
    "\n",
    "    ok2, msg2 = parquet_validate(final_path)\n",
    "    if not ok2:\n",
    "        raise OSError(f\"[FINAL PARQUET CORRUPT] {final_path} | {msg2}\")\n",
    "\n",
    "    # cleanup tmp\n",
    "    try:\n",
    "        tmp_path.unlink(missing_ok=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(f\"[OK] wrote+validated: {final_path} | rows={len(df):,} cols={df.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f0b90a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"run\": {\n",
      "    \"seed\": 42\n",
      "  },\n",
      "  \"preprocess\": {\n",
      "    \"min_checkins\": 10,\n",
      "    \"min_degree\": 3,\n",
      "    \"iterative_filter\": true,\n",
      "    \"lat_range\": [\n",
      "      -90.0,\n",
      "      90.0\n",
      "    ],\n",
      "    \"lon_range\": [\n",
      "      -180.0,\n",
      "      180.0\n",
      "    ]\n",
      "  },\n",
      "  \"features\": {\n",
      "    \"log1p_counts\": true,\n",
      "    \"standardize\": true\n",
      "  },\n",
      "  \"lbsn2vec\": {\n",
      "    \"snapshot\": \"old\",\n",
      "    \"tier\": \"curated\",\n",
      "    \"sample_frac\": 1.0,\n",
      "    \"chunksize\": 2000000\n",
      "  },\n",
      "  \"brightkite\": {\n",
      "    \"sample_frac\": 1.0,\n",
      "    \"chunksize\": 2000000\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "CFG: Dict[str, Any] = {\n",
    "    \"run\": {\n",
    "        \"seed\": 42,\n",
    "    },\n",
    "    \"preprocess\": {\n",
    "        \"min_checkins\": 10,\n",
    "        \"min_degree\": 3,\n",
    "        \"iterative_filter\": True,\n",
    "        \"lat_range\": [-90.0, 90.0],\n",
    "        \"lon_range\": [-180.0, 180.0],\n",
    "    },\n",
    "    \"features\": {\n",
    "        \"log1p_counts\": True,\n",
    "        \"standardize\": True,\n",
    "    },\n",
    "    \"lbsn2vec\": {\n",
    "        \"snapshot\": \"old\",     # old/new\n",
    "        \"tier\": \"curated\",     # curated/raw (mình rebuild theo curated)\n",
    "        \"sample_frac\": 1.0,    # đặt 1.0 để chạy full; 0.1 nếu cần test nhanh\n",
    "        \"chunksize\": 2_000_000,\n",
    "    },\n",
    "    \"brightkite\": {\n",
    "        \"sample_frac\": 1.0,\n",
    "        \"chunksize\": 2_000_000,\n",
    "    }\n",
    "}\n",
    "\n",
    "np.random.seed(int(CFG[\"run\"][\"seed\"]))\n",
    "print(json.dumps(CFG, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65b69104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== lbsn2vec\n",
      "           root: /mnt/d/community-detection/data/LBSN2Vec | exists= True\n",
      " friendship_old: /mnt/d/community-detection/data/LBSN2Vec/dataset_WWW_friendship_old.txt | exists= True\n",
      " friendship_new: /mnt/d/community-detection/data/LBSN2Vec/dataset_WWW_friendship_new.txt | exists= True\n",
      "checkins_curated: /mnt/d/community-detection/data/LBSN2Vec/dataset_WWW_Checkins_anonymized.txt | exists= True\n",
      "            poi: /mnt/d/community-detection/data/LBSN2Vec/raw_POIs.txt | exists= True\n",
      "\n",
      "== brightkite\n",
      "           root: /mnt/d/community-detection/data/Brightkite | exists= True\n",
      "          edges: /mnt/d/community-detection/data/Brightkite/Brightkite_edges.txt | exists= True\n",
      "       checkins: /mnt/d/community-detection/data/Brightkite/Brightkite_totalCheckins.txt | exists= True\n"
     ]
    }
   ],
   "source": [
    "PATHS = {\n",
    "    \"lbsn2vec\": {\n",
    "        \"root\": DATA_DIR / \"LBSN2Vec\",\n",
    "        \"friendship_old\": DATA_DIR / \"LBSN2Vec\" / \"dataset_WWW_friendship_old.txt\",\n",
    "        \"friendship_new\": DATA_DIR / \"LBSN2Vec\" / \"dataset_WWW_friendship_new.txt\",\n",
    "        \"checkins_curated\": DATA_DIR / \"LBSN2Vec\" / \"dataset_WWW_Checkins_anonymized.txt\",\n",
    "        \"poi\": DATA_DIR / \"LBSN2Vec\" / \"raw_POIs.txt\",\n",
    "    },\n",
    "    \"brightkite\": {\n",
    "        \"root\": DATA_DIR / \"Brightkite\",\n",
    "        \"edges\": DATA_DIR / \"Brightkite\" / \"Brightkite_edges.txt\",\n",
    "        \"checkins\": DATA_DIR / \"Brightkite\" / \"Brightkite_totalCheckins.txt\",\n",
    "    }\n",
    "}\n",
    "\n",
    "for ds, p in PATHS.items():\n",
    "    print(\"\\n==\", ds)\n",
    "    for k, v in p.items():\n",
    "        print(f\"{k:>15}:\", v, \"| exists=\", v.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc31e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_edges_two_cols(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=r\"\\s+\", header=None, usecols=[0, 1], dtype=str, engine=\"python\")\n",
    "    df.columns = [\"u\", \"v\"]\n",
    "    df[\"u\"] = df[\"u\"].astype(str)\n",
    "    df[\"v\"] = df[\"v\"].astype(str)\n",
    "    return df\n",
    "\n",
    "def read_pois_minimal(path: Path) -> pd.DataFrame:\n",
    "    poi = pd.read_csv(path, sep=r\"\\s+\", header=None, usecols=[0, 1, 2], dtype=str, engine=\"python\")\n",
    "    poi.columns = [\"venue_id\", \"lat\", \"lon\"]\n",
    "    poi[\"venue_id\"] = poi[\"venue_id\"].astype(str)\n",
    "    poi[\"lat\"] = pd.to_numeric(poi[\"lat\"], errors=\"coerce\")\n",
    "    poi[\"lon\"] = pd.to_numeric(poi[\"lon\"], errors=\"coerce\")\n",
    "    return poi\n",
    "\n",
    "def parse_lbsn_curated_checkins_9col_chunk(chunk: pd.DataFrame) -> pd.DataFrame:\n",
    "    if chunk.shape[1] < 9:\n",
    "        raise ValueError(f\"Expected >=9 columns, got {chunk.shape}\")\n",
    "\n",
    "    df = chunk.iloc[:, :9].copy()\n",
    "    df.columns = [\"user_id\", \"venue_id\", \"dow\", \"mon\", \"day\", \"time\", \"tz\", \"year\", \"tz_offset_min\"]\n",
    "\n",
    "    ts_str = (\n",
    "        df[\"dow\"].astype(str) + \" \" +\n",
    "        df[\"mon\"].astype(str) + \" \" +\n",
    "        df[\"day\"].astype(str) + \" \" +\n",
    "        df[\"time\"].astype(str) + \" \" +\n",
    "        df[\"tz\"].astype(str) + \" \" +\n",
    "        df[\"year\"].astype(str)\n",
    "    )\n",
    "    ts = pd.to_datetime(ts_str, format=\"%a %b %d %H:%M:%S %z %Y\", errors=\"coerce\")\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"user_id\": df[\"user_id\"].astype(str),\n",
    "        \"venue_id\": df[\"venue_id\"].astype(str),\n",
    "        \"ts\": ts,\n",
    "    })\n",
    "    return out\n",
    "\n",
    "def parse_snap_checkins_chunk(chunk: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Brightkite/Gowalla thường: user_id, ts, lat, lon, location_id (>=5 cols).\n",
    "    Mình giữ 5 cột đầu nếu có.\n",
    "    \"\"\"\n",
    "    if chunk.shape[1] < 4:\n",
    "        raise ValueError(f\"SNAP chunk has too few cols: {chunk.shape}\")\n",
    "\n",
    "    if chunk.shape[1] >= 5:\n",
    "        sub = chunk.iloc[:, :5].copy()\n",
    "        sub.columns = [\"user_id\", \"ts\", \"lat\", \"lon\", \"venue_id\"]\n",
    "    else:\n",
    "        sub = chunk.iloc[:, :4].copy()\n",
    "        sub.columns = [\"user_id\", \"ts\", \"lat\", \"lon\"]\n",
    "        sub[\"venue_id\"] = \"NA\"\n",
    "\n",
    "    sub[\"user_id\"] = sub[\"user_id\"].astype(str)\n",
    "    sub[\"venue_id\"] = sub[\"venue_id\"].astype(str)\n",
    "    sub[\"ts\"] = pd.to_datetime(sub[\"ts\"], errors=\"coerce\")\n",
    "    sub[\"lat\"] = pd.to_numeric(sub[\"lat\"], errors=\"coerce\")\n",
    "    sub[\"lon\"] = pd.to_numeric(sub[\"lon\"], errors=\"coerce\")\n",
    "    return sub[[\"user_id\", \"ts\", \"lat\", \"lon\", \"venue_id\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2d48ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LBSN C1] edges_raw: (363704, 2)\n",
      "[LBSN C1] checkins_core: (22809624, 3)\n",
      "[LBSN C1] checkins_raw: (22809624, 5) | ts_ok=1.000 lat_ok=1.000 lon_ok=1.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(    u       v\n",
       " 0  15  595326\n",
       " 1  19      54\n",
       " 2  19    1061\n",
       " 3  19    1356\n",
       " 4  19    1668,\n",
       "    user_id                  venue_id                        ts        lat  \\\n",
       " 0   822121  4b4b87b5f964a5204a9f26e3 2012-04-03 18:00:07+00:00  41.029717   \n",
       " 1   208842  4b4606f2f964a520751426e3 2012-04-03 18:00:08+00:00  30.270786   \n",
       " 2   113817  4b4bade2f964a520cfa326e3 2012-04-03 18:00:09+00:00  40.436712   \n",
       " 3    14732  4c143cada5eb76b0dc7dc1b7 2012-04-03 18:00:09+00:00  31.188807   \n",
       " 4  1397630  4e88cf4ed22d53877981fdab 2012-04-03 18:00:09+00:00  19.399745   \n",
       " \n",
       "          lon  \n",
       " 0  28.974420  \n",
       " 1 -97.753153  \n",
       " 2 -79.990132  \n",
       " 3 -81.376461  \n",
       " 4 -99.102595  )"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lbsn_step1_load_parse(cfg: Dict[str, Any]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    c = cfg[\"lbsn2vec\"]\n",
    "    snapshot = c[\"snapshot\"]\n",
    "    sample_frac = float(c[\"sample_frac\"])\n",
    "    chunksize = int(c[\"chunksize\"])\n",
    "    seed = int(cfg[\"run\"][\"seed\"])\n",
    "\n",
    "    edges_path = PATHS[\"lbsn2vec\"][\"friendship_old\"] if snapshot == \"old\" else PATHS[\"lbsn2vec\"][\"friendship_new\"]\n",
    "    checkins_path = PATHS[\"lbsn2vec\"][\"checkins_curated\"]\n",
    "    poi_path = PATHS[\"lbsn2vec\"][\"poi\"]\n",
    "\n",
    "    # 1) edges\n",
    "    edges_all = read_edges_two_cols(edges_path)\n",
    "\n",
    "    # 2) sample users (optional)\n",
    "    keep_users: Optional[Set[str]] = None\n",
    "    if sample_frac < 1.0:\n",
    "        users = set()\n",
    "        for chunk in pd.read_csv(checkins_path, sep=r\"\\s+\", header=None, dtype=str, engine=\"python\", chunksize=chunksize):\n",
    "            users.update(chunk.iloc[:, 0].astype(str).unique().tolist())\n",
    "        users = np.array(list(users), dtype=object)\n",
    "        rng = np.random.default_rng(seed)\n",
    "        rng.shuffle(users)\n",
    "        n = int(np.ceil(len(users) * sample_frac))\n",
    "        keep_users = set(users[:n].tolist())\n",
    "        print(f\"[LBSN C1] sampled users: {len(keep_users):,} / total≈{len(users):,}\")\n",
    "\n",
    "    if keep_users is not None:\n",
    "        edges_raw = edges_all[edges_all[\"u\"].isin(keep_users) & edges_all[\"v\"].isin(keep_users)].copy()\n",
    "    else:\n",
    "        edges_raw = edges_all\n",
    "\n",
    "    print(f\"[LBSN C1] edges_raw: {edges_raw.shape}\")\n",
    "\n",
    "    # 3) checkins chunked -> parse -> filter users\n",
    "    pieces = []\n",
    "    for chunk in pd.read_csv(checkins_path, sep=r\"\\s+\", header=None, dtype=str, engine=\"python\", chunksize=chunksize):\n",
    "        if keep_users is not None:\n",
    "            chunk = chunk[chunk.iloc[:, 0].astype(str).isin(keep_users)]\n",
    "        if len(chunk) == 0:\n",
    "            continue\n",
    "        pieces.append(parse_lbsn_curated_checkins_9col_chunk(chunk))\n",
    "\n",
    "    checkins_core = pd.concat(pieces, ignore_index=True) if pieces else pd.DataFrame(columns=[\"user_id\",\"venue_id\",\"ts\"])\n",
    "    print(f\"[LBSN C1] checkins_core: {checkins_core.shape}\")\n",
    "\n",
    "    # 4) POI join\n",
    "    pois = read_pois_minimal(poi_path)\n",
    "    if len(checkins_core) > 0:\n",
    "        needed = set(checkins_core[\"venue_id\"].unique().tolist())\n",
    "        pois = pois[pois[\"venue_id\"].isin(needed)].copy()\n",
    "\n",
    "    checkins_raw = checkins_core.merge(pois, on=\"venue_id\", how=\"left\")\n",
    "    checkins_raw[\"lat\"] = pd.to_numeric(checkins_raw[\"lat\"], errors=\"coerce\")\n",
    "    checkins_raw[\"lon\"] = pd.to_numeric(checkins_raw[\"lon\"], errors=\"coerce\")\n",
    "\n",
    "    print(f\"[LBSN C1] checkins_raw: {checkins_raw.shape} | ts_ok={checkins_raw['ts'].notna().mean():.3f} lat_ok={checkins_raw['lat'].notna().mean():.3f} lon_ok={checkins_raw['lon'].notna().mean():.3f}\")\n",
    "    return edges_raw, checkins_raw\n",
    "\n",
    "edges_raw_lbsn, checkins_raw_lbsn = lbsn_step1_load_parse(CFG)\n",
    "edges_raw_lbsn.head(), checkins_raw_lbsn.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0bec860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BK C1] edges_raw: (428156, 2)\n",
      "[BK C1] checkins_raw: (4747287, 5) | ts_ok=1.000 lat_ok=1.000 lon_ok=1.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   u  v\n",
       " 0  0  1\n",
       " 1  0  2\n",
       " 2  0  3\n",
       " 3  0  4\n",
       " 4  0  5,\n",
       "   user_id                        ts        lat         lon  \\\n",
       " 0       0 2010-10-17 01:48:53+00:00  39.747652 -104.992510   \n",
       " 1       0 2010-10-16 06:02:04+00:00  39.891383 -105.070814   \n",
       " 2       0 2010-10-16 03:48:54+00:00  39.891077 -105.068532   \n",
       " 3       0 2010-10-14 18:25:51+00:00  39.750469 -104.999073   \n",
       " 4       0 2010-10-14 00:21:47+00:00  39.752713 -104.996337   \n",
       " \n",
       "                                    venue_id  \n",
       " 0          88c46bf20db295831bd2d1718ad7e6f5  \n",
       " 1          7a0f88982aa015062b95e3b4843f9ca2  \n",
       " 2          dd7cd3d264c2d063832db506fba8bf79  \n",
       " 3  9848afcc62e500a01cf6fbf24b797732f8963683  \n",
       " 4          2ef143e12038c870038df53e0478cefc  )"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def brightkite_step1_load_parse(cfg: Dict[str, Any]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    c = cfg[\"brightkite\"]\n",
    "    sample_frac = float(c[\"sample_frac\"])\n",
    "    chunksize = int(c[\"chunksize\"])\n",
    "    seed = int(cfg[\"run\"][\"seed\"])\n",
    "\n",
    "    edges_path = PATHS[\"brightkite\"][\"edges\"]\n",
    "    checkins_path = PATHS[\"brightkite\"][\"checkins\"]\n",
    "\n",
    "    edges_all = read_edges_two_cols(edges_path)\n",
    "\n",
    "    # sample users from checkins (optional)\n",
    "    keep_users: Optional[Set[str]] = None\n",
    "    if sample_frac < 1.0:\n",
    "        users = set()\n",
    "        for chunk in pd.read_csv(checkins_path, sep=r\"\\s+\", header=None, dtype=str, engine=\"python\", chunksize=chunksize):\n",
    "            users.update(chunk.iloc[:, 0].astype(str).unique().tolist())\n",
    "        users = np.array(list(users), dtype=object)\n",
    "        rng = np.random.default_rng(seed)\n",
    "        rng.shuffle(users)\n",
    "        n = int(np.ceil(len(users) * sample_frac))\n",
    "        keep_users = set(users[:n].tolist())\n",
    "        print(f\"[BK C1] sampled users: {len(keep_users):,} / total≈{len(users):,}\")\n",
    "\n",
    "    if keep_users is not None:\n",
    "        edges_raw = edges_all[edges_all[\"u\"].isin(keep_users) & edges_all[\"v\"].isin(keep_users)].copy()\n",
    "    else:\n",
    "        edges_raw = edges_all\n",
    "    print(f\"[BK C1] edges_raw: {edges_raw.shape}\")\n",
    "\n",
    "    pieces = []\n",
    "    for chunk in pd.read_csv(checkins_path, sep=r\"\\s+\", header=None, dtype=str, engine=\"python\", chunksize=chunksize):\n",
    "        if keep_users is not None:\n",
    "            chunk = chunk[chunk.iloc[:, 0].astype(str).isin(keep_users)]\n",
    "        if len(chunk) == 0:\n",
    "            continue\n",
    "        pieces.append(parse_snap_checkins_chunk(chunk))\n",
    "\n",
    "    checkins_raw = pd.concat(pieces, ignore_index=True) if pieces else pd.DataFrame(columns=[\"user_id\",\"ts\",\"lat\",\"lon\",\"venue_id\"])\n",
    "    print(f\"[BK C1] checkins_raw: {checkins_raw.shape} | ts_ok={checkins_raw['ts'].notna().mean():.3f} lat_ok={checkins_raw['lat'].notna().mean():.3f} lon_ok={checkins_raw['lon'].notna().mean():.3f}\")\n",
    "    return edges_raw, checkins_raw\n",
    "\n",
    "edges_raw_bk, checkins_raw_bk = brightkite_step1_load_parse(CFG)\n",
    "edges_raw_bk.head(), checkins_raw_bk.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f8d11b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LBSN C2] (363699, 2) (22809619, 5)\n",
      "[BK   C2] (214078, 2) (4747172, 5)\n"
     ]
    }
   ],
   "source": [
    "def make_undirected_dedup(edges: pd.DataFrame) -> pd.DataFrame:\n",
    "    u = edges[\"u\"].astype(str).to_numpy()\n",
    "    v = edges[\"v\"].astype(str).to_numpy()\n",
    "    u2 = np.where(u <= v, u, v)\n",
    "    v2 = np.where(u <= v, v, u)\n",
    "    out = pd.DataFrame({\"u\": u2, \"v\": v2})\n",
    "    out = out[out[\"u\"] != out[\"v\"]]\n",
    "    out = out.drop_duplicates([\"u\", \"v\"]).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "def clean_checkins(chk: pd.DataFrame, cfg: dict) -> pd.DataFrame:\n",
    "    lat_lo, lat_hi = cfg[\"preprocess\"][\"lat_range\"]\n",
    "    lon_lo, lon_hi = cfg[\"preprocess\"][\"lon_range\"]\n",
    "\n",
    "    out = chk.copy()\n",
    "    out[\"user_id\"] = out[\"user_id\"].astype(str)\n",
    "    out[\"venue_id\"] = out[\"venue_id\"].astype(str)\n",
    "    out[\"ts\"] = pd.to_datetime(out[\"ts\"], errors=\"coerce\")\n",
    "    out[\"lat\"] = pd.to_numeric(out[\"lat\"], errors=\"coerce\")\n",
    "    out[\"lon\"] = pd.to_numeric(out[\"lon\"], errors=\"coerce\")\n",
    "\n",
    "    out = out.dropna(subset=[\"user_id\", \"venue_id\", \"ts\", \"lat\", \"lon\"])\n",
    "    out = out[(out[\"lat\"] >= lat_lo) & (out[\"lat\"] <= lat_hi) & (out[\"lon\"] >= lon_lo) & (out[\"lon\"] <= lon_hi)]\n",
    "\n",
    "    # normalize tz-aware -> UTC naive\n",
    "    try:\n",
    "        if hasattr(out[\"ts\"].dt, \"tz\") and out[\"ts\"].dt.tz is not None:\n",
    "            out[\"ts\"] = out[\"ts\"].dt.tz_convert(\"UTC\").dt.tz_localize(None)\n",
    "    except Exception:\n",
    "        out[\"ts\"] = pd.to_datetime(out[\"ts\"], errors=\"coerce\")\n",
    "\n",
    "    out = out.dropna(subset=[\"ts\"]).reset_index(drop=True)\n",
    "    return out[[\"user_id\", \"ts\", \"lat\", \"lon\", \"venue_id\"]].copy()\n",
    "\n",
    "\n",
    "def step2_clean(edges_raw: pd.DataFrame, checkins_raw: pd.DataFrame, cfg: dict):\n",
    "    edges_clean = make_undirected_dedup(edges_raw)\n",
    "    checkins_clean = clean_checkins(checkins_raw, cfg)\n",
    "    return edges_clean, checkins_clean\n",
    "\n",
    "edges_clean_lbsn, checkins_clean_lbsn = step2_clean(edges_raw_lbsn, checkins_raw_lbsn, CFG)\n",
    "edges_clean_bk, checkins_clean_bk = step2_clean(edges_raw_bk, checkins_raw_bk, CFG)\n",
    "\n",
    "print(\"[LBSN C2]\", edges_clean_lbsn.shape, checkins_clean_lbsn.shape)\n",
    "print(\"[BK   C2]\", edges_clean_bk.shape, checkins_clean_bk.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82fc31d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LBSN C3] (47389, 1) (279816, 2) (10328914, 5)\n",
      "[BK   C3] (15092, 1) (116506, 2) (3656191, 5)\n"
     ]
    }
   ],
   "source": [
    "def degree_from_edges(edges: pd.DataFrame) -> pd.Series:\n",
    "    u = edges[\"u\"].astype(str)\n",
    "    v = edges[\"v\"].astype(str)\n",
    "    return pd.concat([u, v]).value_counts()\n",
    "\n",
    "def filter_induced_once(edges: pd.DataFrame, chk: pd.DataFrame, k: int, d: int):\n",
    "    ccount = chk[\"user_id\"].astype(str).value_counts()\n",
    "    deg = degree_from_edges(edges)\n",
    "\n",
    "    users_ok = ccount[ccount >= k].index\n",
    "    deg_ok = deg[deg >= d].index\n",
    "    v_keep = pd.Index(users_ok).intersection(pd.Index(deg_ok))\n",
    "\n",
    "    edges2 = edges[edges[\"u\"].isin(v_keep) & edges[\"v\"].isin(v_keep)].copy().reset_index(drop=True)\n",
    "    chk2 = chk[chk[\"user_id\"].isin(v_keep)].copy().reset_index(drop=True)\n",
    "\n",
    "    return v_keep, edges2, chk2\n",
    "\n",
    "def step3_filter(edges_clean: pd.DataFrame, checkins_clean: pd.DataFrame, cfg: dict):\n",
    "    k = int(cfg[\"preprocess\"].get(\"min_checkins\", 10))\n",
    "    d = int(cfg[\"preprocess\"].get(\"min_degree\", 3))\n",
    "    iterative = bool(cfg[\"preprocess\"].get(\"iterative_filter\", True))\n",
    "\n",
    "    edges_tmp = edges_clean.copy()\n",
    "    chk_tmp = checkins_clean.copy()\n",
    "\n",
    "    prev_users = -1\n",
    "    for r in range(1, 21):\n",
    "        v_keep, edges_tmp, chk_tmp = filter_induced_once(edges_tmp, chk_tmp, k=k, d=d)\n",
    "        n_users = len(v_keep)\n",
    "        if (not iterative) or (n_users == prev_users):\n",
    "            break\n",
    "        prev_users = n_users\n",
    "\n",
    "    users_final = pd.DataFrame({\"user_id\": pd.Index(chk_tmp[\"user_id\"].unique()).sort_values()})\n",
    "    edges_final = edges_tmp\n",
    "    checkins_final = chk_tmp\n",
    "    return users_final, edges_final, checkins_final\n",
    "\n",
    "users_final_lbsn, edges_final_lbsn, checkins_final_lbsn = step3_filter(edges_clean_lbsn, checkins_clean_lbsn, CFG)\n",
    "users_final_bk, edges_final_bk, checkins_final_bk = step3_filter(edges_clean_bk, checkins_clean_bk, CFG)\n",
    "\n",
    "print(\"[LBSN C3]\", users_final_lbsn.shape, edges_final_lbsn.shape, checkins_final_lbsn.shape)\n",
    "print(\"[BK   C3]\", users_final_bk.shape, edges_final_bk.shape, checkins_final_bk.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d56177d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LBSN C4] X: (47389, 43) feat: (47389, 43)\n",
      "[BK   C4] X: (15092, 43) feat: (15092, 43)\n"
     ]
    }
   ],
   "source": [
    "def _entropy_from_counts(counts: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    s = counts.sum()\n",
    "    if s <= 0:\n",
    "        return 0.0\n",
    "    p = counts / (s + eps)\n",
    "    p = p[p > 0]\n",
    "    return float(-(p * np.log(p + eps)).sum())\n",
    "\n",
    "def _haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    c = 2*np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "def build_user_features_from_checkins(users_final: pd.DataFrame, checkins_final: pd.DataFrame,\n",
    "                                      log1p_counts: bool = True, standardize: bool = True):\n",
    "    chk = checkins_final.copy()\n",
    "    chk[\"user_id\"] = chk[\"user_id\"].astype(str)\n",
    "    chk[\"venue_id\"] = chk[\"venue_id\"].astype(str)\n",
    "    chk[\"ts\"] = pd.to_datetime(chk[\"ts\"], errors=\"coerce\")\n",
    "    chk = chk.dropna(subset=[\"ts\", \"lat\", \"lon\", \"user_id\"])\n",
    "\n",
    "    chk[\"hour\"] = chk[\"ts\"].dt.hour.astype(int)\n",
    "    chk[\"dow\"]  = chk[\"ts\"].dt.dayofweek.astype(int)\n",
    "    chk[\"date\"] = chk[\"ts\"].dt.date\n",
    "\n",
    "    g = chk.groupby(\"user_id\", sort=False)\n",
    "\n",
    "    num_checkins = g.size().rename(\"num_checkins\")\n",
    "    num_active_days = g[\"date\"].nunique().rename(\"num_active_days\")\n",
    "    num_unique_venues = g[\"venue_id\"].nunique().rename(\"num_unique_venues\")\n",
    "\n",
    "    mean_lat = g[\"lat\"].mean().rename(\"mean_lat\")\n",
    "    mean_lon = g[\"lon\"].mean().rename(\"mean_lon\")\n",
    "    std_lat  = g[\"lat\"].std(ddof=0).fillna(0.0).rename(\"std_lat\")\n",
    "    std_lon  = g[\"lon\"].std(ddof=0).fillna(0.0).rename(\"std_lon\")\n",
    "\n",
    "    rog = {}\n",
    "    med_dist = {}\n",
    "    for uid, sub in g:\n",
    "        latc = float(sub[\"lat\"].mean())\n",
    "        lonc = float(sub[\"lon\"].mean())\n",
    "        d = _haversine_km(sub[\"lat\"].to_numpy(), sub[\"lon\"].to_numpy(), latc, lonc)\n",
    "        rog[uid] = float(np.sqrt(np.mean(d**2))) if len(d) else 0.0\n",
    "        med_dist[uid] = float(np.median(d)) if len(d) else 0.0\n",
    "    rog = pd.Series(rog, name=\"radius_of_gyration_km\")\n",
    "    med_dist = pd.Series(med_dist, name=\"median_dist_to_centroid_km\")\n",
    "\n",
    "    hour_counts = pd.crosstab(chk[\"user_id\"], chk[\"hour\"])\n",
    "    for h in range(24):\n",
    "        if h not in hour_counts.columns:\n",
    "            hour_counts[h] = 0\n",
    "    hour_counts = hour_counts[list(range(24))].sort_index(axis=1)\n",
    "\n",
    "    dow_counts = pd.crosstab(chk[\"user_id\"], chk[\"dow\"])\n",
    "    for d0 in range(7):\n",
    "        if d0 not in dow_counts.columns:\n",
    "            dow_counts[d0] = 0\n",
    "    dow_counts = dow_counts[list(range(7))].sort_index(axis=1)\n",
    "\n",
    "    hour_entropy = hour_counts.apply(lambda r: _entropy_from_counts(r.to_numpy()), axis=1).rename(\"hour_entropy\")\n",
    "    dow_entropy  = dow_counts.apply(lambda r: _entropy_from_counts(r.to_numpy()), axis=1).rename(\"dow_entropy\")\n",
    "    venue_entropy = g[\"venue_id\"].apply(lambda s: _entropy_from_counts(s.value_counts().to_numpy())).rename(\"venue_entropy\")\n",
    "\n",
    "    feat = pd.concat([\n",
    "        num_checkins, num_active_days, num_unique_venues,\n",
    "        mean_lat, mean_lon, std_lat, std_lon,\n",
    "        rog, med_dist,\n",
    "        hour_entropy, dow_entropy, venue_entropy\n",
    "    ], axis=1)\n",
    "\n",
    "    hour_prop = hour_counts.div(hour_counts.sum(axis=1).replace(0, np.nan), axis=0).fillna(0.0)\n",
    "    hour_prop.columns = [f\"hour_{h:02d}_p\" for h in hour_prop.columns]\n",
    "    dow_prop = dow_counts.div(dow_counts.sum(axis=1).replace(0, np.nan), axis=0).fillna(0.0)\n",
    "    dow_prop.columns = [f\"dow_{d0}_p\" for d0 in dow_prop.columns]\n",
    "\n",
    "    feat = feat.join(hour_prop, how=\"left\").join(dow_prop, how=\"left\").fillna(0.0)\n",
    "\n",
    "    if log1p_counts:\n",
    "        for c in [\"num_checkins\", \"num_active_days\", \"num_unique_venues\"]:\n",
    "            feat[c] = np.log1p(feat[c].astype(float))\n",
    "\n",
    "    user_order = users_final[\"user_id\"].astype(str).tolist()\n",
    "    feat = feat.reindex(user_order).fillna(0.0)\n",
    "    feat.index.name = \"user_id\"\n",
    "\n",
    "    if standardize:\n",
    "        mu = feat.mean(axis=0)\n",
    "        sd = feat.std(axis=0, ddof=0).replace(0, 1.0)\n",
    "        feat = (feat - mu) / sd\n",
    "\n",
    "    X_users = feat.to_numpy(dtype=np.float32)\n",
    "    return X_users, feat\n",
    "\n",
    "X_lbsn, feat_lbsn = build_user_features_from_checkins(\n",
    "    users_final_lbsn, checkins_final_lbsn,\n",
    "    log1p_counts=bool(CFG[\"features\"][\"log1p_counts\"]),\n",
    "    standardize=bool(CFG[\"features\"][\"standardize\"]),\n",
    ")\n",
    "X_bk, feat_bk = build_user_features_from_checkins(\n",
    "    users_final_bk, checkins_final_bk,\n",
    "    log1p_counts=bool(CFG[\"features\"][\"log1p_counts\"]),\n",
    "    standardize=bool(CFG[\"features\"][\"standardize\"]),\n",
    ")\n",
    "\n",
    "print(\"[LBSN C4] X:\", X_lbsn.shape, \"feat:\", feat_lbsn.shape)\n",
    "print(\"[BK   C4] X:\", X_bk.shape, \"feat:\", feat_bk.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ba41ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] wrote+validated: /mnt/d/community-detection/data/processed/data_cleared/lbsn2vec/edges_clean.parquet | rows=363,699 cols=2\n",
      "[OK] wrote+validated: /mnt/d/community-detection/data/processed/data_cleared/lbsn2vec/edges_final.parquet | rows=279,816 cols=2\n",
      "[OK] wrote+validated: /mnt/d/community-detection/data/processed/data_cleared/lbsn2vec/checkins_clean.parquet | rows=22,809,619 cols=5\n",
      "[OK] wrote+validated: /mnt/d/community-detection/data/processed/data_cleared/lbsn2vec/checkins_final.parquet | rows=10,328,914 cols=5\n",
      "[OK] wrote+validated: /mnt/d/community-detection/data/processed/data_cleared/lbsn2vec/users_final.parquet | rows=47,389 cols=1\n",
      "[OK] wrote+validated: /mnt/d/community-detection/data/processed/data_cleared/lbsn2vec/feat_df.parquet | rows=47,389 cols=44\n",
      "\n",
      "[DONE] data_cleared/lbsn2vec ready: /mnt/d/community-detection/data/processed/data_cleared/lbsn2vec\n",
      "[OK] wrote+validated: /mnt/d/community-detection/data/processed/data_cleared/brightkite/edges_clean.parquet | rows=214,078 cols=2\n",
      "[OK] wrote+validated: /mnt/d/community-detection/data/processed/data_cleared/brightkite/edges_final.parquet | rows=116,506 cols=2\n",
      "[OK] wrote+validated: /mnt/d/community-detection/data/processed/data_cleared/brightkite/checkins_clean.parquet | rows=4,747,172 cols=5\n",
      "[OK] wrote+validated: /mnt/d/community-detection/data/processed/data_cleared/brightkite/checkins_final.parquet | rows=3,656,191 cols=5\n",
      "[OK] wrote+validated: /mnt/d/community-detection/data/processed/data_cleared/brightkite/users_final.parquet | rows=15,092 cols=1\n",
      "[OK] wrote+validated: /mnt/d/community-detection/data/processed/data_cleared/brightkite/feat_df.parquet | rows=15,092 cols=44\n",
      "\n",
      "[DONE] data_cleared/brightkite ready: /mnt/d/community-detection/data/processed/data_cleared/brightkite\n"
     ]
    }
   ],
   "source": [
    "def save_step0_4_artifacts(dataset: str,\n",
    "                           users_final: pd.DataFrame,\n",
    "                           edges_clean: pd.DataFrame,\n",
    "                           edges_final: pd.DataFrame,\n",
    "                           checkins_clean: pd.DataFrame,\n",
    "                           checkins_final: pd.DataFrame,\n",
    "                           X_users: np.ndarray,\n",
    "                           feat_df: pd.DataFrame):\n",
    "    out_dir = DATA_CLEARED_DIR / dataset\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Parquet (write via Linux tmp -> copy -> validate)\n",
    "    parquet_write_safe(edges_clean,   out_dir / \"edges_clean.parquet\")\n",
    "    parquet_write_safe(edges_final,   out_dir / \"edges_final.parquet\")\n",
    "    parquet_write_safe(checkins_clean, out_dir / \"checkins_clean.parquet\")\n",
    "    parquet_write_safe(checkins_final, out_dir / \"checkins_final.parquet\")\n",
    "    parquet_write_safe(users_final,   out_dir / \"users_final.parquet\")\n",
    "\n",
    "    # NPY (rất ổn định, ít rủi ro hơn parquet)\n",
    "    np.save(out_dir / \"X_users.npy\", X_users)\n",
    "\n",
    "    # feat_df: nếu bạn vẫn muốn lưu parquet, giữ như dưới (đã safe write)\n",
    "    parquet_write_safe(feat_df.reset_index(), out_dir / \"feat_df.parquet\")\n",
    "\n",
    "    # lưu config nhỏ để trace\n",
    "    meta = {\"dataset\": dataset, \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"), \"cfg\": CFG}\n",
    "    with open(out_dir / \"data_cleared_meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\n[DONE] data_cleared/{dataset} ready:\", out_dir)\n",
    "\n",
    "save_step0_4_artifacts(\n",
    "    \"lbsn2vec\",\n",
    "    users_final_lbsn, edges_clean_lbsn, edges_final_lbsn, checkins_clean_lbsn, checkins_final_lbsn,\n",
    "    X_lbsn, feat_lbsn\n",
    ")\n",
    "\n",
    "save_step0_4_artifacts(\n",
    "    \"brightkite\",\n",
    "    users_final_bk, edges_clean_bk, edges_final_bk, checkins_clean_bk, checkins_final_bk,\n",
    "    X_bk, feat_bk\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "399e1467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== VERIFY lbsn2vec ===\n",
      "edges_clean.parquet -> OK\n",
      "edges_final.parquet -> OK\n",
      "checkins_clean.parquet -> OK\n",
      "checkins_final.parquet -> OK\n",
      "users_final.parquet -> OK\n",
      "feat_df.parquet -> OK\n",
      "X_users.npy -> (47389, 43) float32\n",
      "\n",
      "=== VERIFY brightkite ===\n",
      "edges_clean.parquet -> OK\n",
      "edges_final.parquet -> OK\n",
      "checkins_clean.parquet -> OK\n",
      "checkins_final.parquet -> OK\n",
      "users_final.parquet -> OK\n",
      "feat_df.parquet -> OK\n",
      "X_users.npy -> (15092, 43) float32\n"
     ]
    }
   ],
   "source": [
    "def verify_cleared(dataset: str):\n",
    "    base = DATA_CLEARED_DIR / dataset\n",
    "    print(\"\\n=== VERIFY\", dataset, \"===\")\n",
    "\n",
    "    for name in [\"edges_clean.parquet\",\"edges_final.parquet\",\"checkins_clean.parquet\",\"checkins_final.parquet\",\"users_final.parquet\",\"feat_df.parquet\"]:\n",
    "        p = base / name\n",
    "        ok, msg = parquet_validate(p)\n",
    "        print(name, \"->\", \"OK\" if ok else f\"FAIL ({msg})\")\n",
    "\n",
    "    x = np.load(base / \"X_users.npy\")\n",
    "    print(\"X_users.npy ->\", x.shape, x.dtype)\n",
    "\n",
    "verify_cleared(\"lbsn2vec\")\n",
    "verify_cleared(\"brightkite\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
