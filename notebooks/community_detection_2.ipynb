{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7e44541",
   "metadata": {},
   "source": [
    "## Imports & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecedc296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Environment Info ===\n",
      "Python        : 3.11.8\n",
      "Platform      : Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39\n",
      "NumPy         : 2.3.5\n",
      "Pandas        : 2.3.3\n",
      "PyYAML        : OK\n",
      "PyTorch       : 2.9.0+cu126\n",
      "CUDA available: True\n",
      "CUDA device   : NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "CUDA version  : 12.6\n",
      "========================\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import logging\n",
    "import platform\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional, Tuple, List, Literal, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "try:\n",
    "    import yaml\n",
    "    _HAS_YAML = True\n",
    "except Exception:\n",
    "    _HAS_YAML = False\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    _HAS_TORCH = True\n",
    "except Exception:\n",
    "    torch = None\n",
    "    _HAS_TORCH = False\n",
    "\n",
    "def print_env_info() -> None:\n",
    "    print(\"=== Environment Info ===\")\n",
    "    print(f\"Python        : {sys.version.split()[0]}\")\n",
    "    print(f\"Platform      : {platform.platform()}\")\n",
    "    print(f\"NumPy         : {np.__version__}\")\n",
    "    print(f\"Pandas        : {pd.__version__}\")\n",
    "    print(f\"PyYAML        : {'OK' if _HAS_YAML else 'NOT INSTALLED'}\")\n",
    "    if _HAS_TORCH:\n",
    "        print(f\"PyTorch       : {torch.__version__}\")\n",
    "        cuda_ok = torch.cuda.is_available()\n",
    "        print(f\"CUDA available: {cuda_ok}\")\n",
    "        if cuda_ok:\n",
    "            print(f\"CUDA device   : {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"CUDA version  : {torch.version.cuda}\")\n",
    "    else:\n",
    "        print(\"PyTorch       : NOT INSTALLED\")\n",
    "    print(\"========================\")\n",
    "\n",
    "print_env_info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2bcfb9",
   "metadata": {},
   "source": [
    "## Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ea1e6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Paths]\n",
      "  CWD          : notebooks\n",
      "  PROJECT_ROOT : community-detection\n",
      "  CONFIG_PATH  : configs/default.yaml\n",
      "[Config] YAML not found, using notebook defaults: configs/default.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'project': {'name': 'community-detection',\n",
       "  'data_dir': 'data',\n",
       "  'processed_dir': 'data/processed',\n",
       "  'artifacts_format': 'parquet'},\n",
       " 'run': {'seed': 42,\n",
       "  'log_level': 'INFO',\n",
       "  'save_run_config': True,\n",
       "  'overwrite': False},\n",
       " 'preprocess': {'min_checkins': 10,\n",
       "  'min_degree': 3,\n",
       "  'iterative_filter': True,\n",
       "  'drop_self_loops': True,\n",
       "  'dedup_edges': True,\n",
       "  'enforce_undirected': True,\n",
       "  'lat_range': [-90.0, 90.0],\n",
       "  'lon_range': [-180.0, 180.0]},\n",
       " 'features': {'use_spatial': True,\n",
       "  'use_temporal': True,\n",
       "  'use_venue': True,\n",
       "  'log1p_counts': True,\n",
       "  'standardize': True},\n",
       " 'model': {'encoder': 'graphsage',\n",
       "  'num_layers': 2,\n",
       "  'hidden_dim': 128,\n",
       "  'embed_dim': 128,\n",
       "  'neighbor_sampling': [25, 10]},\n",
       " 'train': {'epochs': 10,\n",
       "  'lr': 0.001,\n",
       "  'batch_size': 1024,\n",
       "  'num_negative': 5,\n",
       "  'positive_strategy': 'random_walk',\n",
       "  'rw_length': 10,\n",
       "  'rw_window': 5},\n",
       " 'community': {'similarity': 'cosine',\n",
       "  'knn_k': 30,\n",
       "  'mutual_knn': True,\n",
       "  'clip_negative_weights': True,\n",
       "  'leiden_resolution': 1.0},\n",
       " 'metrics': {'random_baseline_runs': 10,\n",
       "  'distance_metric': 'haversine',\n",
       "  'aggregate': 'median'},\n",
       " 'datasets': {'active': ['brightkite', 'gowalla', 'lbsn2vec'],\n",
       "  'lbsn2vec_snapshot': 'old',\n",
       "  'lbsn2vec_tier': 'curated'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def find_repo_root(start: Optional[Path] = None) -> Path:\n",
    "\n",
    "    env_root = os.environ.get(\"CD_PROJECT_ROOT\")\n",
    "    if env_root:\n",
    "        p = Path(env_root).expanduser().resolve()\n",
    "        if p.exists():\n",
    "            return p\n",
    "        raise FileNotFoundError(f\"CD_PROJECT_ROOT is set but does not exist: {env_root}\")\n",
    "\n",
    "    if start is None:\n",
    "        start = Path.cwd()\n",
    "\n",
    "    start = start.resolve()\n",
    "    for p in [start] + list(start.parents):\n",
    "        if (p / \"pyproject.toml\").exists():\n",
    "            return p\n",
    "        if (p / \"configs\" / \"default.yaml\").exists():\n",
    "            return p\n",
    "        if (p / \"README.md\").exists():\n",
    "            return p\n",
    "\n",
    "    return start  # fallback\n",
    "\n",
    "PROJECT_ROOT = find_repo_root()\n",
    "CONFIG_PATH_DEFAULT = PROJECT_ROOT / \"configs\" / \"default.yaml\"\n",
    "\n",
    "def _rel(p: Path) -> str:\n",
    "    try:\n",
    "        return str(p.resolve().relative_to(PROJECT_ROOT.resolve()))\n",
    "    except Exception:\n",
    "        return str(p)\n",
    "\n",
    "print(\"[Paths]\")\n",
    "print(\"  CWD          :\", _rel(Path.cwd()))\n",
    "print(\"  PROJECT_ROOT :\", PROJECT_ROOT.name)\n",
    "print(\"  CONFIG_PATH  :\", _rel(CONFIG_PATH_DEFAULT))\n",
    "\n",
    "def load_config(config_path: Optional[Path] = None) -> Dict[str, Any]:\n",
    "    cfg: Dict[str, Any] = {\n",
    "        \"project\": {\n",
    "            \"name\": \"community-detection\",\n",
    "            \"data_dir\": \"data\",\n",
    "            \"processed_dir\": \"data/processed\",\n",
    "            \"artifacts_format\": \"parquet\",\n",
    "        },\n",
    "        \"run\": {\n",
    "            \"seed\": 42,\n",
    "            \"log_level\": \"INFO\",\n",
    "            \"save_run_config\": True,\n",
    "            \"overwrite\": False,\n",
    "        },\n",
    "        \"preprocess\": {\n",
    "            \"min_checkins\": 10,\n",
    "            \"min_degree\": 3,\n",
    "            \"iterative_filter\": True,\n",
    "            \"drop_self_loops\": True,\n",
    "            \"dedup_edges\": True,\n",
    "            \"enforce_undirected\": True,\n",
    "            \"lat_range\": [-90.0, 90.0],\n",
    "            \"lon_range\": [-180.0, 180.0],\n",
    "        },\n",
    "        \"features\": {\n",
    "            \"use_spatial\": True,\n",
    "            \"use_temporal\": True,\n",
    "            \"use_venue\": True,\n",
    "            \"log1p_counts\": True,\n",
    "            \"standardize\": True,\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"encoder\": \"graphsage\",\n",
    "            \"num_layers\": 2,\n",
    "            \"hidden_dim\": 128,\n",
    "            \"embed_dim\": 128,\n",
    "            \"neighbor_sampling\": [25, 10],\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"epochs\": 10,\n",
    "            \"lr\": 1e-3,\n",
    "            \"batch_size\": 1024,\n",
    "            \"num_negative\": 5,\n",
    "            \"positive_strategy\": \"random_walk\",\n",
    "            \"rw_length\": 10,\n",
    "            \"rw_window\": 5,\n",
    "        },\n",
    "        \"community\": {\n",
    "            \"similarity\": \"cosine\",\n",
    "            \"knn_k\": 30,\n",
    "            \"mutual_knn\": True,\n",
    "            \"clip_negative_weights\": True,\n",
    "            \"leiden_resolution\": 1.0,\n",
    "        },\n",
    "        \"metrics\": {\n",
    "            \"random_baseline_runs\": 10,\n",
    "            \"distance_metric\": \"haversine\",\n",
    "            \"aggregate\": \"median\",\n",
    "        },\n",
    "        \"datasets\": {\n",
    "            \"active\": [\"brightkite\", \"gowalla\", \"lbsn2vec\"],\n",
    "            \"lbsn2vec_snapshot\": \"old\",\n",
    "            \"lbsn2vec_tier\": \"curated\",\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if config_path is None:\n",
    "        config_path = CONFIG_PATH_DEFAULT\n",
    "\n",
    "    if config_path.exists():\n",
    "        if not _HAS_YAML:\n",
    "            raise RuntimeError(\"configs/default.yaml exists but PyYAML is not installed. Install pyyaml.\")\n",
    "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            y = yaml.safe_load(f) or {}\n",
    "\n",
    "        def _deep_update(d: Dict[str, Any], u: Dict[str, Any]) -> Dict[str, Any]:\n",
    "            for k, v in u.items():\n",
    "                if isinstance(v, dict) and isinstance(d.get(k), dict):\n",
    "                    _deep_update(d[k], v)\n",
    "                else:\n",
    "                    d[k] = v\n",
    "            return d\n",
    "\n",
    "        cfg = _deep_update(cfg, y)\n",
    "        print(f\"[Config] Loaded YAML: {_rel(config_path)}\")\n",
    "    else:\n",
    "        print(f\"[Config] YAML not found, using notebook defaults: {_rel(config_path)}\")\n",
    "\n",
    "    return cfg\n",
    "\n",
    "CFG = load_config()\n",
    "CFG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd4bb5c",
   "metadata": {},
   "source": [
    "## Dataset Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81d9fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Derived Paths]\n",
      "  DATA_DIR      : data\n",
      "  PROCESSED_DIR : data/processed\n",
      "=== Dataset Registry Validation ===\n",
      "- brightkite: root=data/Brightkite\n",
      "  [OK] Brightkite_edges.txt\n",
      "  [OK] Brightkite_totalCheckins.txt\n",
      "- gowalla: root=data/Gowalla\n",
      "  [OK] Gowalla_edges.txt\n",
      "  [OK] Gowalla_totalCheckins.txt\n",
      "- lbsn2vec: root=data/LBSN2Vec\n",
      "  [OK] dataset_WWW_friendship_old.txt\n",
      "  [OK] dataset_WWW_friendship_new.txt\n",
      "  [OK] dataset_WWW_Checkins_anonymized.txt\n",
      "  [OK] dataset_WWW_readme.txt\n",
      "  [OK] raw_POIs.txt\n",
      "  [OK] raw_Checkins_anonymized.txt\n",
      "[OK] Dataset folders/files look good.\n",
      "===================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'brightkite': DatasetSpec(name='brightkite', root=PosixPath('/mnt/d/community-detection/data/Brightkite'), edges_path=PosixPath('/mnt/d/community-detection/data/Brightkite/Brightkite_edges.txt'), checkins_path=PosixPath('/mnt/d/community-detection/data/Brightkite/Brightkite_totalCheckins.txt'), friendship_old_path=None, friendship_new_path=None, readme_path=None, poi_path=None, raw_checkins_path=None, source='SNAP', tier=None, snapshot=None),\n",
       " 'gowalla': DatasetSpec(name='gowalla', root=PosixPath('/mnt/d/community-detection/data/Gowalla'), edges_path=PosixPath('/mnt/d/community-detection/data/Gowalla/Gowalla_edges.txt'), checkins_path=PosixPath('/mnt/d/community-detection/data/Gowalla/Gowalla_totalCheckins.txt'), friendship_old_path=None, friendship_new_path=None, readme_path=None, poi_path=None, raw_checkins_path=None, source='SNAP', tier=None, snapshot=None),\n",
       " 'lbsn2vec': DatasetSpec(name='lbsn2vec', root=PosixPath('/mnt/d/community-detection/data/LBSN2Vec'), edges_path=None, checkins_path=PosixPath('/mnt/d/community-detection/data/LBSN2Vec/dataset_WWW_Checkins_anonymized.txt'), friendship_old_path=PosixPath('/mnt/d/community-detection/data/LBSN2Vec/dataset_WWW_friendship_old.txt'), friendship_new_path=PosixPath('/mnt/d/community-detection/data/LBSN2Vec/dataset_WWW_friendship_new.txt'), readme_path=PosixPath('/mnt/d/community-detection/data/LBSN2Vec/dataset_WWW_readme.txt'), poi_path=PosixPath('/mnt/d/community-detection/data/LBSN2Vec/raw_POIs.txt'), raw_checkins_path=PosixPath('/mnt/d/community-detection/data/LBSN2Vec/raw_Checkins_anonymized.txt'), source='LBSN2Vec', tier='curated', snapshot='old')}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_DIR = PROJECT_ROOT / CFG[\"project\"][\"data_dir\"]\n",
    "PROCESSED_DIR = PROJECT_ROOT / CFG[\"project\"][\"processed_dir\"]\n",
    "\n",
    "print(\"[Derived Paths]\")\n",
    "print(\"  DATA_DIR      :\", _rel(DATA_DIR))\n",
    "print(\"  PROCESSED_DIR :\", _rel(PROCESSED_DIR))\n",
    "\n",
    "DatasetName = Literal[\"brightkite\", \"gowalla\", \"lbsn2vec\"]\n",
    "TierName = Literal[\"curated\", \"raw\"]\n",
    "SnapshotName = Literal[\"old\", \"new\"]\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DatasetSpec:\n",
    "    name: DatasetName\n",
    "    root: Path\n",
    "    edges_path: Optional[Path] = None\n",
    "    checkins_path: Optional[Path] = None\n",
    "\n",
    "    # LBSN2Vec++ extras\n",
    "    friendship_old_path: Optional[Path] = None\n",
    "    friendship_new_path: Optional[Path] = None\n",
    "    readme_path: Optional[Path] = None\n",
    "    poi_path: Optional[Path] = None\n",
    "    raw_checkins_path: Optional[Path] = None\n",
    "\n",
    "    # meta flags\n",
    "    source: Literal[\"SNAP\", \"LBSN2Vec\"] = \"SNAP\"\n",
    "    tier: Optional[TierName] = None\n",
    "    snapshot: Optional[SnapshotName] = None\n",
    "\n",
    "def build_dataset_registry(cfg: Dict[str, Any]) -> Dict[DatasetName, DatasetSpec]:\n",
    "    brightkite_root = DATA_DIR / \"Brightkite\"\n",
    "    gowalla_root = DATA_DIR / \"Gowalla\"\n",
    "    lbsn_root = DATA_DIR / \"LBSN2Vec\"\n",
    "\n",
    "    tier: TierName = cfg[\"datasets\"].get(\"lbsn2vec_tier\", \"curated\")\n",
    "    snapshot: SnapshotName = cfg[\"datasets\"].get(\"lbsn2vec_snapshot\", \"old\")\n",
    "\n",
    "    return {\n",
    "        \"brightkite\": DatasetSpec(\n",
    "            name=\"brightkite\",\n",
    "            root=brightkite_root,\n",
    "            edges_path=brightkite_root / \"Brightkite_edges.txt\",\n",
    "            checkins_path=brightkite_root / \"Brightkite_totalCheckins.txt\",\n",
    "            source=\"SNAP\",\n",
    "        ),\n",
    "        \"gowalla\": DatasetSpec(\n",
    "            name=\"gowalla\",\n",
    "            root=gowalla_root,\n",
    "            edges_path=gowalla_root / \"Gowalla_edges.txt\",\n",
    "            checkins_path=gowalla_root / \"Gowalla_totalCheckins.txt\",\n",
    "            source=\"SNAP\",\n",
    "        ),\n",
    "        \"lbsn2vec\": DatasetSpec(\n",
    "            name=\"lbsn2vec\",\n",
    "            root=lbsn_root,\n",
    "            friendship_old_path=lbsn_root / \"dataset_WWW_friendship_old.txt\",\n",
    "            friendship_new_path=lbsn_root / \"dataset_WWW_friendship_new.txt\",\n",
    "            readme_path=lbsn_root / \"dataset_WWW_readme.txt\",\n",
    "            checkins_path=lbsn_root / \"dataset_WWW_Checkins_anonymized.txt\",\n",
    "            raw_checkins_path=lbsn_root / \"raw_Checkins_anonymized.txt\",\n",
    "            poi_path=lbsn_root / \"raw_POIs.txt\",\n",
    "            source=\"LBSN2Vec\",\n",
    "            tier=tier,\n",
    "            snapshot=snapshot,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "DATASETS = build_dataset_registry(CFG)\n",
    "\n",
    "def validate_dataset_files(datasets: Dict[DatasetName, DatasetSpec]) -> None:\n",
    "    print(\"=== Dataset Registry Validation ===\")\n",
    "    ok = True\n",
    "\n",
    "    if not DATA_DIR.exists():\n",
    "        print(f\"[FAIL] Missing DATA_DIR: {_rel(DATA_DIR)}\")\n",
    "        ok = False\n",
    "\n",
    "    for name, spec in datasets.items():\n",
    "        print(f\"- {name}: root={_rel(spec.root)}\")\n",
    "        if not spec.root.exists():\n",
    "            print(f\"  [FAIL] Missing folder: {_rel(spec.root)}\")\n",
    "            ok = False\n",
    "            continue\n",
    "\n",
    "        if spec.source == \"SNAP\":\n",
    "            for p in [spec.edges_path, spec.checkins_path]:\n",
    "                if p is None or not p.exists():\n",
    "                    print(f\"  [FAIL] Missing file: {_rel(p) if p else p}\")\n",
    "                    ok = False\n",
    "                else:\n",
    "                    print(f\"  [OK] {p.name}\")\n",
    "\n",
    "        if spec.source == \"LBSN2Vec\":\n",
    "            for p in [spec.friendship_old_path, spec.friendship_new_path, spec.checkins_path]:\n",
    "                if p is None or not p.exists():\n",
    "                    print(f\"  [FAIL] Missing file: {_rel(p) if p else p}\")\n",
    "                    ok = False\n",
    "                else:\n",
    "                    print(f\"  [OK] {p.name}\")\n",
    "\n",
    "            for p in [spec.readme_path, spec.poi_path, spec.raw_checkins_path]:\n",
    "                if p is None or not p.exists():\n",
    "                    print(f\"  [i] Optional missing: {_rel(p) if p else p}\")\n",
    "                else:\n",
    "                    print(f\"  [OK] {p.name}\")\n",
    "\n",
    "    if ok:\n",
    "        print(\"[OK] Dataset folders/files look good.\")\n",
    "    else:\n",
    "        print(\"[WARN] Dataset registry has missing paths. Fix before continuing.\")\n",
    "    print(\"===================================\")\n",
    "\n",
    "validate_dataset_files(DATASETS)\n",
    "DATASETS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7fddde",
   "metadata": {},
   "source": [
    "## Seed + Logging + Output Dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26b76911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:38:03] INFO - PROJECT_ROOT : .\n",
      "[05:38:03] INFO - DATA_DIR     : data\n",
      "[05:38:03] INFO - Seed set to  : 42\n",
      "[05:38:03] INFO - Processed dir: data/processed\n",
      "[05:38:03] INFO - Run dir      : data/processed/_runs/20251214_053803\n",
      "[05:38:03] INFO - Saved run config to: data/processed/_runs/20251214_053803/run_config.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def set_global_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    if _HAS_TORCH:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def setup_logging(log_level: str = \"INFO\") -> logging.Logger:\n",
    "    logger = logging.getLogger(\"osnclusters\")\n",
    "    logger.setLevel(getattr(logging, log_level.upper(), logging.INFO))\n",
    "    logger.handlers.clear()\n",
    "\n",
    "    ch = logging.StreamHandler(stream=sys.stdout)\n",
    "    ch.setLevel(getattr(logging, log_level.upper(), logging.INFO))\n",
    "    fmt = logging.Formatter(\"[%(asctime)s] %(levelname)s - %(message)s\", datefmt=\"%H:%M:%S\")\n",
    "    ch.setFormatter(fmt)\n",
    "    logger.addHandler(ch)\n",
    "    logger.propagate = False\n",
    "    return logger\n",
    "\n",
    "def ensure_dirs() -> Dict[str, Path]:\n",
    "    PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    run_id = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_dir = PROCESSED_DIR / \"_runs\" / run_id\n",
    "    run_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return {\"processed\": PROCESSED_DIR, \"run_dir\": run_dir}\n",
    "\n",
    "set_global_seed(int(CFG[\"run\"][\"seed\"]))\n",
    "LOGGER = setup_logging(CFG[\"run\"][\"log_level\"])\n",
    "DIRS = ensure_dirs()\n",
    "\n",
    "LOGGER.info(f\"PROJECT_ROOT : {_rel(PROJECT_ROOT)}\")\n",
    "LOGGER.info(f\"DATA_DIR     : {_rel(DATA_DIR)}\")\n",
    "LOGGER.info(f\"Seed set to  : {CFG['run']['seed']}\")\n",
    "LOGGER.info(f\"Processed dir: {_rel(DIRS['processed'])}\")\n",
    "LOGGER.info(f\"Run dir      : {_rel(DIRS['run_dir'])}\")\n",
    "\n",
    "if CFG[\"run\"].get(\"save_run_config\", True):\n",
    "    out = DIRS[\"run_dir\"] / \"run_config.json\"\n",
    "    with open(out, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(CFG, f, indent=2, ensure_ascii=False)\n",
    "    LOGGER.info(f\"Saved run config to: {_rel(out)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b41a42",
   "metadata": {},
   "source": [
    "## Data Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df984bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges schema ready: ['u', 'v']\n",
      "Checkins schema ready: ['user_id', 'ts', 'lat', 'lon'] + optional ['venue_id', 'category']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@dataclass(frozen=True)\n",
    "class EdgesSchema:\n",
    "    u: str\n",
    "    v: str\n",
    "    weight: Optional[float] = None\n",
    "\n",
    "EDGES_COLUMNS = [\"u\", \"v\"]\n",
    "\n",
    "def enforce_edges_schema(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    missing = [c for c in EDGES_COLUMNS if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Edges missing columns: {missing}. Found: {list(df.columns)}\")\n",
    "    out = df[EDGES_COLUMNS].copy()\n",
    "    out[\"u\"] = out[\"u\"].astype(str)\n",
    "    out[\"v\"] = out[\"v\"].astype(str)\n",
    "    return out\n",
    "\n",
    "print(\"Edges schema ready:\", EDGES_COLUMNS)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class CheckinsSchema:\n",
    "    user_id: str\n",
    "    ts: \"pd.Timestamp\"\n",
    "    lat: float\n",
    "    lon: float\n",
    "    venue_id: Optional[str] = None\n",
    "    category: Optional[str] = None\n",
    "\n",
    "CHECKINS_REQUIRED = [\"user_id\", \"ts\", \"lat\", \"lon\"]\n",
    "CHECKINS_OPTIONAL = [\"venue_id\", \"category\"]\n",
    "\n",
    "def enforce_checkins_schema(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    missing = [c for c in CHECKINS_REQUIRED if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Checkins missing columns: {missing}. Found: {list(df.columns)}\")\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"user_id\"] = out[\"user_id\"].astype(str)\n",
    "    out[\"ts\"] = pd.to_datetime(out[\"ts\"], errors=\"coerce\")\n",
    "    out[\"lat\"] = pd.to_numeric(out[\"lat\"], errors=\"coerce\")\n",
    "    out[\"lon\"] = pd.to_numeric(out[\"lon\"], errors=\"coerce\")\n",
    "\n",
    "    for c in CHECKINS_OPTIONAL:\n",
    "        if c in out.columns:\n",
    "            out[c] = out[c].astype(str)\n",
    "\n",
    "    return out\n",
    "\n",
    "print(\"Checkins schema ready:\", CHECKINS_REQUIRED, \"+ optional\", CHECKINS_OPTIONAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acae48f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>source</th>\n",
       "      <th>edges_format</th>\n",
       "      <th>checkins_format</th>\n",
       "      <th>notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brightkite</td>\n",
       "      <td>SNAP</td>\n",
       "      <td>Edge list (2 cols): user_id_1 user_id_2 (white...</td>\n",
       "      <td>SNAP checkins: user_id, timestamp, lat, lon, l...</td>\n",
       "      <td>Verify column order by reading first lines; SN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gowalla</td>\n",
       "      <td>SNAP</td>\n",
       "      <td>Edge list (2 cols): user_id_1 user_id_2 (white...</td>\n",
       "      <td>SNAP checkins: user_id, timestamp, lat, lon, l...</td>\n",
       "      <td>Gowalla is larger; use chunked read later if n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lbsn2vec</td>\n",
       "      <td>LBSN2Vec</td>\n",
       "      <td>Friendship snapshots old/new: edge list (u v)</td>\n",
       "      <td>Curated checkins (observed 9 cols): user_id ve...</td>\n",
       "      <td>Curated checkins need timestamp rebuild and PO...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dataset    source                                       edges_format  \\\n",
       "0  brightkite      SNAP  Edge list (2 cols): user_id_1 user_id_2 (white...   \n",
       "1     gowalla      SNAP  Edge list (2 cols): user_id_1 user_id_2 (white...   \n",
       "2    lbsn2vec  LBSN2Vec      Friendship snapshots old/new: edge list (u v)   \n",
       "\n",
       "                                     checkins_format  \\\n",
       "0  SNAP checkins: user_id, timestamp, lat, lon, l...   \n",
       "1  SNAP checkins: user_id, timestamp, lat, lon, l...   \n",
       "2  Curated checkins (observed 9 cols): user_id ve...   \n",
       "\n",
       "                                               notes  \n",
       "0  Verify column order by reading first lines; SN...  \n",
       "1  Gowalla is larger; use chunked read later if n...  \n",
       "2  Curated checkins need timestamp rebuild and PO...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "@dataclass(frozen=True)\n",
    "class ParserRule:\n",
    "    dataset: DatasetName\n",
    "    source: Literal[\"SNAP\", \"LBSN2Vec\"]\n",
    "    edges_format: str\n",
    "    checkins_format: str\n",
    "    notes: str\n",
    "\n",
    "PARSER_RULES: Dict[DatasetName, ParserRule] = {\n",
    "    \"brightkite\": ParserRule(\n",
    "        dataset=\"brightkite\",\n",
    "        source=\"SNAP\",\n",
    "        edges_format=\"Edge list (2 cols): user_id_1 user_id_2 (whitespace-delimited)\",\n",
    "        checkins_format=\"SNAP checkins: user_id, timestamp, lat, lon, location_id (often 5 cols)\",\n",
    "        notes=\"Verify column order by reading first lines; SNAP formats can differ.\"\n",
    "    ),\n",
    "    \"gowalla\": ParserRule(\n",
    "        dataset=\"gowalla\",\n",
    "        source=\"SNAP\",\n",
    "        edges_format=\"Edge list (2 cols): user_id_1 user_id_2 (whitespace-delimited)\",\n",
    "        checkins_format=\"SNAP checkins: user_id, timestamp, lat, lon, location_id (often 5 cols)\",\n",
    "        notes=\"Gowalla is larger; use chunked read later if needed.\"\n",
    "    ),\n",
    "    \"lbsn2vec\": ParserRule(\n",
    "        dataset=\"lbsn2vec\",\n",
    "        source=\"LBSN2Vec\",\n",
    "        edges_format=\"Friendship snapshots old/new: edge list (u v)\",\n",
    "        checkins_format=\"Curated checkins (observed 9 cols): user_id venue_id + tokenized timestamp; lat/lon from POIs join\",\n",
    "        notes=\"Curated checkins need timestamp rebuild and POI join for lat/lon.\"\n",
    "    ),\n",
    "}\n",
    "\n",
    "def preview_text_file(path: Path, n: int = 5) -> None:\n",
    "    print(f\"--- Preview: {path.name} ---\")\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        for _ in range(n):\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            print(line.rstrip(\"\\n\"))\n",
    "    print(\"--------------------------\")\n",
    "\n",
    "def parse_snap_edges(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=r\"\\s+\", header=None, names=[\"u\", \"v\"], dtype=str, engine=\"python\")\n",
    "    return enforce_edges_schema(df)\n",
    "\n",
    "def parse_snap_checkins(path: Path, column_guess: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    df0 = pd.read_csv(path, sep=r\"\\s+\", header=None, dtype=str, engine=\"python\")\n",
    "\n",
    "    if column_guess is None:\n",
    "        if df0.shape[1] >= 5:\n",
    "            column_guess = [\"user_id\", \"ts\", \"lat\", \"lon\", \"venue_id\"] + [f\"extra_{i}\" for i in range(df0.shape[1] - 5)]\n",
    "        else:\n",
    "            column_guess = [\"user_id\", \"ts\", \"lat\", \"lon\"] + [f\"extra_{i}\" for i in range(df0.shape[1] - 4)]\n",
    "\n",
    "    df0.columns = column_guess[:df0.shape[1]]\n",
    "    keep = [c for c in [\"user_id\", \"ts\", \"lat\", \"lon\", \"venue_id\"] if c in df0.columns]\n",
    "    df = df0[keep].copy()\n",
    "    return enforce_checkins_schema(df)\n",
    "\n",
    "def parse_lbsn_friendship(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=r\"\\s+\", header=None, names=[\"u\", \"v\"], dtype=str, engine=\"python\")\n",
    "    return enforce_edges_schema(df)\n",
    "\n",
    "def parse_lbsn_pois(poi_path: Path) -> pd.DataFrame:\n",
    "    poi0 = pd.read_csv(poi_path, sep=r\"\\s+\", header=None, dtype=str, engine=\"python\")\n",
    "    if poi0.shape[1] < 3:\n",
    "        raise ValueError(f\"Unexpected POIs cols: {poi0.shape[1]} (need >=3)\")\n",
    "    poi0 = poi0.rename(columns={0: \"venue_id\", 1: \"lat\", 2: \"lon\"})\n",
    "    poi = poi0[[\"venue_id\", \"lat\", \"lon\"]].copy()\n",
    "    poi[\"venue_id\"] = poi[\"venue_id\"].astype(str)\n",
    "    poi[\"lat\"] = pd.to_numeric(poi[\"lat\"], errors=\"coerce\")\n",
    "    poi[\"lon\"] = pd.to_numeric(poi[\"lon\"], errors=\"coerce\")\n",
    "    return poi\n",
    "\n",
    "def parse_lbsn_checkins_curated(checkins_path: Path, poi_path: Optional[Path] = None) -> pd.DataFrame:\n",
    "    df0 = pd.read_csv(checkins_path, sep=r\"\\s+\", header=None, dtype=str, engine=\"python\")\n",
    "\n",
    "    if df0.shape[1] != 9:\n",
    "        raise ValueError(f\"Unexpected curated checkins column count: {df0.shape[1]} (expected 9).\")\n",
    "\n",
    "    df0 = df0.iloc[:, :9].copy()\n",
    "    df0.columns = [\"user_id\", \"venue_id\", \"wday\", \"mon\", \"day\", \"time\", \"tz\", \"year\", \"utc_offset_min\"]\n",
    "\n",
    "    ts_raw = (\n",
    "        df0[\"wday\"].astype(str) + \" \" +\n",
    "        df0[\"mon\"].astype(str) + \" \" +\n",
    "        df0[\"day\"].astype(str) + \" \" +\n",
    "        df0[\"time\"].astype(str) + \" \" +\n",
    "        df0[\"tz\"].astype(str) + \" \" +\n",
    "        df0[\"year\"].astype(str)\n",
    "    )\n",
    "    df0[\"ts\"] = pd.to_datetime(ts_raw, errors=\"coerce\", format=\"%a %b %d %H:%M:%S %z %Y\")\n",
    "\n",
    "\n",
    "    df0[\"lat\"] = np.nan\n",
    "    df0[\"lon\"] = np.nan\n",
    "\n",
    "    if poi_path is not None and poi_path.exists():\n",
    "        poi = parse_lbsn_pois(poi_path)\n",
    "        df0 = df0.merge(poi, on=\"venue_id\", how=\"left\", suffixes=(\"\", \"_poi\"))\n",
    "        if \"lat_poi\" in df0.columns: df0[\"lat\"] = df0[\"lat_poi\"]\n",
    "        if \"lon_poi\" in df0.columns: df0[\"lon\"] = df0[\"lon_poi\"]\n",
    "\n",
    "    df = df0[[\"user_id\", \"ts\", \"lat\", \"lon\", \"venue_id\"]].copy()\n",
    "    return enforce_checkins_schema(df)\n",
    "\n",
    "def parse_lbsn_checkins_raw(checkins_path: Path, poi_path: Path) -> pd.DataFrame:\n",
    "    chk0 = pd.read_csv(checkins_path, sep=r\"\\s+\", header=None, dtype=str, engine=\"python\")\n",
    "    if chk0.shape[1] < 3:\n",
    "        raise ValueError(f\"Unexpected raw checkins cols: {chk0.shape[1]} (need >=3)\")\n",
    "\n",
    "    user = chk0.iloc[:, 0].astype(str)\n",
    "    venue = chk0.iloc[:, 1].astype(str)\n",
    "    ts_tokens = chk0.iloc[:, 2:].astype(str).agg(\" \".join, axis=1)\n",
    "    ts = pd.to_datetime(ts_tokens, errors=\"coerce\")\n",
    "\n",
    "    poi = parse_lbsn_pois(poi_path)\n",
    "    df0 = pd.DataFrame({\"user_id\": user, \"venue_id\": venue, \"ts\": ts})\n",
    "    df0 = df0.merge(poi, on=\"venue_id\", how=\"left\")\n",
    "\n",
    "    df = df0[[\"user_id\", \"ts\", \"lat\", \"lon\", \"venue_id\"]].copy()\n",
    "    return enforce_checkins_schema(df)\n",
    "\n",
    "pd.DataFrame([asdict(v) for v in PARSER_RULES.values()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d67e3b",
   "metadata": {},
   "source": [
    "## Validation utils + registry check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "551e3fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1759/2094987576.py:123: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  ts = pd.to_datetime(tmp[\"ts\"], errors=\"coerce\")\n",
      "/tmp/ipykernel_1759/2094987576.py:123: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  ts = pd.to_datetime(tmp[\"ts\"], errors=\"coerce\")\n",
      "/tmp/ipykernel_1759/2094987576.py:76: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  ts = pd.to_datetime(tmp[\"ts\"], errors=\"coerce\")\n",
      "/tmp/ipykernel_1759/2094987576.py:76: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  ts = pd.to_datetime(tmp[\"ts\"], errors=\"coerce\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>source</th>\n",
       "      <th>root_exists</th>\n",
       "      <th>root</th>\n",
       "      <th>edges_exists</th>\n",
       "      <th>edges_size</th>\n",
       "      <th>checkins_exists</th>\n",
       "      <th>checkins_size</th>\n",
       "      <th>edges_ok</th>\n",
       "      <th>edges_self_loop_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>readme_exists</th>\n",
       "      <th>readme_size</th>\n",
       "      <th>poi_exists</th>\n",
       "      <th>poi_size</th>\n",
       "      <th>friendship_old_ok</th>\n",
       "      <th>friendship_new_ok</th>\n",
       "      <th>friendship_old_head</th>\n",
       "      <th>friendship_new_head</th>\n",
       "      <th>checkins_needs_poi_join</th>\n",
       "      <th>readme_head</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brightkite</td>\n",
       "      <td>SNAP</td>\n",
       "      <td>True</td>\n",
       "      <td>data/Brightkite</td>\n",
       "      <td>True</td>\n",
       "      <td>4.37 MB</td>\n",
       "      <td>True</td>\n",
       "      <td>364.42 MB</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gowalla</td>\n",
       "      <td>SNAP</td>\n",
       "      <td>True</td>\n",
       "      <td>data/Gowalla</td>\n",
       "      <td>True</td>\n",
       "      <td>21.09 MB</td>\n",
       "      <td>True</td>\n",
       "      <td>376.36 MB</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lbsn2vec</td>\n",
       "      <td>LBSN2Vec</td>\n",
       "      <td>True</td>\n",
       "      <td>data/LBSN2Vec</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>1.85 KB</td>\n",
       "      <td>True</td>\n",
       "      <td>672.12 MB</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>15\\t595326 | 19\\t54</td>\n",
       "      <td>15\\t595326 | 19\\t54</td>\n",
       "      <td>True</td>\n",
       "      <td>This dataset includes long-term (about 22 mont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      dataset    source  root_exists             root edges_exists edges_size  \\\n",
       "0  brightkite      SNAP         True  data/Brightkite         True    4.37 MB   \n",
       "1     gowalla      SNAP         True     data/Gowalla         True   21.09 MB   \n",
       "2    lbsn2vec  LBSN2Vec         True    data/LBSN2Vec          NaN        NaN   \n",
       "\n",
       "  checkins_exists checkins_size edges_ok  edges_self_loop_ratio  ...  \\\n",
       "0            True     364.42 MB     True                    0.0  ...   \n",
       "1            True     376.36 MB     True                    0.0  ...   \n",
       "2             NaN           NaN      NaN                    NaN  ...   \n",
       "\n",
       "   readme_exists readme_size  poi_exists   poi_size  friendship_old_ok  \\\n",
       "0            NaN         NaN         NaN        NaN                NaN   \n",
       "1            NaN         NaN         NaN        NaN                NaN   \n",
       "2           True     1.85 KB        True  672.12 MB               True   \n",
       "\n",
       "   friendship_new_ok  friendship_old_head  friendship_new_head  \\\n",
       "0                NaN                  NaN                  NaN   \n",
       "1                NaN                  NaN                  NaN   \n",
       "2               True  15\\t595326 | 19\\t54  15\\t595326 | 19\\t54   \n",
       "\n",
       "  checkins_needs_poi_join                                        readme_head  \n",
       "0                     NaN                                                NaN  \n",
       "1                     NaN                                                NaN  \n",
       "2                    True  This dataset includes long-term (about 22 mont...  \n",
       "\n",
       "[3 rows x 35 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def _human_size(nbytes: int) -> str:\n",
    "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]\n",
    "    size = float(nbytes)\n",
    "    for u in units:\n",
    "        if size < 1024.0:\n",
    "            return f\"{size:.2f} {u}\"\n",
    "        size /= 1024.0\n",
    "    return f\"{size:.2f} PB\"\n",
    "\n",
    "def file_meta(path: Path) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"exists\": path.exists(),\n",
    "        \"size\": _human_size(path.stat().st_size) if path.exists() else None,\n",
    "        \"path\": _rel(path),\n",
    "        \"name\": path.name,\n",
    "    }\n",
    "\n",
    "def safe_head_lines(path: Path, n: int = 3) -> List[str]:\n",
    "    if path is None or (not path.exists()):\n",
    "        return []\n",
    "    lines = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "        for _ in range(n):\n",
    "            line = f.readline()\n",
    "            if not line:\n",
    "                break\n",
    "            lines.append(line.rstrip(\"\\n\"))\n",
    "    return lines\n",
    "\n",
    "def read_sample_whitespace(path: Path, nrows: int = 50) -> pd.DataFrame:\n",
    "    return pd.read_csv(path, sep=r\"\\s+\", header=None, dtype=str, engine=\"python\", nrows=nrows)\n",
    "\n",
    "def validate_edges_df(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    report = {\"ok\": True, \"issues\": []}\n",
    "    if df.shape[1] < 2:\n",
    "        report[\"ok\"] = False\n",
    "        report[\"issues\"].append(f\"Edges sample has <2 columns: shape={df.shape}\")\n",
    "        return report\n",
    "    u = df.iloc[:, 0].astype(str)\n",
    "    v = df.iloc[:, 1].astype(str)\n",
    "    report[\"n_rows\"] = len(df)\n",
    "    report[\"n_unique_u\"] = u.nunique()\n",
    "    report[\"n_unique_v\"] = v.nunique()\n",
    "    report[\"self_loop_ratio\"] = float((u == v).mean())\n",
    "    pairs = pd.DataFrame({\"u\": u, \"v\": v})\n",
    "    report[\"dup_ratio_sample\"] = float(pairs.duplicated().mean())\n",
    "    return report\n",
    "\n",
    "def validate_checkins_df(df: pd.DataFrame, dataset: str) -> Dict[str, Any]:\n",
    "    report = {\"ok\": True, \"issues\": [], \"shape\": df.shape}\n",
    "    ncol = df.shape[1]\n",
    "    if ncol < 3:\n",
    "        report[\"ok\"] = False\n",
    "        report[\"issues\"].append(f\"Checkins sample has too few columns: {ncol}\")\n",
    "        return report\n",
    "\n",
    "    candidates = []\n",
    "    if ncol >= 5:\n",
    "        candidates.append((\"A5:user,ts,lat,lon,venue\", [0,1,2,3,4]))\n",
    "    if ncol >= 4:\n",
    "        candidates.append((\"B4:user,ts,lat,lon\", [0,1,2,3]))\n",
    "    if ncol == 9:\n",
    "        candidates.append((\"L9:user,venue,wday,mon,day,time,tz,year,utc_offset\", list(range(9))))\n",
    "    candidates.append((\"C3:user,venue,ts\", [0,1,2]))\n",
    "\n",
    "    best = None\n",
    "    best_score = -1\n",
    "    best_detail = None\n",
    "\n",
    "    for name, idx in candidates:\n",
    "        tmp = df.iloc[:, idx].copy()\n",
    "\n",
    "        if name.startswith((\"A5\", \"B4\")):\n",
    "            tmp.columns = [\"user_id\", \"ts\", \"lat\", \"lon\"] + ([\"venue_id\"] if len(idx) == 5 else [])\n",
    "            user = tmp[\"user_id\"].astype(str)\n",
    "            ts = pd.to_datetime(tmp[\"ts\"], errors=\"coerce\")\n",
    "            lat = pd.to_numeric(tmp[\"lat\"], errors=\"coerce\")\n",
    "            lon = pd.to_numeric(tmp[\"lon\"], errors=\"coerce\")\n",
    "\n",
    "            ts_ok = ts.notna().mean()\n",
    "            lat_ok = lat.notna().mean()\n",
    "            lon_ok = lon.notna().mean()\n",
    "            lat_in = ((lat >= -90) & (lat <= 90)).mean()\n",
    "            lon_in = ((lon >= -180) & (lon <= 180)).mean()\n",
    "\n",
    "            score = ts_ok + lat_ok + lon_ok + lat_in + lon_in\n",
    "            detail = {\n",
    "                \"layout\": name,\n",
    "                \"ts_parse_rate\": float(ts_ok),\n",
    "                \"lat_parse_rate\": float(lat_ok),\n",
    "                \"lon_parse_rate\": float(lon_ok),\n",
    "                \"lat_in_range_rate\": float(lat_in),\n",
    "                \"lon_in_range_rate\": float(lon_in),\n",
    "                \"n_unique_users_sample\": int(user.nunique())\n",
    "            }\n",
    "\n",
    "        elif name.startswith(\"L9\"):\n",
    "            tmp.columns = [\"user_id\", \"venue_id\", \"wday\", \"mon\", \"day\", \"time\", \"tz\", \"year\", \"utc_offset_min\"]\n",
    "            user = tmp[\"user_id\"].astype(str)\n",
    "\n",
    "            ts_raw = (\n",
    "                tmp[\"wday\"].astype(str) + \" \" +\n",
    "                tmp[\"mon\"].astype(str) + \" \" +\n",
    "                tmp[\"day\"].astype(str) + \" \" +\n",
    "                tmp[\"time\"].astype(str) + \" \" +\n",
    "                tmp[\"tz\"].astype(str) + \" \" +\n",
    "                tmp[\"year\"].astype(str)\n",
    "            )\n",
    "            ts = pd.to_datetime(ts_raw, errors=\"coerce\", format=\"%a %b %d %H:%M:%S %z %Y\")\n",
    "            ts_ok = ts.notna().mean()\n",
    "\n",
    "            score = ts_ok\n",
    "            detail = {\n",
    "                \"layout\": name,\n",
    "                \"ts_parse_rate\": float(ts_ok),\n",
    "                \"n_unique_users_sample\": int(user.nunique()),\n",
    "                \"note\": \"LBSN2Vec curated detected; lat/lon require POI join by venue_id.\"\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            tmp.columns = [\"user_id\", \"venue_id\", \"ts\"]\n",
    "            user = tmp[\"user_id\"].astype(str)\n",
    "            ts = pd.to_datetime(tmp[\"ts\"], errors=\"coerce\")\n",
    "            ts_ok = ts.notna().mean()\n",
    "            score = ts_ok\n",
    "            detail = {\n",
    "                \"layout\": name,\n",
    "                \"ts_parse_rate\": float(ts_ok),\n",
    "                \"n_unique_users_sample\": int(user.nunique()),\n",
    "                \"note\": \"lat/lon not present; requires POI join if needed.\"\n",
    "            }\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best = name\n",
    "            best_detail = detail\n",
    "\n",
    "    report[\"best_layout_guess\"] = best\n",
    "    report[\"best_layout_detail\"] = best_detail\n",
    "\n",
    "    if best is None:\n",
    "        report[\"ok\"] = False\n",
    "        report[\"issues\"].append(\"Cannot infer checkins layout from sample.\")\n",
    "        return report\n",
    "\n",
    "    if best.startswith((\"A5\", \"B4\")):\n",
    "        d = best_detail\n",
    "        if d[\"ts_parse_rate\"] < 0.7 or d[\"lat_parse_rate\"] < 0.7 or d[\"lon_parse_rate\"] < 0.7:\n",
    "            report[\"ok\"] = False\n",
    "            report[\"issues\"].append(\"Low parse rate for ts/lat/lon in inferred layout.\")\n",
    "    elif best.startswith(\"L9\"):\n",
    "        if best_detail[\"ts_parse_rate\"] < 0.7:\n",
    "            report[\"ok\"] = False\n",
    "            report[\"issues\"].append(\"Low timestamp parse rate in LBSN2Vec curated layout.\")\n",
    "        report[\"needs_poi_join_for_latlon\"] = True\n",
    "    else:\n",
    "        if best_detail[\"ts_parse_rate\"] < 0.7:\n",
    "            report[\"ok\"] = False\n",
    "            report[\"issues\"].append(\"Low timestamp parse rate in 3-col layout.\")\n",
    "        report[\"needs_poi_join_for_latlon\"] = True\n",
    "\n",
    "    return report\n",
    "\n",
    "def validate_dataset_registry_and_load(\n",
    "    datasets: Dict[DatasetName, DatasetSpec],\n",
    "    active: Optional[List[str]] = None,\n",
    "    sample_rows: int = 50,\n",
    "    preview_lines: int = 2\n",
    ") -> pd.DataFrame:\n",
    "    rows = []\n",
    "    active_set = set(active) if active else set(datasets.keys())\n",
    "\n",
    "    for name, spec in datasets.items():\n",
    "        if name not in active_set:\n",
    "            continue\n",
    "\n",
    "        row = {\n",
    "            \"dataset\": name,\n",
    "            \"source\": spec.source,\n",
    "            \"root_exists\": spec.root.exists(),\n",
    "            \"root\": _rel(spec.root),\n",
    "        }\n",
    "\n",
    "        paths = {}\n",
    "        if spec.source == \"SNAP\":\n",
    "            paths[\"edges\"] = spec.edges_path\n",
    "            paths[\"checkins\"] = spec.checkins_path\n",
    "        else:\n",
    "            paths[\"friendship_old\"] = spec.friendship_old_path\n",
    "            paths[\"friendship_new\"] = spec.friendship_new_path\n",
    "            paths[\"checkins_curated\"] = spec.checkins_path\n",
    "            paths[\"readme\"] = spec.readme_path\n",
    "            paths[\"poi\"] = spec.poi_path\n",
    "\n",
    "        for k, p in paths.items():\n",
    "            if p is None:\n",
    "                row[f\"{k}_exists\"] = False\n",
    "                row[f\"{k}_size\"] = None\n",
    "                continue\n",
    "            meta = file_meta(p)\n",
    "            row[f\"{k}_exists\"] = meta[\"exists\"]\n",
    "            row[f\"{k}_size\"] = meta[\"size\"]\n",
    "\n",
    "        try:\n",
    "            if spec.source == \"SNAP\":\n",
    "                e_sample = read_sample_whitespace(spec.edges_path, nrows=sample_rows)\n",
    "                e_val = validate_edges_df(e_sample)\n",
    "                row[\"edges_ok\"] = e_val[\"ok\"]\n",
    "                row[\"edges_self_loop_ratio\"] = e_val.get(\"self_loop_ratio\", None)\n",
    "                row[\"edges_dup_ratio_sample\"] = e_val.get(\"dup_ratio_sample\", None)\n",
    "                row[\"edges_head\"] = \" | \".join(safe_head_lines(spec.edges_path, n=preview_lines))\n",
    "\n",
    "                c_sample = read_sample_whitespace(spec.checkins_path, nrows=sample_rows)\n",
    "                c_val = validate_checkins_df(c_sample, dataset=name)\n",
    "                row[\"checkins_ok\"] = c_val[\"ok\"]\n",
    "                row[\"checkins_best_layout\"] = c_val.get(\"best_layout_guess\", None)\n",
    "                row[\"checkins_ts_parse_rate\"] = (c_val.get(\"best_layout_detail\") or {}).get(\"ts_parse_rate\", None)\n",
    "                row[\"checkins_lat_parse_rate\"] = (c_val.get(\"best_layout_detail\") or {}).get(\"lat_parse_rate\", None)\n",
    "                row[\"checkins_lon_parse_rate\"] = (c_val.get(\"best_layout_detail\") or {}).get(\"lon_parse_rate\", None)\n",
    "                row[\"checkins_head\"] = \" | \".join(safe_head_lines(spec.checkins_path, n=preview_lines))\n",
    "                row[\"issues\"] = \"; \".join((e_val.get(\"issues\") or []) + (c_val.get(\"issues\") or []))\n",
    "\n",
    "            else:\n",
    "                old_sample = read_sample_whitespace(spec.friendship_old_path, nrows=sample_rows)\n",
    "                new_sample = read_sample_whitespace(spec.friendship_new_path, nrows=sample_rows)\n",
    "                old_val = validate_edges_df(old_sample)\n",
    "                new_val = validate_edges_df(new_sample)\n",
    "                row[\"friendship_old_ok\"] = old_val[\"ok\"]\n",
    "                row[\"friendship_new_ok\"] = new_val[\"ok\"]\n",
    "                row[\"friendship_old_head\"] = \" | \".join(safe_head_lines(spec.friendship_old_path, n=preview_lines))\n",
    "                row[\"friendship_new_head\"] = \" | \".join(safe_head_lines(spec.friendship_new_path, n=preview_lines))\n",
    "\n",
    "                chk_sample = read_sample_whitespace(spec.checkins_path, nrows=sample_rows)\n",
    "                chk_val = validate_checkins_df(chk_sample, dataset=name)\n",
    "                row[\"checkins_ok\"] = chk_val[\"ok\"]\n",
    "                row[\"checkins_best_layout\"] = chk_val.get(\"best_layout_guess\", None)\n",
    "                row[\"checkins_ts_parse_rate\"] = (chk_val.get(\"best_layout_detail\") or {}).get(\"ts_parse_rate\", None)\n",
    "                row[\"checkins_lat_parse_rate\"] = (chk_val.get(\"best_layout_detail\") or {}).get(\"lat_parse_rate\", None)\n",
    "                row[\"checkins_lon_parse_rate\"] = (chk_val.get(\"best_layout_detail\") or {}).get(\"lon_parse_rate\", None)\n",
    "                row[\"checkins_needs_poi_join\"] = chk_val.get(\"needs_poi_join_for_latlon\", False)\n",
    "                row[\"checkins_head\"] = \" | \".join(safe_head_lines(spec.checkins_path, n=preview_lines))\n",
    "\n",
    "                if spec.readme_path and spec.readme_path.exists():\n",
    "                    row[\"readme_head\"] = \" | \".join(safe_head_lines(spec.readme_path, n=min(10, preview_lines)))\n",
    "                else:\n",
    "                    row[\"readme_head\"] = None\n",
    "\n",
    "                row[\"issues\"] = \"; \".join((old_val.get(\"issues\") or []) + (new_val.get(\"issues\") or []) + (chk_val.get(\"issues\") or []))\n",
    "\n",
    "        except Exception as ex:\n",
    "            row[\"issues\"] = (row.get(\"issues\", \"\") + f\"; EXCEPTION: {type(ex).__name__}: {ex}\").strip(\"; \")\n",
    "            row.setdefault(\"edges_ok\", False)\n",
    "            row.setdefault(\"checkins_ok\", False)\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def summarize_validation(df_val: pd.DataFrame) -> None:\n",
    "    print(\"=== Validation Summary ===\")\n",
    "    for _, r in df_val.iterrows():\n",
    "        name = r[\"dataset\"]\n",
    "        src = r[\"source\"]\n",
    "        ok_root = bool(r[\"root_exists\"])\n",
    "        if not ok_root:\n",
    "            print(f\"[FAIL] {name} - missing root folder\")\n",
    "            continue\n",
    "\n",
    "        if src == \"SNAP\":\n",
    "            edges_ok = bool(r.get(\"edges_ok\", False))\n",
    "            chk_ok = bool(r.get(\"checkins_ok\", False))\n",
    "            print(f\"{'[OK] ' if (edges_ok and chk_ok) else '[FAIL]'} {name} (SNAP) | edges_ok={edges_ok} | checkins_ok={chk_ok} | layout={r.get('checkins_best_layout')}\")\n",
    "            if not (edges_ok and chk_ok):\n",
    "                print(f\"  issues: {r.get('issues')}\")\n",
    "        else:\n",
    "            old_ok = bool(r.get(\"friendship_old_ok\", False))\n",
    "            new_ok = bool(r.get(\"friendship_new_ok\", False))\n",
    "            chk_ok = bool(r.get(\"checkins_ok\", False))\n",
    "            need_join = bool(r.get(\"checkins_needs_poi_join\", False))\n",
    "            print(f\"{'[OK] ' if (old_ok and new_ok and chk_ok) else '[FAIL]'} {name} (LBSN2Vec) | old_ok={old_ok} new_ok={new_ok} checkins_ok={chk_ok} need_POI_join={need_join} | layout={r.get('checkins_best_layout')}\")\n",
    "            if not (old_ok and new_ok and chk_ok):\n",
    "                print(f\"  issues: {r.get('issues')}\")\n",
    "    print(\"==========================\")\n",
    "\n",
    "# ---- Run validation ----\n",
    "active = CFG[\"datasets\"].get(\"active\", [\"brightkite\", \"gowalla\", \"lbsn2vec\"])\n",
    "df_val = validate_dataset_registry_and_load(DATASETS, active=active, sample_rows=100, preview_lines=2)\n",
    "df_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c140771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Validation Summary ===\n",
      "[OK]  brightkite (SNAP) | edges_ok=True | checkins_ok=True | layout=A5:user,ts,lat,lon,venue\n",
      "[OK]  gowalla (SNAP) | edges_ok=True | checkins_ok=True | layout=A5:user,ts,lat,lon,venue\n",
      "[OK]  lbsn2vec (LBSN2Vec) | old_ok=True new_ok=True checkins_ok=True need_POI_join=True | layout=L9:user,venue,wday,mon,day,time,tz,year,utc_offset\n",
      "==========================\n"
     ]
    }
   ],
   "source": [
    "summarize_validation(df_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b390c8e",
   "metadata": {},
   "source": [
    "## Pipeline runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f7ec02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:38:04] INFO - [C0] dataset=lbsn2vec | snapshot=old | tier=curated\n",
      "[05:38:04] INFO - [C0] LBSN_ROOT=/mnt/d/community-detection/notebooks/data/LBSN2Vec\n",
      "[05:38:04] INFO - [C0] friendship_old: exists=False | path=/mnt/d/community-detection/notebooks/data/LBSN2Vec/dataset_WWW_friendship_old.txt\n",
      "[05:38:04] INFO - [C0] friendship_new: exists=False | path=/mnt/d/community-detection/notebooks/data/LBSN2Vec/dataset_WWW_friendship_new.txt\n",
      "[05:38:04] INFO - [C0] checkins_curated: exists=False | path=/mnt/d/community-detection/notebooks/data/LBSN2Vec/dataset_WWW_Checkins_anonymized.txt\n",
      "[05:38:04] INFO - [C0] poi: exists=False | path=/mnt/d/community-detection/notebooks/data/LBSN2Vec/raw_POIs.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import logging, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "\n",
    "if \"LOGGER\" not in globals():\n",
    "    LOGGER = logging.getLogger(\"community-detection\")\n",
    "    LOGGER.setLevel(logging.INFO)\n",
    "    LOGGER.handlers.clear()\n",
    "    h = logging.StreamHandler(sys.stdout)\n",
    "    h.setFormatter(logging.Formatter(\"[%(asctime)s] %(levelname)s - %(message)s\", \"%H:%M:%S\"))\n",
    "    LOGGER.addHandler(h)\n",
    "    LOGGER.propagate = False\n",
    "\n",
    "\n",
    "if \"CFG\" not in globals():\n",
    "    CFG = {\n",
    "        \"datasets\": {\"lbsn2vec_snapshot\": \"old\", \"lbsn2vec_tier\": \"curated\"},\n",
    "        \"preprocess\": {\n",
    "            \"min_checkins\": 10,\n",
    "            \"min_degree\": 3,\n",
    "            \"iterative_filter\": True,\n",
    "            \"lat_range\": [-90.0, 90.0],\n",
    "            \"lon_range\": [-180.0, 180.0],\n",
    "        }\n",
    "    }\n",
    "\n",
    "DATASET_NAME = \"lbsn2vec\"\n",
    "\n",
    "\n",
    "LBSN_ROOT = PROJECT_ROOT / \"data\" / \"LBSN2Vec\"\n",
    "PATHS = {\n",
    "    \"friendship_old\": LBSN_ROOT / \"dataset_WWW_friendship_old.txt\",\n",
    "    \"friendship_new\": LBSN_ROOT / \"dataset_WWW_friendship_new.txt\",\n",
    "    \"checkins_curated\": LBSN_ROOT / \"dataset_WWW_Checkins_anonymized.txt\",\n",
    "    \"poi\": LBSN_ROOT / \"raw_POIs.txt\",\n",
    "}\n",
    "\n",
    "LOGGER.info(f\"[C0] dataset={DATASET_NAME} | snapshot={CFG['datasets'].get('lbsn2vec_snapshot')} | tier={CFG['datasets'].get('lbsn2vec_tier')}\")\n",
    "LOGGER.info(f\"[C0] LBSN_ROOT={LBSN_ROOT}\")\n",
    "for k, p in PATHS.items():\n",
    "    LOGGER.info(f\"[C0] {k}: exists={p.exists()} | path={p}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3307ef",
   "metadata": {},
   "source": [
    "### Step 1: load raw -> parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b3eef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C0] LBSN sample frac=0.1 (set 1.0 for full run)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "LBSN_SAMPLE_FRAC = 0.10   # 10% Ä‘á»ƒ cháº¡y thá»­; Ä‘áº·t = 1.0 Ä‘á»ƒ cháº¡y full\n",
    "LBSN_SAMPLE_SEED = int(CFG[\"run\"].get(\"seed\", 42))\n",
    "\n",
    "def sample_users_from_checkins_chunked(\n",
    "    checkins_path,\n",
    "    sample_frac: float,\n",
    "    seed: int,\n",
    "    chunksize: int = 2_000_000\n",
    "):\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    users = set()\n",
    "    for chunk in pd.read_csv(checkins_path, sep=r\"\\s+\", header=None, dtype=str, engine=\"python\",\n",
    "                             chunksize=chunksize):\n",
    "\n",
    "        users.update(chunk.iloc[:, 0].astype(str).unique().tolist())\n",
    "\n",
    "    users = np.array(list(users), dtype=object)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rng.shuffle(users)\n",
    "    n = int(np.ceil(len(users) * float(sample_frac)))\n",
    "    keep = set(users[:n].tolist())\n",
    "    return keep\n",
    "\n",
    "print(f\"[C0] LBSN sample frac={LBSN_SAMPLE_FRAC} (set 1.0 for full run)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ea686d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:38:04] INFO - [C1] dataset=lbsn2vec | snapshot=old | sampling=0.10\n",
      "[05:38:04] INFO - [C1] collecting unique users (chunked) to sample ...\n",
      "[05:41:36] INFO - [C1] sampled users: 11,433\n",
      "[05:41:36] INFO - [C1] loading edges ...\n",
      "[05:41:37] INFO - [C1] edges_raw after sampling filter: (3474, 2)\n",
      "[05:41:37] INFO - [C1] loading curated checkins (chunked) ...\n",
      "[05:45:14] INFO - [C1] checkins_core after sampling filter: (2284971, 4)\n",
      "[05:45:14] INFO - [C1] loading POIs ...\n",
      "[05:47:42] INFO - [C1] POIs filtered to used venues: (679734, 3)\n",
      "[05:47:42] INFO - [C1] join checkins with POIs ...\n",
      "[05:47:43] INFO - [C1] edges_raw=(3474, 2) | checkins_raw=(2284971, 6)\n",
      "[05:47:43] INFO - [C1] ts_parse_ok_rate=1.000 | lat_ok_rate=1.000 | lon_ok_rate=1.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(       u      v\n",
       " 235  190  76637\n",
       " 803  541   3063\n",
       " 809  541   4562\n",
       " 812  541   4826\n",
       " 824  541   7199,\n",
       "    user_id                  venue_id                        ts  tz_offset_min  \\\n",
       " 0  1583419  4f770f6be4b0f52db8b2976b 2012-04-03 18:00:13+00:00          180.0   \n",
       " 1  1270024  4d273474915fa093c63df109 2012-04-03 18:00:24+00:00          480.0   \n",
       " 2  1599762  4e482dde1f6e29f10dbd6e41 2012-04-03 18:00:28+00:00          180.0   \n",
       " 3   295503  4df7a1d01838c789e8ba2af6 2012-04-03 18:00:39+00:00         -240.0   \n",
       " 4  1583941  4e08b4fbfa767637fd324447 2012-04-03 18:00:47+00:00         -300.0   \n",
       " \n",
       "          lat         lon  \n",
       " 0  37.926483   23.709966  \n",
       " 1   2.933741  101.766744  \n",
       " 2  29.309856   47.968669  \n",
       " 3  34.015523  -84.576258  \n",
       " 4  34.806880  -87.679396  )"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "DATASET_NAME = \"lbsn2vec\"\n",
    "spec = DATASETS[DATASET_NAME]\n",
    "\n",
    "def read_edges_two_cols(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=r\"\\s+\", header=None, usecols=[0, 1], dtype=str, engine=\"python\")\n",
    "    df.columns = [\"u\", \"v\"]\n",
    "    df[\"u\"] = df[\"u\"].astype(str)\n",
    "    df[\"v\"] = df[\"v\"].astype(str)\n",
    "    return df\n",
    "\n",
    "def read_pois_minimal(path: Path) -> pd.DataFrame:\n",
    "    poi = pd.read_csv(path, sep=r\"\\s+\", header=None, usecols=[0, 1, 2], dtype=str, engine=\"python\")\n",
    "    poi.columns = [\"venue_id\", \"lat\", \"lon\"]\n",
    "    poi[\"venue_id\"] = poi[\"venue_id\"].astype(str)\n",
    "    poi[\"lat\"] = pd.to_numeric(poi[\"lat\"], errors=\"coerce\")\n",
    "    poi[\"lon\"] = pd.to_numeric(poi[\"lon\"], errors=\"coerce\")\n",
    "    return poi\n",
    "\n",
    "def parse_lbsn_curated_checkins_9col_chunk(chunk: pd.DataFrame) -> pd.DataFrame:\n",
    "    if chunk.shape[1] < 9:\n",
    "        raise ValueError(f\"Expected >=9 columns for curated checkins, got shape={chunk.shape}\")\n",
    "\n",
    "    df = chunk.iloc[:, :9].copy()\n",
    "    df.columns = [\"user_id\", \"venue_id\", \"dow\", \"mon\", \"day\", \"time\", \"tz\", \"year\", \"tz_offset_min\"]\n",
    "\n",
    "    ts_str = (\n",
    "        df[\"dow\"].astype(str) + \" \" +\n",
    "        df[\"mon\"].astype(str) + \" \" +\n",
    "        df[\"day\"].astype(str) + \" \" +\n",
    "        df[\"time\"].astype(str) + \" \" +\n",
    "        df[\"tz\"].astype(str) + \" \" +\n",
    "        df[\"year\"].astype(str)\n",
    "    )\n",
    "    ts = pd.to_datetime(ts_str, format=\"%a %b %d %H:%M:%S %z %Y\", errors=\"coerce\")\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"user_id\": df[\"user_id\"].astype(str),\n",
    "        \"venue_id\": df[\"venue_id\"].astype(str),\n",
    "        \"ts\": ts,\n",
    "        \"tz_offset_min\": pd.to_numeric(df[\"tz_offset_min\"], errors=\"coerce\"),\n",
    "    })\n",
    "    return out\n",
    "\n",
    "# ---- choose snapshot edges ----\n",
    "snapshot = CFG[\"datasets\"].get(\"lbsn2vec_snapshot\", \"old\")\n",
    "edges_path = spec.friendship_old_path if snapshot == \"old\" else spec.friendship_new_path\n",
    "checkins_path = spec.checkins_path\n",
    "poi_path = spec.poi_path\n",
    "\n",
    "# ---- sanity check required files ----\n",
    "required_files = [edges_path, checkins_path, poi_path]\n",
    "missing = [p for p in required_files if (p is None or not Path(p).exists())]\n",
    "if missing:\n",
    "    raise FileNotFoundError(\"Missing required file(s) in registry:\\n\" + \"\\n\".join([str(p) for p in missing]))\n",
    "\n",
    "LOGGER.info(f\"[C1] dataset={DATASET_NAME} | snapshot={snapshot} | sampling={LBSN_SAMPLE_FRAC:.2f}\")\n",
    "\n",
    "# ---- Step 1a: sample 10% users (fast enough; one pass by chunks) ----\n",
    "if LBSN_SAMPLE_FRAC < 1.0:\n",
    "    LOGGER.info(\"[C1] collecting unique users (chunked) to sample ...\")\n",
    "    keep_users = sample_users_from_checkins_chunked(\n",
    "        checkins_path=checkins_path,\n",
    "        sample_frac=LBSN_SAMPLE_FRAC,\n",
    "        seed=LBSN_SAMPLE_SEED,\n",
    "        chunksize=2_000_000\n",
    "    )\n",
    "    LOGGER.info(f\"[C1] sampled users: {len(keep_users):,}\")\n",
    "else:\n",
    "    keep_users = None\n",
    "    LOGGER.info(\"[C1] sampling disabled (full dataset).\")\n",
    "\n",
    "# ---- Step 1b: load edges then filter by sampled users ----\n",
    "LOGGER.info(\"[C1] loading edges ...\")\n",
    "edges_all = read_edges_two_cols(edges_path)\n",
    "\n",
    "if keep_users is not None:\n",
    "    edges_raw = edges_all[edges_all[\"u\"].isin(keep_users) & edges_all[\"v\"].isin(keep_users)].copy()\n",
    "else:\n",
    "    edges_raw = edges_all\n",
    "\n",
    "LOGGER.info(f\"[C1] edges_raw after sampling filter: {edges_raw.shape}\")\n",
    "\n",
    "# ---- Step 1c: load curated checkins chunked; keep only sampled users ----\n",
    "LOGGER.info(\"[C1] loading curated checkins (chunked) ...\")\n",
    "chunks = []\n",
    "for chunk in pd.read_csv(checkins_path, sep=r\"\\s+\", header=None, dtype=str, engine=\"python\",\n",
    "                         chunksize=2_000_000):\n",
    "    if keep_users is not None:\n",
    "        chunk = chunk[chunk.iloc[:, 0].astype(str).isin(keep_users)]\n",
    "    if len(chunk) == 0:\n",
    "        continue\n",
    "    chunks.append(parse_lbsn_curated_checkins_9col_chunk(chunk))\n",
    "\n",
    "checkins_core = pd.concat(chunks, ignore_index=True) if chunks else pd.DataFrame(columns=[\"user_id\",\"venue_id\",\"ts\",\"tz_offset_min\"])\n",
    "LOGGER.info(f\"[C1] checkins_core after sampling filter: {checkins_core.shape}\")\n",
    "\n",
    "# ---- Step 1d: join POIs (full POI file might be big, but 672MB is still ok once)\n",
    "# Optional optimization: filter POIs to only venues in sampled checkins.\n",
    "LOGGER.info(\"[C1] loading POIs ...\")\n",
    "pois = read_pois_minimal(poi_path)\n",
    "\n",
    "if len(checkins_core) > 0:\n",
    "    needed_venues = set(checkins_core[\"venue_id\"].unique().tolist())\n",
    "    pois = pois[pois[\"venue_id\"].isin(needed_venues)].copy()\n",
    "    LOGGER.info(f\"[C1] POIs filtered to used venues: {pois.shape}\")\n",
    "\n",
    "LOGGER.info(\"[C1] join checkins with POIs ...\")\n",
    "checkins_raw = checkins_core.merge(pois, on=\"venue_id\", how=\"left\")\n",
    "\n",
    "checkins_raw[\"lat\"] = pd.to_numeric(checkins_raw[\"lat\"], errors=\"coerce\")\n",
    "checkins_raw[\"lon\"] = pd.to_numeric(checkins_raw[\"lon\"], errors=\"coerce\")\n",
    "\n",
    "LOGGER.info(f\"[C1] edges_raw={edges_raw.shape} | checkins_raw={checkins_raw.shape}\")\n",
    "LOGGER.info(f\"[C1] ts_parse_ok_rate={(checkins_raw['ts'].notna().mean()):.3f} | lat_ok_rate={(checkins_raw['lat'].notna().mean()):.3f} | lon_ok_rate={(checkins_raw['lon'].notna().mean()):.3f}\")\n",
    "\n",
    "edges_raw.head(), checkins_raw.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0a7b9",
   "metadata": {},
   "source": [
    "### Step 2: cleaning chuáº©n hoÃ¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b539663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:47:43] INFO - [C2] cleaning edges ...\n",
      "[05:47:43] INFO - [C2] cleaning checkins ...\n",
      "[05:47:46] INFO - [C2] edges_clean=(3474, 2) | checkins_clean=(2284967, 5)\n",
      "[05:47:47] INFO - [C2] checkins users=11433 | venues=679733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(      u      v\n",
       " 0   190  76637\n",
       " 1  3063    541\n",
       " 2  4562    541\n",
       " 3  4826    541\n",
       " 4   541   7199,\n",
       "    user_id                  ts        lat         lon  \\\n",
       " 0  1583419 2012-04-03 18:00:13  37.926483   23.709966   \n",
       " 1  1270024 2012-04-03 18:00:24   2.933741  101.766744   \n",
       " 2  1599762 2012-04-03 18:00:28  29.309856   47.968669   \n",
       " 3   295503 2012-04-03 18:00:39  34.015523  -84.576258   \n",
       " 4  1583941 2012-04-03 18:00:47  34.806880  -87.679396   \n",
       " \n",
       "                    venue_id  \n",
       " 0  4f770f6be4b0f52db8b2976b  \n",
       " 1  4d273474915fa093c63df109  \n",
       " 2  4e482dde1f6e29f10dbd6e41  \n",
       " 3  4df7a1d01838c789e8ba2af6  \n",
       " 4  4e08b4fbfa767637fd324447  )"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def make_undirected_dedup(edges: pd.DataFrame) -> pd.DataFrame:\n",
    "    u = edges[\"u\"].astype(str).to_numpy()\n",
    "    v = edges[\"v\"].astype(str).to_numpy()\n",
    "    u2 = np.where(u <= v, u, v)\n",
    "    v2 = np.where(u <= v, v, u)\n",
    "    out = pd.DataFrame({\"u\": u2, \"v\": v2})\n",
    "    out = out[out[\"u\"] != out[\"v\"]]              \n",
    "    out = out.drop_duplicates([\"u\", \"v\"]).reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "def clean_checkins(chk: pd.DataFrame, cfg: dict) -> pd.DataFrame:\n",
    "    lat_lo, lat_hi = cfg[\"preprocess\"][\"lat_range\"]\n",
    "    lon_lo, lon_hi = cfg[\"preprocess\"][\"lon_range\"]\n",
    "\n",
    "    out = chk.copy()\n",
    "    out[\"user_id\"] = out[\"user_id\"].astype(str)\n",
    "    out[\"venue_id\"] = out[\"venue_id\"].astype(str)\n",
    "    out[\"ts\"] = pd.to_datetime(out[\"ts\"], errors=\"coerce\")\n",
    "    out[\"lat\"] = pd.to_numeric(out[\"lat\"], errors=\"coerce\")\n",
    "    out[\"lon\"] = pd.to_numeric(out[\"lon\"], errors=\"coerce\")\n",
    "\n",
    "\n",
    "    out = out.dropna(subset=[\"user_id\", \"venue_id\", \"ts\", \"lat\", \"lon\"])\n",
    "\n",
    "\n",
    "    out = out[(out[\"lat\"] >= lat_lo) & (out[\"lat\"] <= lat_hi) & (out[\"lon\"] >= lon_lo) & (out[\"lon\"] <= lon_hi)]\n",
    "\n",
    "\n",
    "    try:\n",
    "        if hasattr(out[\"ts\"].dt, \"tz\") and out[\"ts\"].dt.tz is not None:\n",
    "            out[\"ts\"] = out[\"ts\"].dt.tz_convert(\"UTC\").dt.tz_localize(None)\n",
    "        else:\n",
    "            out[\"ts\"] = pd.to_datetime(out[\"ts\"], errors=\"coerce\")\n",
    "    except Exception:\n",
    "        out[\"ts\"] = pd.to_datetime(out[\"ts\"], errors=\"coerce\")\n",
    "\n",
    "    out = out.dropna(subset=[\"ts\"]).reset_index(drop=True)\n",
    "    return out[[\"user_id\", \"ts\", \"lat\", \"lon\", \"venue_id\"]].copy()\n",
    "\n",
    "LOGGER.info(\"[C2] cleaning edges ...\")\n",
    "edges_clean = make_undirected_dedup(edges_raw)\n",
    "\n",
    "LOGGER.info(\"[C2] cleaning checkins ...\")\n",
    "checkins_clean = clean_checkins(checkins_raw, CFG)\n",
    "\n",
    "LOGGER.info(f\"[C2] edges_clean={edges_clean.shape} | checkins_clean={checkins_clean.shape}\")\n",
    "LOGGER.info(f\"[C2] checkins users={checkins_clean['user_id'].nunique()} | venues={checkins_clean['venue_id'].nunique()}\")\n",
    "\n",
    "edges_clean.head(), checkins_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b1727",
   "metadata": {},
   "source": [
    "### Step 3: filter users + induced subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "041f4b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:47:47] INFO - [C3] start filter: min_checkins(k)=10, min_degree(d)=3, iterative=True\n",
      "[05:47:48] INFO - [C3] round=1 | users=658 | edges=935 | checkins=159222\n",
      "[05:47:48] INFO - [C3] round=2 | users=310 | edges=592 | checkins=77112\n",
      "[05:47:48] INFO - [C3] round=3 | users=231 | edges=487 | checkins=56410\n",
      "[05:47:48] INFO - [C3] round=4 | users=207 | edges=449 | checkins=48501\n",
      "[05:47:48] INFO - [C3] round=5 | users=196 | edges=431 | checkins=46550\n",
      "[05:47:48] INFO - [C3] round=6 | users=191 | edges=424 | checkins=45820\n",
      "[05:47:48] INFO - [C3] round=7 | users=187 | edges=419 | checkins=44916\n",
      "[05:47:48] INFO - [C3] round=8 | users=185 | edges=415 | checkins=44375\n",
      "[05:47:48] INFO - [C3] round=9 | users=182 | edges=410 | checkins=43968\n",
      "[05:47:48] INFO - [C3] round=10 | users=179 | edges=407 | checkins=43487\n",
      "[05:47:48] INFO - [C3] round=11 | users=178 | edges=405 | checkins=43352\n",
      "[05:47:48] INFO - [C3] round=12 | users=178 | edges=405 | checkins=43352\n",
      "[05:47:48] INFO - [C3] DONE | users_final=(178, 1) | edges_final=(405, 2) | checkins_final=(43352, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([(1, 658, 935, 159222),\n",
       "  (2, 310, 592, 77112),\n",
       "  (3, 231, 487, 56410),\n",
       "  (4, 207, 449, 48501),\n",
       "  (5, 196, 431, 46550)],\n",
       " (12, 178, 405, 43352))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def degree_from_edges(edges: pd.DataFrame) -> pd.Series:\n",
    "    u = edges[\"u\"].astype(str)\n",
    "    v = edges[\"v\"].astype(str)\n",
    "    return pd.concat([u, v]).value_counts()\n",
    "\n",
    "def filter_induced_once(edges: pd.DataFrame, chk: pd.DataFrame, k: int, d: int):\n",
    "    ccount = chk[\"user_id\"].astype(str).value_counts()\n",
    "    deg = degree_from_edges(edges)\n",
    "\n",
    "    users_ok = ccount[ccount >= k].index\n",
    "    deg_ok = deg[deg >= d].index\n",
    "    v_keep = pd.Index(users_ok).intersection(pd.Index(deg_ok))\n",
    "\n",
    "    edges2 = edges[edges[\"u\"].isin(v_keep) & edges[\"v\"].isin(v_keep)].copy().reset_index(drop=True)\n",
    "    chk2 = chk[chk[\"user_id\"].isin(v_keep)].copy().reset_index(drop=True)\n",
    "\n",
    "    return v_keep, edges2, chk2\n",
    "\n",
    "k = int(CFG[\"preprocess\"].get(\"min_checkins\", 10))\n",
    "d = int(CFG[\"preprocess\"].get(\"min_degree\", 3))\n",
    "iterative = bool(CFG[\"preprocess\"].get(\"iterative_filter\", True))\n",
    "\n",
    "LOGGER.info(f\"[C3] start filter: min_checkins(k)={k}, min_degree(d)={d}, iterative={iterative}\")\n",
    "\n",
    "edges_tmp = edges_clean.copy()\n",
    "chk_tmp = checkins_clean.copy()\n",
    "\n",
    "prev_users = -1\n",
    "history = []\n",
    "for r in range(1, 21):\n",
    "    v_keep, edges_tmp, chk_tmp = filter_induced_once(edges_tmp, chk_tmp, k=k, d=d)\n",
    "    n_users = len(v_keep)\n",
    "    history.append((r, n_users, len(edges_tmp), len(chk_tmp)))\n",
    "    LOGGER.info(f\"[C3] round={r} | users={n_users} | edges={len(edges_tmp)} | checkins={len(chk_tmp)}\")\n",
    "\n",
    "    if (not iterative) or (n_users == prev_users):\n",
    "        break\n",
    "    prev_users = n_users\n",
    "\n",
    "users_final = pd.DataFrame({\"user_id\": pd.Index(chk_tmp[\"user_id\"].unique()).sort_values()})\n",
    "edges_final = edges_tmp\n",
    "checkins_final = chk_tmp\n",
    "\n",
    "LOGGER.info(f\"[C3] DONE | users_final={users_final.shape} | edges_final={edges_final.shape} | checkins_final={checkins_final.shape}\")\n",
    "history[:5], history[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c629a87",
   "metadata": {},
   "source": [
    "### Step 4: build X_users "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d1998d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:47:48] INFO - [C4] X_users shape = (178, 43) | #features=43\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_checkins</th>\n",
       "      <th>num_active_days</th>\n",
       "      <th>num_unique_venues</th>\n",
       "      <th>mean_lat</th>\n",
       "      <th>mean_lon</th>\n",
       "      <th>std_lat</th>\n",
       "      <th>std_lon</th>\n",
       "      <th>radius_of_gyration_km</th>\n",
       "      <th>median_dist_to_centroid_km</th>\n",
       "      <th>hour_entropy</th>\n",
       "      <th>...</th>\n",
       "      <th>hour_21_p</th>\n",
       "      <th>hour_22_p</th>\n",
       "      <th>hour_23_p</th>\n",
       "      <th>dow_0_p</th>\n",
       "      <th>dow_1_p</th>\n",
       "      <th>dow_2_p</th>\n",
       "      <th>dow_3_p</th>\n",
       "      <th>dow_4_p</th>\n",
       "      <th>dow_5_p</th>\n",
       "      <th>dow_6_p</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10067</th>\n",
       "      <td>0.590789</td>\n",
       "      <td>0.909083</td>\n",
       "      <td>1.322706</td>\n",
       "      <td>1.302186</td>\n",
       "      <td>0.049988</td>\n",
       "      <td>-0.305042</td>\n",
       "      <td>-0.256626</td>\n",
       "      <td>-0.316204</td>\n",
       "      <td>-0.208769</td>\n",
       "      <td>0.111082</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.624574</td>\n",
       "      <td>-0.872944</td>\n",
       "      <td>-1.023420</td>\n",
       "      <td>1.215113</td>\n",
       "      <td>0.926638</td>\n",
       "      <td>-0.356168</td>\n",
       "      <td>0.820342</td>\n",
       "      <td>0.130334</td>\n",
       "      <td>-0.400101</td>\n",
       "      <td>-1.340650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010482</th>\n",
       "      <td>1.700363</td>\n",
       "      <td>1.434874</td>\n",
       "      <td>2.172274</td>\n",
       "      <td>0.588717</td>\n",
       "      <td>-1.053078</td>\n",
       "      <td>-0.449577</td>\n",
       "      <td>-0.183486</td>\n",
       "      <td>-0.282021</td>\n",
       "      <td>-0.193406</td>\n",
       "      <td>1.415218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.799747</td>\n",
       "      <td>0.299856</td>\n",
       "      <td>0.233436</td>\n",
       "      <td>-0.760165</td>\n",
       "      <td>-0.413037</td>\n",
       "      <td>-0.208375</td>\n",
       "      <td>0.458095</td>\n",
       "      <td>0.503226</td>\n",
       "      <td>0.283624</td>\n",
       "      <td>-0.004398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020789</th>\n",
       "      <td>-0.038288</td>\n",
       "      <td>-0.094755</td>\n",
       "      <td>-0.720353</td>\n",
       "      <td>0.166448</td>\n",
       "      <td>1.019148</td>\n",
       "      <td>-0.467476</td>\n",
       "      <td>-0.144914</td>\n",
       "      <td>-0.232801</td>\n",
       "      <td>-0.190406</td>\n",
       "      <td>0.378636</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.251141</td>\n",
       "      <td>-0.876722</td>\n",
       "      <td>-1.272655</td>\n",
       "      <td>2.166257</td>\n",
       "      <td>-0.608678</td>\n",
       "      <td>0.511357</td>\n",
       "      <td>0.197082</td>\n",
       "      <td>0.445610</td>\n",
       "      <td>-0.481001</td>\n",
       "      <td>-1.184623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103827</th>\n",
       "      <td>-0.406470</td>\n",
       "      <td>-0.151763</td>\n",
       "      <td>-1.216992</td>\n",
       "      <td>1.135293</td>\n",
       "      <td>1.701050</td>\n",
       "      <td>-0.443254</td>\n",
       "      <td>-0.229466</td>\n",
       "      <td>-0.340867</td>\n",
       "      <td>-0.162072</td>\n",
       "      <td>-1.714017</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.251141</td>\n",
       "      <td>-0.964742</td>\n",
       "      <td>4.276102</td>\n",
       "      <td>-0.058243</td>\n",
       "      <td>-0.656620</td>\n",
       "      <td>2.876468</td>\n",
       "      <td>-1.747237</td>\n",
       "      <td>-0.359127</td>\n",
       "      <td>-0.432351</td>\n",
       "      <td>0.354759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1064906</th>\n",
       "      <td>2.149414</td>\n",
       "      <td>1.686610</td>\n",
       "      <td>2.318824</td>\n",
       "      <td>0.267539</td>\n",
       "      <td>-0.881170</td>\n",
       "      <td>-0.420800</td>\n",
       "      <td>-0.281658</td>\n",
       "      <td>-0.367026</td>\n",
       "      <td>-0.204562</td>\n",
       "      <td>1.232195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024037</td>\n",
       "      <td>0.232916</td>\n",
       "      <td>0.439276</td>\n",
       "      <td>-0.753861</td>\n",
       "      <td>1.469926</td>\n",
       "      <td>0.123061</td>\n",
       "      <td>-0.074554</td>\n",
       "      <td>-0.066282</td>\n",
       "      <td>-0.747770</td>\n",
       "      <td>0.295944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         num_checkins  num_active_days  num_unique_venues  mean_lat  mean_lon  \\\n",
       "user_id                                                                         \n",
       "10067        0.590789         0.909083           1.322706  1.302186  0.049988   \n",
       "1010482      1.700363         1.434874           2.172274  0.588717 -1.053078   \n",
       "1020789     -0.038288        -0.094755          -0.720353  0.166448  1.019148   \n",
       "103827      -0.406470        -0.151763          -1.216992  1.135293  1.701050   \n",
       "1064906      2.149414         1.686610           2.318824  0.267539 -0.881170   \n",
       "\n",
       "          std_lat   std_lon  radius_of_gyration_km  \\\n",
       "user_id                                              \n",
       "10067   -0.305042 -0.256626              -0.316204   \n",
       "1010482 -0.449577 -0.183486              -0.282021   \n",
       "1020789 -0.467476 -0.144914              -0.232801   \n",
       "103827  -0.443254 -0.229466              -0.340867   \n",
       "1064906 -0.420800 -0.281658              -0.367026   \n",
       "\n",
       "         median_dist_to_centroid_km  hour_entropy  ...  hour_21_p  hour_22_p  \\\n",
       "user_id                                            ...                         \n",
       "10067                     -0.208769      0.111082  ...  -0.624574  -0.872944   \n",
       "1010482                   -0.193406      1.415218  ...   0.799747   0.299856   \n",
       "1020789                   -0.190406      0.378636  ...  -1.251141  -0.876722   \n",
       "103827                    -0.162072     -1.714017  ...  -1.251141  -0.964742   \n",
       "1064906                   -0.204562      1.232195  ...   0.024037   0.232916   \n",
       "\n",
       "         hour_23_p   dow_0_p   dow_1_p   dow_2_p   dow_3_p   dow_4_p  \\\n",
       "user_id                                                                \n",
       "10067    -1.023420  1.215113  0.926638 -0.356168  0.820342  0.130334   \n",
       "1010482   0.233436 -0.760165 -0.413037 -0.208375  0.458095  0.503226   \n",
       "1020789  -1.272655  2.166257 -0.608678  0.511357  0.197082  0.445610   \n",
       "103827    4.276102 -0.058243 -0.656620  2.876468 -1.747237 -0.359127   \n",
       "1064906   0.439276 -0.753861  1.469926  0.123061 -0.074554 -0.066282   \n",
       "\n",
       "          dow_5_p   dow_6_p  \n",
       "user_id                      \n",
       "10067   -0.400101 -1.340650  \n",
       "1010482  0.283624 -0.004398  \n",
       "1020789 -0.481001 -1.184623  \n",
       "103827  -0.432351  0.354759  \n",
       "1064906 -0.747770  0.295944  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def _entropy_from_counts(counts: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    s = counts.sum()\n",
    "    if s <= 0:\n",
    "        return 0.0\n",
    "    p = counts / (s + eps)\n",
    "    p = p[p > 0]\n",
    "    return float(-(p * np.log(p + eps)).sum())\n",
    "\n",
    "def _haversine_km(lat1, lon1, lat2, lon2):\n",
    "\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    c = 2*np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "def build_user_features_from_checkins(\n",
    "    users_final: pd.DataFrame,\n",
    "    checkins_final: pd.DataFrame,\n",
    "    log1p_counts: bool = True,\n",
    "    standardize: bool = True\n",
    "):\n",
    "\n",
    "    chk = checkins_final.copy()\n",
    "    chk[\"user_id\"] = chk[\"user_id\"].astype(str)\n",
    "    chk[\"venue_id\"] = chk[\"venue_id\"].astype(str)\n",
    "    chk[\"ts\"] = pd.to_datetime(chk[\"ts\"], errors=\"coerce\")\n",
    "    chk = chk.dropna(subset=[\"ts\", \"lat\", \"lon\", \"user_id\"])\n",
    "    chk[\"hour\"] = chk[\"ts\"].dt.hour.astype(int)\n",
    "    chk[\"dow\"]  = chk[\"ts\"].dt.dayofweek.astype(int)  #\n",
    "    chk[\"date\"] = chk[\"ts\"].dt.date\n",
    "\n",
    "    g = chk.groupby(\"user_id\", sort=False)\n",
    "\n",
    "\n",
    "    num_checkins = g.size().rename(\"num_checkins\")\n",
    "\n",
    "    num_active_days = g[\"date\"].nunique().rename(\"num_active_days\")\n",
    "    num_unique_venues = g[\"venue_id\"].nunique().rename(\"num_unique_venues\")\n",
    "\n",
    "    mean_lat = g[\"lat\"].mean().rename(\"mean_lat\")\n",
    "    mean_lon = g[\"lon\"].mean().rename(\"mean_lon\")\n",
    "    std_lat  = g[\"lat\"].std(ddof=0).fillna(0.0).rename(\"std_lat\")\n",
    "    std_lon  = g[\"lon\"].std(ddof=0).fillna(0.0).rename(\"std_lon\")\n",
    "\n",
    "    rog = {}\n",
    "    med_dist = {}\n",
    "    for uid, sub in g:\n",
    "        latc = float(sub[\"lat\"].mean())\n",
    "        lonc = float(sub[\"lon\"].mean())\n",
    "        d = _haversine_km(sub[\"lat\"].to_numpy(), sub[\"lon\"].to_numpy(), latc, lonc)\n",
    "        rog[uid] = float(np.sqrt(np.mean(d**2))) if len(d) else 0.0\n",
    "        med_dist[uid] = float(np.median(d)) if len(d) else 0.0\n",
    "    rog = pd.Series(rog, name=\"radius_of_gyration_km\")\n",
    "    med_dist = pd.Series(med_dist, name=\"median_dist_to_centroid_km\")\n",
    "\n",
    "\n",
    "    hour_counts = pd.crosstab(chk[\"user_id\"], chk[\"hour\"])  \n",
    "    for h in range(24):\n",
    "        if h not in hour_counts.columns:\n",
    "            hour_counts[h] = 0\n",
    "    hour_counts = hour_counts[list(range(24))].sort_index(axis=1)\n",
    "\n",
    "    dow_counts = pd.crosstab(chk[\"user_id\"], chk[\"dow\"])\n",
    "    for d0 in range(7):\n",
    "        if d0 not in dow_counts.columns:\n",
    "            dow_counts[d0] = 0\n",
    "    dow_counts = dow_counts[list(range(7))].sort_index(axis=1)\n",
    "\n",
    "    hour_entropy = hour_counts.apply(lambda r: _entropy_from_counts(r.to_numpy()), axis=1).rename(\"hour_entropy\")\n",
    "    dow_entropy  = dow_counts.apply(lambda r: _entropy_from_counts(r.to_numpy()), axis=1).rename(\"dow_entropy\")\n",
    "\n",
    "    venue_entropy = g[\"venue_id\"].apply(lambda s: _entropy_from_counts(s.value_counts().to_numpy())).rename(\"venue_entropy\")\n",
    "\n",
    "    feat = pd.concat([\n",
    "        num_checkins, num_active_days, num_unique_venues,\n",
    "        mean_lat, mean_lon, std_lat, std_lon,\n",
    "        rog, med_dist,\n",
    "        hour_entropy, dow_entropy, venue_entropy\n",
    "    ], axis=1)\n",
    "\n",
    "    hour_prop = hour_counts.div(hour_counts.sum(axis=1).replace(0, np.nan), axis=0).fillna(0.0)\n",
    "    hour_prop.columns = [f\"hour_{h:02d}_p\" for h in hour_prop.columns]\n",
    "\n",
    "    dow_prop = dow_counts.div(dow_counts.sum(axis=1).replace(0, np.nan), axis=0).fillna(0.0)\n",
    "    dow_prop.columns = [f\"dow_{d0}_p\" for d0 in dow_prop.columns]\n",
    "\n",
    "    feat = feat.join(hour_prop, how=\"left\").join(dow_prop, how=\"left\").fillna(0.0)\n",
    "\n",
    "    if log1p_counts:\n",
    "        for c in [\"num_checkins\", \"num_active_days\", \"num_unique_venues\"]:\n",
    "            feat[c] = np.log1p(feat[c].astype(float))\n",
    "\n",
    "\n",
    "    user_order = users_final[\"user_id\"].astype(str).tolist()\n",
    "    feat = feat.reindex(user_order).fillna(0.0)\n",
    "    feat.index.name = \"user_id\"\n",
    "\n",
    "    if standardize:\n",
    "        mu = feat.mean(axis=0)\n",
    "        sd = feat.std(axis=0, ddof=0).replace(0, 1.0)\n",
    "        feat = (feat - mu) / sd\n",
    "\n",
    "    X_users = feat.to_numpy(dtype=np.float32)\n",
    "    feature_names = feat.columns.tolist()\n",
    "    return X_users, feat, feature_names\n",
    "\n",
    "X_users, feat_df, feature_names = build_user_features_from_checkins(\n",
    "    users_final, checkins_final,\n",
    "    log1p_counts=bool(CFG.get(\"features\", {}).get(\"log1p_counts\", True)),\n",
    "    standardize=bool(CFG.get(\"features\", {}).get(\"standardize\", True)),\n",
    ")\n",
    "\n",
    "LOGGER.info(f\"[C4] X_users shape = {X_users.shape} | #features={len(feature_names)}\")\n",
    "feat_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8ac3bf",
   "metadata": {},
   "source": [
    "### Step 5: GraphSAGE unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7372aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:48:19] INFO - [C5] device=cuda | N=178 | train_pos_edges=40 (frac=0.1) | in_dim=43\n",
      "[05:48:19] INFO - [C5] epochs=10 | batch_size=1024 | neg=5 | neighbor_sampling=(25, 10)\n",
      "[05:48:20] INFO - [C5] epoch 1/10 | avg_loss=1.3778\n",
      "[05:48:21] INFO - [C5] epoch 2/10 | avg_loss=1.2876\n",
      "[05:48:21] INFO - [C5] epoch 3/10 | avg_loss=1.2168\n",
      "[05:48:22] INFO - [C5] epoch 4/10 | avg_loss=1.1572\n",
      "[05:48:22] INFO - [C5] epoch 5/10 | avg_loss=1.1414\n",
      "[05:48:22] INFO - [C5] epoch 6/10 | avg_loss=1.1392\n",
      "[05:48:23] INFO - [C5] epoch 7/10 | avg_loss=1.1192\n",
      "[05:48:23] INFO - [C5] epoch 8/10 | avg_loss=1.1259\n",
      "[05:48:23] INFO - [C5] epoch 9/10 | avg_loss=1.1031\n",
      "[05:48:24] INFO - [C5] epoch 10/10 | avg_loss=1.1021\n",
      "[05:48:24] INFO - [C5] Z shape = (178, 128)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Step C5 requires PyTorch. Please install torch (GPU if available) before running C5.\") from e\n",
    "\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def build_id_maps(users_final: pd.DataFrame):\n",
    "    user_ids = users_final[\"user_id\"].astype(str).tolist()\n",
    "    id2idx = {u:i for i,u in enumerate(user_ids)}\n",
    "    idx2id = user_ids\n",
    "    return id2idx, idx2id\n",
    "\n",
    "def build_adj_list(edges_final: pd.DataFrame, id2idx: dict):\n",
    "    adj = defaultdict(list)\n",
    "    for u, v in zip(edges_final[\"u\"].astype(str), edges_final[\"v\"].astype(str)):\n",
    "        if u in id2idx and v in id2idx:\n",
    "            ui = id2idx[u]; vi = id2idx[v]\n",
    "            if ui != vi:\n",
    "                adj[ui].append(vi)\n",
    "                adj[vi].append(ui)\n",
    "    return adj\n",
    "\n",
    "def sample_neighbors(adj, nodes, sample_size: int):\n",
    "    out = []\n",
    "    for n in nodes.tolist():\n",
    "        neigh = adj.get(n, [])\n",
    "        if len(neigh) == 0:\n",
    "            out.append([n])\n",
    "        elif len(neigh) <= sample_size:\n",
    "            out.append(neigh)\n",
    "        else:\n",
    "            out.append(random.sample(neigh, sample_size))\n",
    "    return out\n",
    "\n",
    "class MeanAggregator(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(in_dim * 2, out_dim)\n",
    "\n",
    "    def forward(self, self_h, neigh_h):\n",
    "        return self.lin(torch.cat([self_h, neigh_h], dim=1))\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, embed_dim):\n",
    "        super().__init__()\n",
    "        self.agg1 = MeanAggregator(in_dim, hidden_dim)\n",
    "        self.agg2 = MeanAggregator(hidden_dim, embed_dim)\n",
    "\n",
    "    def forward_batch(self, x, adj, batch_nodes, sizes=(25,10)):\n",
    "        B = batch_nodes.shape[0]\n",
    "        S1, S2 = sizes\n",
    "\n",
    "        # Layer 1\n",
    "        neigh1 = sample_neighbors(adj, batch_nodes, S1)\n",
    "        self_h0 = x[batch_nodes] \n",
    "        neigh1_mean0 = []\n",
    "        for i in range(B):\n",
    "            idxs = torch.tensor(neigh1[i], device=x.device, dtype=torch.long)\n",
    "            neigh1_mean0.append(x[idxs].mean(dim=0))\n",
    "        neigh1_mean0 = torch.stack(neigh1_mean0, dim=0)\n",
    "        h1 = F.relu(self.agg1(self_h0, neigh1_mean0))  \n",
    "\n",
    "        # Layer 2 \n",
    "        neigh2 = sample_neighbors(adj, batch_nodes, S2)\n",
    "        neigh2_mean1 = []\n",
    "        for i in range(B):\n",
    "            idxs = torch.tensor(neigh2[i], device=x.device, dtype=torch.long)\n",
    "            n_nodes = idxs\n",
    "            nB = n_nodes.shape[0]\n",
    "            n_self0 = x[n_nodes]\n",
    "            n_neigh1 = sample_neighbors(adj, n_nodes, S1)\n",
    "            n_neigh_mean0 = []\n",
    "            for j in range(nB):\n",
    "                j_idxs = torch.tensor(n_neigh1[j], device=x.device, dtype=torch.long)\n",
    "                n_neigh_mean0.append(x[j_idxs].mean(dim=0))\n",
    "            n_neigh_mean0 = torch.stack(n_neigh_mean0, dim=0)\n",
    "            n_h1 = F.relu(self.agg1(n_self0, n_neigh_mean0))  \n",
    "            neigh2_mean1.append(n_h1.mean(dim=0))\n",
    "        neigh2_mean1 = torch.stack(neigh2_mean1, dim=0)\n",
    "\n",
    "        z = self.agg2(h1, neigh2_mean1)\n",
    "        z = F.normalize(z, p=2, dim=1)\n",
    "        return z\n",
    "\n",
    "def train_graphsage_unsup(\n",
    "    edges_final: pd.DataFrame,\n",
    "    users_final: pd.DataFrame,\n",
    "    X_users: np.ndarray,\n",
    "    hidden_dim=128,\n",
    "    embed_dim=128,\n",
    "    neighbor_sampling=(25,10),\n",
    "    epochs=3,\n",
    "    batch_size=1024,\n",
    "    num_negative=5,\n",
    "    lr=1e-3,\n",
    "    device=None,\n",
    "    seed=42,\n",
    "    train_edge_frac: float = 0.1,   \n",
    "):\n",
    "    if not (0.0 < train_edge_frac <= 1.0):\n",
    "        raise ValueError(\"train_edge_frac must be in (0, 1].\")\n",
    "\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "    id2idx, idx2id = build_id_maps(users_final)\n",
    "    adj = build_adj_list(edges_final, id2idx)\n",
    "\n",
    "\n",
    "    pos_u = []\n",
    "    pos_v = []\n",
    "    for u, v in zip(edges_final[\"u\"].astype(str), edges_final[\"v\"].astype(str)):\n",
    "        if u in id2idx and v in id2idx:\n",
    "            ui = id2idx[u]; vi = id2idx[v]\n",
    "            if ui != vi:\n",
    "                pos_u.append(ui); pos_v.append(vi)\n",
    "\n",
    "    pos_u = np.array(pos_u, dtype=np.int64)\n",
    "    pos_v = np.array(pos_v, dtype=np.int64)\n",
    "    if len(pos_u) == 0:\n",
    "        raise ValueError(\"No positive edges after filtering. Check Step 3 outputs.\")\n",
    "\n",
    "\n",
    "    if train_edge_frac < 1.0:\n",
    "        m = len(pos_u)\n",
    "        m_sub = max(1, int(m * train_edge_frac))\n",
    "        idx = np.random.choice(m, size=m_sub, replace=False)\n",
    "        pos_u = pos_u[idx]\n",
    "        pos_v = pos_v[idx]\n",
    "\n",
    "    N = len(idx2id)\n",
    "    in_dim = X_users.shape[1]\n",
    "\n",
    "    if device is None:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    x = torch.tensor(X_users, dtype=torch.float32, device=device)\n",
    "    model = GraphSAGE(in_dim=in_dim, hidden_dim=hidden_dim, embed_dim=embed_dim).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    LOGGER.info(f\"[C5] device={device} | N={N} | train_pos_edges={len(pos_u)} (frac={train_edge_frac}) | in_dim={in_dim}\")\n",
    "    LOGGER.info(f\"[C5] epochs={epochs} | batch_size={batch_size} | neg={num_negative} | neighbor_sampling={neighbor_sampling}\")\n",
    "\n",
    "    num_batches = int(np.ceil(len(pos_u) / batch_size))\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        perm = np.random.permutation(len(pos_u))\n",
    "        pu = pos_u[perm]; pv = pos_v[perm]\n",
    "        total_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        for b in range(num_batches):\n",
    "            s = b * batch_size\n",
    "            e = min((b+1) * batch_size, len(pu))\n",
    "            bu = torch.tensor(pu[s:e], device=device)\n",
    "            bv = torch.tensor(pv[s:e], device=device)\n",
    "\n",
    "            zu = model.forward_batch(x, adj, bu, sizes=neighbor_sampling)\n",
    "            zv = model.forward_batch(x, adj, bv, sizes=neighbor_sampling)\n",
    "\n",
    "            pos_logits = (zu * zv).sum(dim=1)\n",
    "\n",
    "            neg_v = torch.randint(low=0, high=N, size=(bu.shape[0], num_negative), device=device)\n",
    "            neg_logits = []\n",
    "            for j in range(num_negative):\n",
    "                zvn = model.forward_batch(x, adj, neg_v[:, j], sizes=neighbor_sampling)\n",
    "                neg_logits.append((zu * zvn).sum(dim=1))\n",
    "            neg_logits = torch.stack(neg_logits, dim=1)\n",
    "\n",
    "            loss_pos = F.binary_cross_entropy_with_logits(pos_logits, torch.ones_like(pos_logits))\n",
    "            loss_neg = F.binary_cross_entropy_with_logits(neg_logits, torch.zeros_like(neg_logits))\n",
    "            loss = loss_pos + loss_neg\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            total_loss += float(loss.item())\n",
    "\n",
    "        LOGGER.info(f\"[C5] epoch {ep}/{epochs} | avg_loss={(total_loss/num_batches):.4f}\")\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    Z = np.zeros((N, embed_dim), dtype=np.float32)\n",
    "    with torch.no_grad():\n",
    "        all_idx = torch.arange(N, device=device)\n",
    "        bs = 2048\n",
    "        for s in range(0, N, bs):\n",
    "            idx = all_idx[s:s+bs]\n",
    "            z = model.forward_batch(x, adj, idx, sizes=neighbor_sampling)\n",
    "            Z[s:s+len(idx)] = z.detach().cpu().numpy()\n",
    "\n",
    "    return Z, model\n",
    "\n",
    "\n",
    "m_cfg = CFG.get(\"model\", {})\n",
    "t_cfg = CFG.get(\"train\", {})\n",
    "\n",
    "\n",
    "TRAIN_EDGE_FRAC = float(t_cfg.get(\"train_edge_frac\", 0.1))  # default 10% if not set\n",
    "\n",
    "Z, sage_model = train_graphsage_unsup(\n",
    "    edges_final=edges_final,\n",
    "    users_final=users_final,\n",
    "    X_users=X_users,\n",
    "    hidden_dim=int(m_cfg.get(\"hidden_dim\", 128)),\n",
    "    embed_dim=int(m_cfg.get(\"embed_dim\", 128)),\n",
    "    neighbor_sampling=tuple(m_cfg.get(\"neighbor_sampling\", [25,10])),\n",
    "    epochs=int(t_cfg.get(\"epochs\", 3)),\n",
    "    batch_size=int(t_cfg.get(\"batch_size\", 1024)),\n",
    "    num_negative=int(t_cfg.get(\"num_negative\", 5)),\n",
    "    lr=float(t_cfg.get(\"lr\", 1e-3)),\n",
    "    seed=int(CFG.get(\"run\", {}).get(\"seed\", 42)),\n",
    "    train_edge_frac=TRAIN_EDGE_FRAC,   \n",
    ")\n",
    "\n",
    "LOGGER.info(f\"[C5] Z shape = {Z.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0839ec1f",
   "metadata": {},
   "source": [
    "### Step 6: kNN similarity graph + Leiden -> community labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4725a1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:48:24] INFO - [C6] building kNN graph: k=30, mutual=True, resolution=1.0\n",
      "[05:48:40] INFO - [C6] kNN edges kept = 3772\n",
      "[05:48:45] INFO - [C6] method=leiden | n_communities=5 | quality=4218.8898\n",
      "[05:48:45] INFO - [C6] stats: {'n_users': 178, 'n_communities': 5, 'largest_comm_size': 51, 'median_comm_size': 35.0, 'method': 'leiden', 'quality': 4218.88981186899}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(   user_id  community_id\n",
       " 0    10067             4\n",
       " 1  1010482             3\n",
       " 2  1020789             0\n",
       " 3   103827             0\n",
       " 4  1064906             3,\n",
       " community_id\n",
       " 0    51\n",
       " 1    38\n",
       " 2    35\n",
       " 3    32\n",
       " 4    22\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def l2_normalize_rows(X: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    n = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    return X / (n + eps)\n",
    "\n",
    "def build_knn_edges_cosine(Z: np.ndarray, k: int = 30, mutual: bool = True):\n",
    "\n",
    "    Z = l2_normalize_rows(Z.astype(np.float32))\n",
    "\n",
    "    try:\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "        nn = NearestNeighbors(n_neighbors=min(k + 1, Z.shape[0]), metric=\"cosine\", algorithm=\"auto\")\n",
    "        nn.fit(Z)\n",
    "        dist, ind = nn.kneighbors(Z, return_distance=True)  \n",
    "        sim = 1.0 - dist \n",
    "        ind = ind[:, 1:]\n",
    "        sim = sim[:, 1:]\n",
    "    except Exception:\n",
    "        N = Z.shape[0]\n",
    "        if N > 20000:\n",
    "            raise RuntimeError(\"sklearn not available and N is large; please install scikit-learn for kNN.\")\n",
    "        S = Z @ Z.T  \n",
    "        np.fill_diagonal(S, -np.inf)\n",
    "        ind = np.argpartition(-S, kth=min(k, N-1), axis=1)[:, :k]\n",
    "\n",
    "        row = np.arange(N)[:, None]\n",
    "        sims = S[row, ind]\n",
    "        order = np.argsort(-sims, axis=1)\n",
    "        ind = ind[row, order]\n",
    "        sim = sims[row, order]\n",
    "\n",
    "\n",
    "    src = np.repeat(np.arange(Z.shape[0]), ind.shape[1])\n",
    "    dst = ind.reshape(-1)\n",
    "    w = sim.reshape(-1)\n",
    "\n",
    "    if not mutual:\n",
    "        return src, dst, w\n",
    "\n",
    "\n",
    "    pairs = set(zip(src.tolist(), dst.tolist()))\n",
    "    keep = np.array([ (j, i) in pairs for i, j in zip(src, dst) ], dtype=bool)\n",
    "    return src[keep], dst[keep], w[keep]\n",
    "\n",
    "def leiden_partition_from_edges(n_nodes: int, src: np.ndarray, dst: np.ndarray, w: np.ndarray, resolution: float = 1.0):\n",
    "\n",
    "    try:\n",
    "        import igraph as ig\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"C6 requires python-igraph for Leiden. Install: pip install igraph\") from e\n",
    "\n",
    "    g = ig.Graph(n=n_nodes, edges=list(zip(src.tolist(), dst.tolist())), directed=False)\n",
    "    g.es[\"weight\"] = w.astype(float).tolist()\n",
    "\n",
    "\n",
    "    try:\n",
    "        import leidenalg\n",
    "        part = leidenalg.find_partition(\n",
    "            g,\n",
    "            leidenalg.RBConfigurationVertexPartition,\n",
    "            weights=\"weight\",\n",
    "            resolution_parameter=float(resolution),\n",
    "        )\n",
    "        labels = np.array(part.membership, dtype=np.int64)\n",
    "        method = \"leiden\"\n",
    "        quality = float(part.quality())\n",
    "    except Exception:\n",
    "\n",
    "        part = g.community_multilevel(weights=\"weight\")\n",
    "        labels = np.array(part.membership, dtype=np.int64)\n",
    "        method = \"igraph_multilevel\"\n",
    "        quality = float(part.modularity)\n",
    "\n",
    "    return labels, {\"method\": method, \"quality\": quality, \"n_communities\": int(labels.max() + 1)}\n",
    "\n",
    "# ---- run C6 ----\n",
    "c_cfg = CFG.get(\"community\", {}) if \"CFG\" in globals() else {}\n",
    "knn_k = int(c_cfg.get(\"knn_k\", 30))\n",
    "mutual_knn = bool(c_cfg.get(\"mutual_knn\", True))\n",
    "resolution = float(c_cfg.get(\"leiden_resolution\", 1.0))\n",
    "\n",
    "LOGGER.info(f\"[C6] building kNN graph: k={knn_k}, mutual={mutual_knn}, resolution={resolution}\")\n",
    "\n",
    "src, dst, w = build_knn_edges_cosine(Z, k=knn_k, mutual=mutual_knn)\n",
    "LOGGER.info(f\"[C6] kNN edges kept = {len(src)}\")\n",
    "\n",
    "labels, info = leiden_partition_from_edges(n_nodes=Z.shape[0], src=src, dst=dst, w=w, resolution=resolution)\n",
    "LOGGER.info(f\"[C6] method={info['method']} | n_communities={info['n_communities']} | quality={info['quality']:.4f}\")\n",
    "\n",
    "comm_df = pd.DataFrame({\n",
    "    \"user_id\": users_final[\"user_id\"].astype(str).tolist(),\n",
    "    \"community_id\": labels.astype(int)\n",
    "})\n",
    "\n",
    "\n",
    "sizes = comm_df[\"community_id\"].value_counts().sort_values(ascending=False)\n",
    "comm_stats = {\n",
    "    \"n_users\": int(len(comm_df)),\n",
    "    \"n_communities\": int(info[\"n_communities\"]),\n",
    "    \"largest_comm_size\": int(sizes.iloc[0]) if len(sizes) else 0,\n",
    "    \"median_comm_size\": float(sizes.median()) if len(sizes) else 0.0,\n",
    "    \"method\": info[\"method\"],\n",
    "    \"quality\": info[\"quality\"],\n",
    "}\n",
    "LOGGER.info(f\"[C6] stats: {comm_stats}\")\n",
    "\n",
    "comm_df.head(), sizes.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd37825a",
   "metadata": {},
   "source": [
    "### Step 7: metrics + random baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d42f3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05:48:45] INFO - [C7] computing user centroids from checkins ...\n",
      "[05:48:45] INFO - [C7] spatial cohesion per community ...\n",
      "[05:48:45] INFO - [C7] structural metrics via igraph ...\n",
      "[05:48:46] INFO - [C7] random baselines: runs=10 (shuffle labels, keep size distribution)\n",
      "[05:48:46] INFO - [C7] global metrics: {'n_users': 178, 'n_communities': 5, 'modularity': 0.6231763450693492, 'conductance_mean': 0.1660330879311771, 'conductance_median': 0.1, 'intra_density_mean': 0.1159609492917193, 'intra_density_median': 0.08907563025210084, 'spatial_median_km_global': 570.2890844457413, 'spatial_random_median_km_mean': 7788.158492197111, 'spatial_random_median_km_std': 270.8763507784968, 'spatial_zscore_vs_random': -26.64636239748222}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'n_users': 178,\n",
       "  'n_communities': 5,\n",
       "  'modularity': 0.6231763450693492,\n",
       "  'conductance_mean': 0.1660330879311771,\n",
       "  'conductance_median': 0.1,\n",
       "  'intra_density_mean': 0.1159609492917193,\n",
       "  'intra_density_median': 0.08907563025210084,\n",
       "  'spatial_median_km_global': 570.2890844457413,\n",
       "  'spatial_random_median_km_mean': 7788.158492197111,\n",
       "  'spatial_random_median_km_std': 270.8763507784968,\n",
       "  'spatial_zscore_vs_random': -26.64636239748222},\n",
       "    community_id  comm_size  spatial_median_km  spatial_mean_km\n",
       " 0             0         51        2355.710506      2447.673152\n",
       " 1             1         38         570.289084      1150.922755\n",
       " 2             2         35         564.708539       961.085564\n",
       " 3             3         32         173.740222       686.017419\n",
       " 4             4         22        1381.782224      1401.957075)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def haversine_km_vec(lat, lon, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat = np.radians(lat); lon = np.radians(lon)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat\n",
    "    dlon = lon2 - lon\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat)*np.cos(lat2)*np.sin(dlon/2.0)**2\n",
    "    c = 2*np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "def compute_user_centroids(checkins_final: pd.DataFrame) -> pd.DataFrame:\n",
    "    chk = checkins_final.copy()\n",
    "    chk[\"user_id\"] = chk[\"user_id\"].astype(str)\n",
    "    cent = chk.groupby(\"user_id\")[[\"lat\",\"lon\"]].mean().reset_index()\n",
    "    cent.columns = [\"user_id\", \"user_lat\", \"user_lon\"]\n",
    "    return cent\n",
    "\n",
    "def structural_metrics_igraph(edges_final: pd.DataFrame, comm_df: pd.DataFrame):\n",
    "\n",
    "    import igraph as ig\n",
    "\n",
    "    users = comm_df[\"user_id\"].astype(str).tolist()\n",
    "    id2idx = {u:i for i,u in enumerate(users)}\n",
    "\n",
    "    # Build graph\n",
    "    e = edges_final.copy()\n",
    "    e[\"u\"] = e[\"u\"].astype(str); e[\"v\"] = e[\"v\"].astype(str)\n",
    "    e = e[e[\"u\"].isin(id2idx) & e[\"v\"].isin(id2idx)]\n",
    "    edges_idx = list(zip(e[\"u\"].map(id2idx).tolist(), e[\"v\"].map(id2idx).tolist()))\n",
    "    g = ig.Graph(n=len(users), edges=edges_idx, directed=False)\n",
    "\n",
    "    labels = comm_df.set_index(\"user_id\").loc[users, \"community_id\"].to_numpy()\n",
    "    mod = float(g.modularity(labels))\n",
    "\n",
    "    deg = np.array(g.degree(), dtype=np.float64)\n",
    "    total_vol = deg.sum()\n",
    "    comm_to_nodes = {}\n",
    "    for i, c in enumerate(labels):\n",
    "        comm_to_nodes.setdefault(int(c), []).append(i)\n",
    "\n",
    "    adj = [set(g.neighbors(i)) for i in range(g.vcount())]\n",
    "\n",
    "    conductances = []\n",
    "    densities = []\n",
    "    for c, nodes in comm_to_nodes.items():\n",
    "        nodes_set = set(nodes)\n",
    "        cut = 0\n",
    "        internal = 0\n",
    "        for u in nodes:\n",
    "            for v in adj[u]:\n",
    "                if v in nodes_set:\n",
    "                    internal += 1\n",
    "                else:\n",
    "                    cut += 1\n",
    "\n",
    "        internal = internal / 2.0\n",
    "        volS = deg[list(nodes)].sum()\n",
    "        volT = total_vol - volS\n",
    "        denom = min(volS, volT) if min(volS, volT) > 0 else np.nan\n",
    "        phi = (cut / denom) if denom and not np.isnan(denom) else np.nan\n",
    "        conductances.append(phi)\n",
    "\n",
    "        n = len(nodes)\n",
    "        possible = n * (n - 1) / 2.0\n",
    "        dens = (internal / possible) if possible > 0 else np.nan\n",
    "        densities.append(dens)\n",
    "\n",
    "    return {\n",
    "        \"modularity\": mod,\n",
    "        \"conductance_mean\": float(np.nanmean(conductances)) if len(conductances) else np.nan,\n",
    "        \"conductance_median\": float(np.nanmedian(conductances)) if len(conductances) else np.nan,\n",
    "        \"intra_density_mean\": float(np.nanmean(densities)) if len(densities) else np.nan,\n",
    "        \"intra_density_median\": float(np.nanmedian(densities)) if len(densities) else np.nan,\n",
    "    }\n",
    "\n",
    "def spatial_cohesion_metrics(comm_df: pd.DataFrame, user_centroids: pd.DataFrame):\n",
    "\n",
    "    df = comm_df.merge(user_centroids, on=\"user_id\", how=\"left\").dropna(subset=[\"user_lat\",\"user_lon\"])\n",
    "    g = df.groupby(\"community_id\")\n",
    "\n",
    "    rows = []\n",
    "    for cid, sub in g:\n",
    "        latc = sub[\"user_lat\"].mean()\n",
    "        lonc = sub[\"user_lon\"].mean()\n",
    "        d = haversine_km_vec(sub[\"user_lat\"].to_numpy(), sub[\"user_lon\"].to_numpy(), latc, lonc)\n",
    "        rows.append({\n",
    "            \"community_id\": int(cid),\n",
    "            \"comm_size\": int(len(sub)),\n",
    "            \"spatial_median_km\": float(np.median(d)) if len(d) else np.nan,\n",
    "            \"spatial_mean_km\": float(np.mean(d)) if len(d) else np.nan,\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def random_baseline_same_sizes(comm_df: pd.DataFrame, n_runs: int = 10, seed: int = 42):\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    labels = comm_df[\"community_id\"].to_numpy()\n",
    "\n",
    "    baselines = []\n",
    "    for r in range(n_runs):\n",
    "        perm = labels.copy()\n",
    "        rng.shuffle(perm)\n",
    "        baselines.append(perm)\n",
    "    return baselines\n",
    "\n",
    "\n",
    "seed = int(CFG.get(\"run\", {}).get(\"seed\", 42)) if \"CFG\" in globals() else 42\n",
    "n_rand = int(CFG.get(\"metrics\", {}).get(\"random_baseline_runs\", 10)) if \"CFG\" in globals() else 10\n",
    "\n",
    "LOGGER.info(\"[C7] computing user centroids from checkins ...\")\n",
    "user_centroids = compute_user_centroids(checkins_final)\n",
    "\n",
    "LOGGER.info(\"[C7] spatial cohesion per community ...\")\n",
    "comm_spatial = spatial_cohesion_metrics(comm_df, user_centroids)\n",
    "\n",
    "\n",
    "try:\n",
    "    import igraph  \n",
    "    LOGGER.info(\"[C7] structural metrics via igraph ...\")\n",
    "    struct = structural_metrics_igraph(edges_final, comm_df)\n",
    "except Exception as e:\n",
    "    struct = {\n",
    "        \"modularity\": np.nan,\n",
    "        \"conductance_mean\": np.nan,\n",
    "        \"conductance_median\": np.nan,\n",
    "        \"intra_density_mean\": np.nan,\n",
    "        \"intra_density_median\": np.nan,\n",
    "    }\n",
    "    LOGGER.warning(f\"[C7] igraph not available for structural metrics. Install igraph for modularity/conductance. Reason: {type(e).__name__}: {e}\")\n",
    "\n",
    "\n",
    "LOGGER.info(f\"[C7] random baselines: runs={n_rand} (shuffle labels, keep size distribution)\")\n",
    "baseline_labels_list = random_baseline_same_sizes(comm_df, n_runs=n_rand, seed=seed)\n",
    "\n",
    "baseline_spatial_medians = []\n",
    "for perm_labels in baseline_labels_list:\n",
    "    tmp = comm_df.copy()\n",
    "    tmp[\"community_id\"] = perm_labels\n",
    "    tmp_sp = spatial_cohesion_metrics(tmp, user_centroids)\n",
    "    baseline_spatial_medians.append(float(tmp_sp[\"spatial_median_km\"].median()))\n",
    "\n",
    "baseline_spatial_medians = np.array(baseline_spatial_medians, dtype=float)\n",
    "\n",
    "\n",
    "obs_global_spatial = float(comm_spatial[\"spatial_median_km\"].median()) if len(comm_spatial) else np.nan\n",
    "rand_mean = float(np.nanmean(baseline_spatial_medians)) if len(baseline_spatial_medians) else np.nan\n",
    "rand_std  = float(np.nanstd(baseline_spatial_medians)) if len(baseline_spatial_medians) else np.nan\n",
    "z_score = (obs_global_spatial - rand_mean) / (rand_std + 1e-12) if np.isfinite(obs_global_spatial) else np.nan\n",
    "\n",
    "metrics_global = {\n",
    "    \"n_users\": int(len(comm_df)),\n",
    "    \"n_communities\": int(comm_df[\"community_id\"].nunique()),\n",
    "    **struct,\n",
    "    \"spatial_median_km_global\": obs_global_spatial,\n",
    "    \"spatial_random_median_km_mean\": rand_mean,\n",
    "    \"spatial_random_median_km_std\": rand_std,\n",
    "    \"spatial_zscore_vs_random\": float(z_score),\n",
    "}\n",
    "\n",
    "LOGGER.info(f\"[C7] global metrics: {metrics_global}\")\n",
    "\n",
    "comm_metrics_df = comm_spatial.sort_values([\"comm_size\"], ascending=False).reset_index(drop=True)\n",
    "\n",
    "metrics_global, comm_metrics_df.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
