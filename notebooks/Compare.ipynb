{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b79bf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT = /mnt/d/community-detection\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, Optional, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 160)\n",
    "\n",
    "def find_repo_root(start: Path | None = None) -> Path:\n",
    "    \"\"\"\n",
    "    Tìm PROJECT_ROOT bằng cách scan lên trên cho tới khi thấy:\n",
    "    - pyproject.toml hoặc\n",
    "    - README.md hoặc\n",
    "    - configs/default.yaml hoặc\n",
    "    - src/\n",
    "    \"\"\"\n",
    "    cur = (start or Path.cwd()).resolve()\n",
    "    markers = [\n",
    "        \"pyproject.toml\",\n",
    "        \"README.md\",\n",
    "        \"configs/default.yaml\",\n",
    "        \"src\",\n",
    "        \".git\",\n",
    "    ]\n",
    "    for _ in range(20):\n",
    "        for m in markers:\n",
    "            if (cur / m).exists():\n",
    "                return cur\n",
    "        if cur.parent == cur:\n",
    "            break\n",
    "        cur = cur.parent\n",
    "    # fallback: assume notebook under PROJECT_ROOT/notebooks\n",
    "    return (Path.cwd().resolve().parent)\n",
    "\n",
    "PROJECT_ROOT = find_repo_root()\n",
    "print(\"PROJECT_ROOT =\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddebcc17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brightkite run: /mnt/d/community-detection/data/processed/_runs/20251214_192049\n",
      "LBSN2Vec++ run: /mnt/d/community-detection/data/processed/_runs/20251214_183903\n"
     ]
    }
   ],
   "source": [
    "RUN_BRIGHTKITE = PROJECT_ROOT / \"data/processed/_runs/20251214_192049\"\n",
    "RUN_LBSN2VEC   = PROJECT_ROOT / \"data/processed/_runs/20251214_183903\"\n",
    "\n",
    "assert RUN_BRIGHTKITE.exists(), f\"Không thấy run: {RUN_BRIGHTKITE}\"\n",
    "assert RUN_LBSN2VEC.exists(), f\"Không thấy run: {RUN_LBSN2VEC}\"\n",
    "\n",
    "print(\"Brightkite run:\", RUN_BRIGHTKITE)\n",
    "print(\"LBSN2Vec++ run:\", RUN_LBSN2VEC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97edc147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Brightkite run files (top) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>ext</th>\n",
       "      <th>size_kb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>run_config.json</td>\n",
       "      <td>.json</td>\n",
       "      <td>1.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>brightkite/metrics_global.json</td>\n",
       "      <td>.json</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brightkite/Z.npy</td>\n",
       "      <td>.npy</td>\n",
       "      <td>4292.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brightkite/X_users.npy</td>\n",
       "      <td>.npy</td>\n",
       "      <td>1440.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>brightkite/checkins_clean.parquet</td>\n",
       "      <td>.parquet</td>\n",
       "      <td>70077.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>brightkite/checkins_final.parquet</td>\n",
       "      <td>.parquet</td>\n",
       "      <td>48604.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>brightkite/feat_df.parquet</td>\n",
       "      <td>.parquet</td>\n",
       "      <td>1892.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>brightkite/edges_clean.parquet</td>\n",
       "      <td>.parquet</td>\n",
       "      <td>477.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>brightkite/edges_final.parquet</td>\n",
       "      <td>.parquet</td>\n",
       "      <td>202.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brightkite/comm_df.parquet</td>\n",
       "      <td>.parquet</td>\n",
       "      <td>56.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>brightkite/users_final.parquet</td>\n",
       "      <td>.parquet</td>\n",
       "      <td>51.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>brightkite/comm_metrics.parquet</td>\n",
       "      <td>.parquet</td>\n",
       "      <td>3.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 path       ext   size_kb\n",
       "11                    run_config.json     .json      1.34\n",
       "9      brightkite/metrics_global.json     .json      0.76\n",
       "1                    brightkite/Z.npy      .npy   4292.34\n",
       "0              brightkite/X_users.npy      .npy   1440.87\n",
       "2   brightkite/checkins_clean.parquet  .parquet  70077.75\n",
       "3   brightkite/checkins_final.parquet  .parquet  48604.95\n",
       "8          brightkite/feat_df.parquet  .parquet   1892.42\n",
       "6      brightkite/edges_clean.parquet  .parquet    477.91\n",
       "7      brightkite/edges_final.parquet  .parquet    202.74\n",
       "4          brightkite/comm_df.parquet  .parquet     56.64\n",
       "10     brightkite/users_final.parquet  .parquet     51.85\n",
       "5     brightkite/comm_metrics.parquet  .parquet      3.32"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== LBSN2Vec++ run files (top) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>ext</th>\n",
       "      <th>size_kb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>run_config.json</td>\n",
       "      <td>.json</td>\n",
       "      <td>1.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lbsn2vec/metrics_global.json</td>\n",
       "      <td>.json</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lbsn2vec/Z.npy</td>\n",
       "      <td>.npy</td>\n",
       "      <td>5934.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lbsn2vec/X_users.npy</td>\n",
       "      <td>.npy</td>\n",
       "      <td>1992.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lbsn2vec/checkins_clean.parquet</td>\n",
       "      <td>.parquet</td>\n",
       "      <td>494019.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lbsn2vec/checkins_final.parquet</td>\n",
       "      <td>.parquet</td>\n",
       "      <td>115684.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lbsn2vec/feat_df.parquet</td>\n",
       "      <td>.parquet</td>\n",
       "      <td>2663.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lbsn2vec/edges_clean.parquet</td>\n",
       "      <td>.parquet</td>\n",
       "      <td>647.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lbsn2vec/edges_final.parquet</td>\n",
       "      <td>.parquet</td>\n",
       "      <td>282.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lbsn2vec/comm_df.parquet</td>\n",
       "      <td>.parquet</td>\n",
       "      <td>90.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>lbsn2vec/users_final.parquet</td>\n",
       "      <td>.parquet</td>\n",
       "      <td>84.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lbsn2vec/comm_metrics.parquet</td>\n",
       "      <td>.parquet</td>\n",
       "      <td>9.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               path       ext    size_kb\n",
       "11                  run_config.json     .json       1.34\n",
       "9      lbsn2vec/metrics_global.json     .json       0.93\n",
       "1                    lbsn2vec/Z.npy      .npy    5934.79\n",
       "0              lbsn2vec/X_users.npy      .npy    1992.68\n",
       "2   lbsn2vec/checkins_clean.parquet  .parquet  494019.84\n",
       "3   lbsn2vec/checkins_final.parquet  .parquet  115684.54\n",
       "8          lbsn2vec/feat_df.parquet  .parquet    2663.53\n",
       "6      lbsn2vec/edges_clean.parquet  .parquet     647.93\n",
       "7      lbsn2vec/edges_final.parquet  .parquet     282.33\n",
       "4          lbsn2vec/comm_df.parquet  .parquet      90.90\n",
       "10     lbsn2vec/users_final.parquet  .parquet      84.41\n",
       "5     lbsn2vec/comm_metrics.parquet  .parquet       9.50"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def list_run_files(run_dir: Path, limit: int = 120) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for p in sorted([x for x in run_dir.rglob(\"*\") if x.is_file()]):\n",
    "        rows.append({\n",
    "            \"path\": str(p.relative_to(run_dir)),\n",
    "            \"ext\": p.suffix.lower(),\n",
    "            \"size_kb\": round(p.stat().st_size / 1024, 2)\n",
    "        })\n",
    "    df = pd.DataFrame(rows).sort_values([\"ext\", \"size_kb\"], ascending=[True, False])\n",
    "    return df.head(limit)\n",
    "\n",
    "print(\"=== Brightkite run files (top) ===\")\n",
    "display(list_run_files(RUN_BRIGHTKITE))\n",
    "\n",
    "print(\"=== LBSN2Vec++ run files (top) ===\")\n",
    "display(list_run_files(RUN_LBSN2VEC))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2876010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(path: Path) -> Dict[str, Any]:\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def read_df(path: Path) -> pd.DataFrame:\n",
    "    suf = path.suffix.lower()\n",
    "    if suf == \".parquet\":\n",
    "        return pd.read_parquet(path)\n",
    "    if suf == \".csv\":\n",
    "        return pd.read_csv(path)\n",
    "    if suf in (\".tsv\", \".txt\"):\n",
    "        return pd.read_csv(path, sep=\"\\t\")\n",
    "    raise ValueError(f\"Unsupported file: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8dfc2bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPECTED = {\n",
    "    \"run_config\": [\"run_config.json\"],\n",
    "    \"comm_stats\": [\"comm_stats.json\", \"community_stats.json\"],\n",
    "    \"metrics_global\": [\"metrics_global.json\", \"global_metrics.json\"],\n",
    "    \"comm_metrics_df\": [\"comm_metrics_df.parquet\", \"community_metrics.parquet\", \"comm_metrics.parquet\", \"comm_metrics_df.csv\", \"community_metrics.csv\"],\n",
    "    \"comm_df\": [\"comm_df.parquet\", \"community_labels.parquet\", \"labels.parquet\", \"comm_df.csv\", \"community_labels.csv\", \"labels.csv\"],\n",
    "    # optional for semantic:\n",
    "    \"checkins\": [\"checkins.parquet\", \"checkins_final.parquet\", \"checkins.csv\", \"checkins_final.csv\"],\n",
    "    \"pois\": [\"pois.parquet\", \"poi.parquet\", \"venues.parquet\", \"pois.csv\", \"poi.csv\", \"venues.csv\"],\n",
    "    \"feat_df\": [\"feat_df.parquet\", \"features.parquet\", \"user_features.parquet\", \"feat_df.csv\", \"features.csv\", \"user_features.csv\"],\n",
    "}\n",
    "\n",
    "def find_first_existing(run_dir: Path, names: List[str]) -> Optional[Path]:\n",
    "    for n in names:\n",
    "        p = run_dir / n\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def find_by_fuzzy(run_dir: Path, kind: str) -> Optional[Path]:\n",
    "    \"\"\"\n",
    "    Fallback: tìm file có chứa keyword trong tên.\n",
    "    \"\"\"\n",
    "    files = [p for p in run_dir.rglob(\"*\") if p.is_file()]\n",
    "    kind_l = kind.lower()\n",
    "\n",
    "    # heuristic patterns\n",
    "    patterns = {\n",
    "        \"comm_stats\": [r\"comm.*stats\", r\"community.*stats\", r\"cluster.*stats\"],\n",
    "        \"metrics_global\": [r\"metrics.*global\", r\"global.*metrics\", r\"summary.*metrics\"],\n",
    "        \"comm_metrics_df\": [r\"comm.*metrics\", r\"community.*metrics\", r\"cohesion\", r\"per.*community\"],\n",
    "        \"comm_df\": [r\"comm.*df\", r\"community.*labels\", r\"labels\", r\"community_id\"],\n",
    "        \"checkins\": [r\"checkins\", r\"checkin\"],\n",
    "        \"pois\": [r\"\\bpoi\\b\", r\"pois\", r\"venues\", r\"venue\"],\n",
    "        \"feat_df\": [r\"feat\", r\"features\", r\"x_users\", r\"user_features\"],\n",
    "    }\n",
    "\n",
    "    pats = patterns.get(kind_l, [kind_l])\n",
    "\n",
    "    # preference: json for json kinds, parquet then csv for df kinds\n",
    "    if kind_l in (\"comm_stats\", \"metrics_global\"):\n",
    "        cands = [p for p in files if p.suffix.lower() == \".json\"]\n",
    "    else:\n",
    "        cands = [p for p in files if p.suffix.lower() in (\".parquet\", \".csv\", \".tsv\", \".txt\")]\n",
    "\n",
    "    scored: List[Tuple[int, Path]] = []\n",
    "    for p in cands:\n",
    "        name = p.name.lower()\n",
    "        score = 1000\n",
    "        if p.suffix.lower() == \".parquet\":\n",
    "            score -= 50\n",
    "        if p.suffix.lower() == \".json\":\n",
    "            score -= 10\n",
    "\n",
    "        for i, pat in enumerate(pats):\n",
    "            if re.search(pat, name):\n",
    "                score -= (300 - i)\n",
    "\n",
    "        # ưu tiên file nằm gần root run_dir\n",
    "        depth = len(p.relative_to(run_dir).parts)\n",
    "        score += depth * 3\n",
    "        scored.append((score, p))\n",
    "\n",
    "    scored.sort(key=lambda x: x[0])\n",
    "    return scored[0][1] if scored else None\n",
    "\n",
    "def resolve_artifact(run_dir: Path, kind: str, required: bool = True) -> Optional[Path]:\n",
    "    # 1) exact expected\n",
    "    p = find_first_existing(run_dir, EXPECTED.get(kind, []))\n",
    "    if p:\n",
    "        return p\n",
    "    # 2) fuzzy fallback\n",
    "    p = find_by_fuzzy(run_dir, kind)\n",
    "    if p:\n",
    "        print(f\"[WARN] {kind}: không thấy tên chuẩn, dùng fallback -> {p.relative_to(run_dir)}\")\n",
    "        return p\n",
    "\n",
    "    if required:\n",
    "        print(f\"[ERROR] Missing required artifact '{kind}' in {run_dir}\")\n",
    "        print(\"Files available:\")\n",
    "        display(list_run_files(run_dir, limit=80))\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77e3573d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] comm_stats: không thấy tên chuẩn, dùng fallback -> run_config.json\n",
      "[WARN] metrics_global: không thấy tên chuẩn, dùng fallback -> brightkite/metrics_global.json\n",
      "[WARN] comm_metrics_df: không thấy tên chuẩn, dùng fallback -> brightkite/comm_metrics.parquet\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Could not open Parquet input source '<Buffer>': Couldn't deserialize thrift: invalid TType\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     34\u001b[39m     out[\u001b[33m\"\u001b[39m\u001b[33mpois\u001b[39m\u001b[33m\"\u001b[39m] = read_df(p_poi) \u001b[38;5;28;01mif\u001b[39;00m p_poi \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m bk = \u001b[43mload_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRUN_BRIGHTKITE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbrightkite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m lb = load_run(RUN_LBSN2VEC, \u001b[33m\"\u001b[39m\u001b[33mlbsn2vec\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow_loaded\u001b[39m(bundle: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) -> pd.DataFrame:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mload_run\u001b[39m\u001b[34m(run_dir, label)\u001b[39m\n\u001b[32m     16\u001b[39m out[\u001b[33m\"\u001b[39m\u001b[33mcomm_stats\u001b[39m\u001b[33m\"\u001b[39m] = read_json(p_cs) \u001b[38;5;28;01mif\u001b[39;00m p_cs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     17\u001b[39m out[\u001b[33m\"\u001b[39m\u001b[33mmetrics_global\u001b[39m\u001b[33m\"\u001b[39m] = read_json(p_mg) \u001b[38;5;28;01mif\u001b[39;00m p_mg \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m out[\u001b[33m\"\u001b[39m\u001b[33mcomm_metrics_df\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mread_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp_cm\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m p_cm \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# optional\u001b[39;00m\n\u001b[32m     21\u001b[39m p_comm = resolve_artifact(run_dir, \u001b[33m\"\u001b[39m\u001b[33mcomm_df\u001b[39m\u001b[33m\"\u001b[39m, required=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mread_df\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m      6\u001b[39m suf = path.suffix.lower()\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m suf == \u001b[33m\"\u001b[39m\u001b[33m.parquet\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m suf == \u001b[33m\"\u001b[39m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.read_csv(path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/community-detection/.venv/lib/python3.11/site-packages/pandas/io/parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/community-detection/.venv/lib/python3.11/site-packages/pandas/io/parquet.py:265\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    258\u001b[39m path_or_handle, handles, filesystem = _get_path_or_handle(\n\u001b[32m    259\u001b[39m     path,\n\u001b[32m    260\u001b[39m     filesystem,\n\u001b[32m    261\u001b[39m     storage_options=storage_options,\n\u001b[32m    262\u001b[39m     mode=\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    263\u001b[39m )\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m     pa_table = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m catch_warnings():\n\u001b[32m    274\u001b[39m         filterwarnings(\n\u001b[32m    275\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    276\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmake_block is deprecated\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    277\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    278\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/community-detection/.venv/lib/python3.11/site-packages/pyarrow/parquet/core.py:1844\u001b[39m, in \u001b[36mread_table\u001b[39m\u001b[34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, binary_type, list_type, memory_map, buffer_size, partitioning, filesystem, filters, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, arrow_extensions_enabled)\u001b[39m\n\u001b[32m   1832\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread_table\u001b[39m(source, *, columns=\u001b[38;5;28;01mNone\u001b[39;00m, use_threads=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1833\u001b[39m                schema=\u001b[38;5;28;01mNone\u001b[39;00m, use_pandas_metadata=\u001b[38;5;28;01mFalse\u001b[39;00m, read_dictionary=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1834\u001b[39m                binary_type=\u001b[38;5;28;01mNone\u001b[39;00m, list_type=\u001b[38;5;28;01mNone\u001b[39;00m, memory_map=\u001b[38;5;28;01mFalse\u001b[39;00m, buffer_size=\u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1840\u001b[39m                page_checksum_verification=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1841\u001b[39m                arrow_extensions_enabled=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1843\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1844\u001b[39m         dataset = \u001b[43mParquetDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1845\u001b[39m \u001b[43m            \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1846\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1847\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1848\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpartitioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1849\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1850\u001b[39m \u001b[43m            \u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mread_dictionary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1851\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbinary_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbinary_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1852\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlist_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlist_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1853\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1854\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1855\u001b[39m \u001b[43m            \u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_prefixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1856\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpre_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1857\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoerce_int96_timestamp_unit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1858\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecryption_properties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1859\u001b[39m \u001b[43m            \u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_string_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1860\u001b[39m \u001b[43m            \u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mthrift_container_size_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1861\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpage_checksum_verification\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1862\u001b[39m \u001b[43m            \u001b[49m\u001b[43marrow_extensions_enabled\u001b[49m\u001b[43m=\u001b[49m\u001b[43marrow_extensions_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1863\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1864\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m   1865\u001b[39m         \u001b[38;5;66;03m# fall back on ParquetFile for simple cases when pyarrow.dataset\u001b[39;00m\n\u001b[32m   1866\u001b[39m         \u001b[38;5;66;03m# module is not available\u001b[39;00m\n\u001b[32m   1867\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m filters \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/community-detection/.venv/lib/python3.11/site-packages/pyarrow/parquet/core.py:1413\u001b[39m, in \u001b[36mParquetDataset.__init__\u001b[39m\u001b[34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, binary_type, list_type, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, arrow_extensions_enabled)\u001b[39m\n\u001b[32m   1409\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m single_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1410\u001b[39m     fragment = parquet_format.make_fragment(single_file, filesystem)\n\u001b[32m   1412\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset = ds.FileSystemDataset(\n\u001b[32m-> \u001b[39m\u001b[32m1413\u001b[39m         [fragment], schema=schema \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mfragment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mphysical_schema\u001b[49m,\n\u001b[32m   1414\u001b[39m         \u001b[38;5;28mformat\u001b[39m=parquet_format,\n\u001b[32m   1415\u001b[39m         filesystem=fragment.filesystem\n\u001b[32m   1416\u001b[39m     )\n\u001b[32m   1417\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1419\u001b[39m \u001b[38;5;66;03m# check partitioning to enable dictionary encoding\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/community-detection/.venv/lib/python3.11/site-packages/pyarrow/_dataset.pyx:1477\u001b[39m, in \u001b[36mpyarrow._dataset.Fragment.physical_schema.__get__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/community-detection/.venv/lib/python3.11/site-packages/pyarrow/error.pxi:155\u001b[39m, in \u001b[36mpyarrow.lib.pyarrow_internal_check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/community-detection/.venv/lib/python3.11/site-packages/pyarrow/error.pxi:92\u001b[39m, in \u001b[36mpyarrow.lib.check_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mOSError\u001b[39m: Could not open Parquet input source '<Buffer>': Couldn't deserialize thrift: invalid TType\n"
     ]
    }
   ],
   "source": [
    "def load_run(run_dir: Path, label: str) -> Dict[str, Any]:\n",
    "    out: Dict[str, Any] = {\"label\": label, \"run_dir\": run_dir}\n",
    "\n",
    "    # required\n",
    "    p_cfg = resolve_artifact(run_dir, \"run_config\", required=True)\n",
    "    p_cs  = resolve_artifact(run_dir, \"comm_stats\", required=False)       # đôi khi pipeline không lưu\n",
    "    p_mg  = resolve_artifact(run_dir, \"metrics_global\", required=False)   # đôi khi pipeline gộp vào cs\n",
    "    p_cm  = resolve_artifact(run_dir, \"comm_metrics_df\", required=True)\n",
    "\n",
    "    out[\"run_config_path\"] = p_cfg\n",
    "    out[\"comm_stats_path\"] = p_cs\n",
    "    out[\"metrics_global_path\"] = p_mg\n",
    "    out[\"comm_metrics_df_path\"] = p_cm\n",
    "\n",
    "    out[\"run_config\"] = read_json(p_cfg) if p_cfg else None\n",
    "    out[\"comm_stats\"] = read_json(p_cs) if p_cs else None\n",
    "    out[\"metrics_global\"] = read_json(p_mg) if p_mg else None\n",
    "    out[\"comm_metrics_df\"] = read_df(p_cm) if p_cm else None\n",
    "\n",
    "    # optional\n",
    "    p_comm = resolve_artifact(run_dir, \"comm_df\", required=False)\n",
    "    p_feat = resolve_artifact(run_dir, \"feat_df\", required=False)\n",
    "    p_chk  = resolve_artifact(run_dir, \"checkins\", required=False)\n",
    "    p_poi  = resolve_artifact(run_dir, \"pois\", required=False)\n",
    "\n",
    "    out[\"comm_df_path\"] = p_comm\n",
    "    out[\"feat_df_path\"] = p_feat\n",
    "    out[\"checkins_path\"] = p_chk\n",
    "    out[\"pois_path\"] = p_poi\n",
    "\n",
    "    out[\"comm_df\"] = read_df(p_comm) if p_comm else None\n",
    "    out[\"feat_df\"] = read_df(p_feat) if p_feat else None\n",
    "    out[\"checkins\"] = read_df(p_chk) if p_chk else None\n",
    "    out[\"pois\"] = read_df(p_poi) if p_poi else None\n",
    "\n",
    "    return out\n",
    "\n",
    "bk = load_run(RUN_BRIGHTKITE, \"brightkite\")\n",
    "lb = load_run(RUN_LBSN2VEC, \"lbsn2vec\")\n",
    "\n",
    "def show_loaded(bundle: Dict[str, Any]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for k in [\"run_config_path\",\"comm_stats_path\",\"metrics_global_path\",\"comm_metrics_df_path\",\"comm_df_path\",\"feat_df_path\",\"checkins_path\",\"pois_path\"]:\n",
    "        p = bundle.get(k)\n",
    "        rows.append({\"artifact\": k, \"path\": str(p) if p else None})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "print(\"=== Loaded artifacts: Brightkite ===\")\n",
    "display(show_loaded(bk))\n",
    "print(\"=== Loaded artifacts: LBSN2Vec++ ===\")\n",
    "display(show_loaded(lb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3e4680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deep_get(d: Dict[str, Any], path: str) -> Any:\n",
    "    cur = d\n",
    "    for part in path.split(\".\"):\n",
    "        if not isinstance(cur, dict) or part not in cur:\n",
    "            return None\n",
    "        cur = cur[part]\n",
    "    return cur\n",
    "\n",
    "CONFIG_KEYS = {\n",
    "    \"dataset\": [\"dataset\", \"cfg.dataset\"],\n",
    "    \"sample_frac\": [\"sample_frac\", \"run.sample_frac\", \"cfg.run.sample_frac\"],\n",
    "    \"train_edge_frac\": [\"train_edge_frac\", \"run.train_edge_frac\", \"cfg.run.train_edge_frac\"],\n",
    "    \"epochs\": [\"epochs\", \"train.epochs\", \"cfg.train.epochs\"],\n",
    "    \"batch_size\": [\"batch_size\", \"train.batch_size\", \"cfg.train.batch_size\"],\n",
    "    \"neg\": [\"neg\", \"train.neg\", \"cfg.train.neg\"],\n",
    "    \"neighbor_sampling\": [\"neighbor_sampling\", \"train.neighbor_sampling\", \"cfg.train.neighbor_sampling\"],\n",
    "    \"knn_k\": [\"knn_k\", \"knn.k\", \"cfg.community.knn_k\", \"cfg.knn.k\"],\n",
    "    \"mutual_knn\": [\"mutual\", \"knn.mutual\", \"cfg.community.mutual_knn\", \"cfg.knn.mutual\"],\n",
    "    \"leiden_resolution\": [\"resolution\", \"leiden.resolution\", \"cfg.community.resolution\", \"cfg.leiden.resolution\"],\n",
    "    \"seed\": [\"seed\", \"run.seed\", \"cfg.run.seed\"],\n",
    "}\n",
    "\n",
    "def extract_cfg(cfg: Optional[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    if cfg is None:\n",
    "        return {}\n",
    "    out = {}\n",
    "    for k, paths in CONFIG_KEYS.items():\n",
    "        v = None\n",
    "        for p in paths:\n",
    "            vv = deep_get(cfg, p)\n",
    "            if vv is not None:\n",
    "                v = vv\n",
    "                break\n",
    "        out[k] = v\n",
    "    return out\n",
    "\n",
    "cfg_compare = pd.DataFrame(\n",
    "    [extract_cfg(bk[\"run_config\"]), extract_cfg(lb[\"run_config\"])],\n",
    "    index=[\"brightkite\", \"lbsn2vec\"]\n",
    ")\n",
    "display(cfg_compare)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee3c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_col(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    for c in candidates:\n",
    "        if c.lower() in cols_lower:\n",
    "            return cols_lower[c.lower()]\n",
    "    # contains heuristic\n",
    "    for c in df.columns:\n",
    "        lc = c.lower()\n",
    "        for cand in candidates:\n",
    "            if cand.lower() in lc:\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def stability_metrics(comm_metrics_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    size_col = guess_col(comm_metrics_df, [\"size\", \"community_size\", \"n_users\", \"count\", \"n\"])\n",
    "    if size_col is None:\n",
    "        raise ValueError(f\"Không tìm thấy cột size trong comm_metrics_df. Columns={list(comm_metrics_df.columns)[:40]}\")\n",
    "    sizes = comm_metrics_df[size_col].dropna().astype(float).values\n",
    "    n_comm = len(sizes)\n",
    "    n_users = float(np.sum(sizes))\n",
    "    largest = float(np.max(sizes)) if n_comm else np.nan\n",
    "    median = float(np.median(sizes)) if n_comm else np.nan\n",
    "    return {\n",
    "        \"n_communities\": int(n_comm),\n",
    "        \"n_users_est\": int(n_users),\n",
    "        \"largest_comm\": largest,\n",
    "        \"median_comm\": median,\n",
    "        \"largest_ratio\": (largest / n_users) if n_users > 0 else np.nan,\n",
    "        \"largest_over_median\": (largest / median) if median > 0 else np.nan,\n",
    "        \"pct_comm_lt_5\": float(np.mean(sizes < 5) * 100.0),\n",
    "        \"pct_comm_lt_10\": float(np.mean(sizes < 10) * 100.0),\n",
    "        \"size_col\": size_col,\n",
    "    }\n",
    "\n",
    "stab_bk = stability_metrics(bk[\"comm_metrics_df\"])\n",
    "stab_lb = stability_metrics(lb[\"comm_metrics_df\"])\n",
    "\n",
    "stab_tbl = pd.DataFrame([stab_bk, stab_lb], index=[\"brightkite\", \"lbsn2vec\"])\n",
    "display(stab_tbl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00507754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sizes(label: str, comm_metrics_df: pd.DataFrame):\n",
    "    size_col = guess_col(comm_metrics_df, [\"size\", \"community_size\", \"n_users\", \"count\", \"n\"])\n",
    "    sizes = comm_metrics_df[size_col].dropna().astype(int).values\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(sizes, bins=60)\n",
    "    plt.title(f\"{label} — community size histogram\")\n",
    "    plt.xlabel(\"community size\")\n",
    "    plt.ylabel(\"count communities\")\n",
    "    plt.show()\n",
    "\n",
    "plot_sizes(\"brightkite\", bk[\"comm_metrics_df\"])\n",
    "plot_sizes(\"lbsn2vec\", lb[\"comm_metrics_df\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f737f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(d: Optional[Dict[str, Any]], prefix: str = \"\") -> Dict[str, Any]:\n",
    "    if d is None:\n",
    "        return {}\n",
    "    out = {}\n",
    "    for k, v in d.items():\n",
    "        kk = f\"{prefix}{k}\"\n",
    "        if isinstance(v, dict):\n",
    "            out.update(flatten(v, prefix=f\"{kk}.\"))\n",
    "        else:\n",
    "            out[kk] = v\n",
    "    return out\n",
    "\n",
    "def pick_first_numeric(flat: Dict[str, Any], keywords: List[str]) -> Tuple[Optional[float], Optional[str]]:\n",
    "    for k, v in flat.items():\n",
    "        lk = k.lower()\n",
    "        if all(kw in lk for kw in keywords):\n",
    "            try:\n",
    "                return float(v), k\n",
    "            except Exception:\n",
    "                continue\n",
    "    return None, None\n",
    "\n",
    "def spatial_summary(bundle: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    df = bundle[\"comm_metrics_df\"]\n",
    "    # find spatial median column\n",
    "    spatial_med_col = guess_col(df, [\"spatial_median_km\", \"spatial_median\", \"median_km\", \"spatial_median_distance_km\"])\n",
    "    if spatial_med_col is None:\n",
    "        # heuristic: any col containing 'spatial' and 'median'\n",
    "        for c in df.columns:\n",
    "            lc = c.lower()\n",
    "            if \"spatial\" in lc and \"median\" in lc:\n",
    "                spatial_med_col = c\n",
    "                break\n",
    "\n",
    "    out = {\"spatial_median_col\": spatial_med_col}\n",
    "    if spatial_med_col is not None:\n",
    "        vals = df[spatial_med_col].dropna().astype(float).values\n",
    "        out[\"spatial_median_km__median_over_comm\"] = float(np.median(vals)) if len(vals) else None\n",
    "        out[\"spatial_median_km__mean_over_comm\"] = float(np.mean(vals)) if len(vals) else None\n",
    "\n",
    "    flat = {\n",
    "        **flatten(bundle.get(\"metrics_global\"), prefix=\"mg.\"),\n",
    "        **flatten(bundle.get(\"comm_stats\"), prefix=\"cs.\"),\n",
    "    }\n",
    "\n",
    "    z, zkey = pick_first_numeric(flat, [\"z\", \"score\"])\n",
    "    if z is None:\n",
    "        # try more specific\n",
    "        z, zkey = pick_first_numeric(flat, [\"spatial\", \"z\"])\n",
    "    out[\"z_score\"] = z\n",
    "    out[\"z_score_key\"] = zkey\n",
    "\n",
    "    # observed vs random (if available)\n",
    "    obs, obskey = pick_first_numeric(flat, [\"spatial\", \"median\"])\n",
    "    rnd_mean, rnd_mean_key = pick_first_numeric(flat, [\"random\", \"mean\"])\n",
    "    rnd_std, rnd_std_key = pick_first_numeric(flat, [\"random\", \"std\"])\n",
    "    out.update({\n",
    "        \"observed_key\": obskey,\n",
    "        \"observed_val\": obs,\n",
    "        \"random_mean_key\": rnd_mean_key,\n",
    "        \"random_mean_val\": rnd_mean,\n",
    "        \"random_std_key\": rnd_std_key,\n",
    "        \"random_std_val\": rnd_std,\n",
    "    })\n",
    "    return out\n",
    "\n",
    "sp_bk = spatial_summary(bk)\n",
    "sp_lb = spatial_summary(lb)\n",
    "\n",
    "sp_tbl = pd.DataFrame([sp_bk, sp_lb], index=[\"brightkite\", \"lbsn2vec\"])\n",
    "display(sp_tbl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a685286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spatial(label: str, comm_metrics_df: pd.DataFrame):\n",
    "    col = guess_col(comm_metrics_df, [\"spatial_median_km\", \"spatial_median\", \"median_km\", \"spatial_median_distance_km\"])\n",
    "    if col is None:\n",
    "        for c in comm_metrics_df.columns:\n",
    "            lc = c.lower()\n",
    "            if \"spatial\" in lc and \"median\" in lc:\n",
    "                col = c\n",
    "                break\n",
    "    if col is None:\n",
    "        print(f\"[{label}] Không thấy spatial median column trong comm_metrics_df.\")\n",
    "        print(\"Columns:\", list(comm_metrics_df.columns)[:50])\n",
    "        return\n",
    "\n",
    "    vals = comm_metrics_df[col].dropna().astype(float).values\n",
    "    plt.figure()\n",
    "    plt.hist(vals, bins=60)\n",
    "    plt.title(f\"{label} — intra-community spatial median (km)\")\n",
    "    plt.xlabel(\"km\")\n",
    "    plt.ylabel(\"count communities\")\n",
    "    plt.show()\n",
    "\n",
    "plot_spatial(\"brightkite\", bk[\"comm_metrics_df\"])\n",
    "plot_spatial(\"lbsn2vec\", lb[\"comm_metrics_df\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bb1c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def structural_summary(bundle: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    flat = {\n",
    "        **flatten(bundle.get(\"metrics_global\"), prefix=\"mg.\"),\n",
    "        **flatten(bundle.get(\"comm_stats\"), prefix=\"cs.\"),\n",
    "    }\n",
    "    modularity, mod_key = pick_first_numeric(flat, [\"modular\"])\n",
    "    conductance, cond_key = pick_first_numeric(flat, [\"conduct\"])\n",
    "    density, dens_key = pick_first_numeric(flat, [\"density\"])\n",
    "    return {\n",
    "        \"modularity\": modularity, \"modularity_key\": mod_key,\n",
    "        \"conductance\": conductance, \"conductance_key\": cond_key,\n",
    "        \"density\": density, \"density_key\": dens_key,\n",
    "    }\n",
    "\n",
    "st_bk = structural_summary(bk)\n",
    "st_lb = structural_summary(lb)\n",
    "\n",
    "struct_tbl = pd.DataFrame([st_bk, st_lb], index=[\"brightkite\", \"lbsn2vec\"])\n",
    "display(struct_tbl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e74b8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_specialization(lb_bundle: Dict[str, Any], topk: int = 5) -> Optional[pd.DataFrame]:\n",
    "    checkins = lb_bundle.get(\"checkins\")\n",
    "    pois = lb_bundle.get(\"pois\")\n",
    "    comm_df = lb_bundle.get(\"comm_df\")\n",
    "    if checkins is None or pois is None or comm_df is None:\n",
    "        print(\"[lbsn2vec] Thiếu checkins/pois/comm_df nên không làm semantic specialization được.\")\n",
    "        return None\n",
    "\n",
    "    u = guess_col(checkins, [\"user_id\", \"user\"])\n",
    "    v = guess_col(checkins, [\"venue_id\", \"venue\", \"poi_id\", \"place_id\"])\n",
    "    cu = guess_col(comm_df, [\"user_id\", \"user\"])\n",
    "    cc = guess_col(comm_df, [\"community_id\", \"community\", \"cluster\", \"label\"])\n",
    "    pv = guess_col(pois, [\"venue_id\", \"venue\", \"poi_id\", \"place_id\"])\n",
    "    cat = guess_col(pois, [\"category\", \"cat\", \"type\", \"venue_category\"])\n",
    "\n",
    "    if None in (u, v, cu, cc, pv, cat):\n",
    "        print(\"[lbsn2vec] Không tìm đủ cột cần thiết.\")\n",
    "        print(\"checkins cols:\", list(checkins.columns)[:30])\n",
    "        print(\"comm_df cols:\", list(comm_df.columns)[:30])\n",
    "        print(\"pois cols:\", list(pois.columns)[:30])\n",
    "        return None\n",
    "\n",
    "    tmp = checkins[[u, v]].merge(comm_df[[cu, cc]], left_on=u, right_on=cu, how=\"inner\")\n",
    "    tmp = tmp.merge(pois[[pv, cat]], left_on=v, right_on=pv, how=\"left\")\n",
    "    tmp = tmp.dropna(subset=[cat])\n",
    "\n",
    "    grp = tmp.groupby([cc, cat]).size().rename(\"n\").reset_index()\n",
    "    total = grp.groupby(cc)[\"n\"].sum().rename(\"N\").reset_index()\n",
    "    grp = grp.merge(total, on=cc, how=\"left\")\n",
    "    grp[\"p\"] = grp[\"n\"] / grp[\"N\"]\n",
    "\n",
    "    # entropy per community\n",
    "    ent = grp.groupby(cc).apply(lambda g: float(-(g[\"p\"] * np.log(g[\"p\"] + 1e-12)).sum())).rename(\"category_entropy\").reset_index()\n",
    "\n",
    "    # top-k categories per community\n",
    "    top = grp.sort_values([cc, \"p\"], ascending=[True, False]).groupby(cc).head(topk)\n",
    "    top = top.rename(columns={cc: \"community_id\", cat: \"category\"})\n",
    "\n",
    "    ent = ent.rename(columns={cc: \"community_id\"})\n",
    "    out = top.merge(ent, on=\"community_id\", how=\"left\")\n",
    "    return out.sort_values([\"category_entropy\", \"community_id\", \"p\"], ascending=[True, True, False])\n",
    "\n",
    "sem = semantic_specialization(lb, topk=5)\n",
    "if sem is not None:\n",
    "    display(sem.head(40))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed2d34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame([\n",
    "    {\n",
    "        \"dataset\": \"brightkite\",\n",
    "        **{f\"cfg.{k}\": v for k, v in extract_cfg(bk[\"run_config\"]).items()},\n",
    "        **{f\"stab.{k}\": v for k, v in stab_bk.items() if k != \"size_col\"},\n",
    "        **{f\"spatial.{k}\": v for k, v in sp_bk.items()},\n",
    "        **{f\"struct.{k}\": v for k, v in st_bk.items()},\n",
    "    },\n",
    "    {\n",
    "        \"dataset\": \"lbsn2vec\",\n",
    "        **{f\"cfg.{k}\": v for k, v in extract_cfg(lb[\"run_config\"]).items()},\n",
    "        **{f\"stab.{k}\": v for k, v in stab_lb.items() if k != \"size_col\"},\n",
    "        **{f\"spatial.{k}\": v for k, v in sp_lb.items()},\n",
    "        **{f\"struct.{k}\": v for k, v in st_lb.items()},\n",
    "    },\n",
    "]).set_index(\"dataset\")\n",
    "\n",
    "display(final)\n",
    "\n",
    "print(\"\\nCách đọc nhanh (Step 8):\")\n",
    "print(\"- Stability: largest_ratio (≈ largest_comm / total_users) càng thấp càng ổn; pct_comm_lt_10 quá cao => nhiều cụm nhỏ lẻ.\")\n",
    "print(\"- Structural: modularity cao + conductance thấp => 'đậm social' hơn.\")\n",
    "print(\"- Spatial: spatial_median_km thấp + z_score cao => 'đậm địa lý' hơn (tốt hơn random baseline).\")\n",
    "print(\"- Semantic (LBSN2Vec++): category_entropy thấp => community chuyên môn hoá theo loại địa điểm mạnh hơn.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
