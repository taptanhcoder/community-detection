{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d31b5dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b732be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /mnt/d/community-detection\n",
      "CLEARED_DIR: /mnt/d/community-detection/data/processed/data_cleared\n",
      "RUNS_DIR: /mnt/d/community-detection/data/processed/_runs\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def find_repo_root(start: Path | None = None) -> Path:\n",
    "    start = start or Path.cwd()\n",
    "    cur = start.resolve()\n",
    "    for p in [cur] + list(cur.parents):\n",
    "        if (p / \"pyproject.toml\").exists() or (p / \"data\").exists():\n",
    "            return p\n",
    "    return cur\n",
    "\n",
    "ROOT = find_repo_root()\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "RUNS_DIR = PROCESSED_DIR / \"_runs\"\n",
    "CLEARED_DIR = PROCESSED_DIR / \"data_cleared\"\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"CLEARED_DIR:\", CLEARED_DIR)\n",
    "print(\"RUNS_DIR:\", RUNS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "722e3107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /mnt/d/community-detection\n",
      "CLEARED_DIR: /mnt/d/community-detection/data/processed/data_cleared\n",
      "RUNS_DIR: /mnt/d/community-detection/data/processed/_runs\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def find_repo_root(start: Path | None = None) -> Path:\n",
    "    start = start or Path.cwd()\n",
    "    cur = start.resolve()\n",
    "    for p in [cur] + list(cur.parents):\n",
    "        if (p / \"pyproject.toml\").exists() or (p / \"data\").exists():\n",
    "            return p\n",
    "    return cur\n",
    "\n",
    "ROOT = find_repo_root()\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "RUNS_DIR = PROCESSED_DIR / \"_runs\"\n",
    "CLEARED_DIR = PROCESSED_DIR / \"data_cleared\"\n",
    "\n",
    "print(\"ROOT:\", ROOT)\n",
    "print(\"CLEARED_DIR:\", CLEARED_DIR)\n",
    "print(\"RUNS_DIR:\", RUNS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5babde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': 'brightkite', 'cleared_dir': PosixPath('/mnt/d/community-detection/data/processed/data_cleared/brightkite'), 'run_id': '20251214_192049', 'run_dir': PosixPath('/mnt/d/community-detection/data/processed/_runs/20251214_192049/brightkite'), 'run_config': PosixPath('/mnt/d/community-detection/data/processed/_runs/20251214_192049/run_config.json')}\n",
      "{'dataset': 'lbsn2vec', 'cleared_dir': PosixPath('/mnt/d/community-detection/data/processed/data_cleared/lbsn2vec'), 'run_id': '20251214_183903', 'run_dir': PosixPath('/mnt/d/community-detection/data/processed/_runs/20251214_183903/lbsn2vec'), 'run_config': PosixPath('/mnt/d/community-detection/data/processed/_runs/20251214_183903/run_config.json')}\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "COMPARE_PAIR = (\"brightkite\", \"lbsn2vec\")\n",
    "\n",
    "RUN_ID_BY_DATASET = {\n",
    "    \"brightkite\": \"20251214_192049\",\n",
    "    \"lbsn2vec\": \"20251214_183903\",\n",
    "}\n",
    "\n",
    "def dataset_paths(dataset: str) -> dict:\n",
    "    dataset = dataset.lower()\n",
    "    cleared = CLEARED_DIR / dataset\n",
    "    run_id = RUN_ID_BY_DATASET.get(dataset)\n",
    "    run_dir = (RUNS_DIR / run_id / dataset) if run_id else None\n",
    "    run_cfg = (RUNS_DIR / run_id / \"run_config.json\") if run_id else None\n",
    "    return {\n",
    "        \"dataset\": dataset,\n",
    "        \"cleared_dir\": cleared,\n",
    "        \"run_id\": run_id,\n",
    "        \"run_dir\": run_dir,\n",
    "        \"run_config\": run_cfg,\n",
    "    }\n",
    "\n",
    "for ds in COMPARE_PAIR:\n",
    "    print(dataset_paths(ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab374ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def read_json_safe(p: Path) -> dict | None:\n",
    "    try:\n",
    "        with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] JSON read failed: {p} -> {type(e).__name__}: {e}\")\n",
    "        return None\n",
    "\n",
    "def read_parquet_safe(p: Path, columns=None) -> pd.DataFrame | None:\n",
    "    try:\n",
    "        return pd.read_parquet(p, columns=columns)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Parquet read failed: {p} -> {type(e).__name__}: {e}\")\n",
    "        return None\n",
    "\n",
    "def read_npy_safe(p: Path):\n",
    "    try:\n",
    "        return np.load(p, allow_pickle=False)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] NPY read failed: {p} -> {type(e).__name__}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "23d053d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def normalize_user_id(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(str).str.strip()\n",
    "    s = s.str.replace(r\"\\.0$\", \"\", regex=True)  # \"123.0\" -> \"123\"\n",
    "    s = s.replace({\"nan\": np.nan, \"None\": np.nan})\n",
    "    return s\n",
    "\n",
    "def normalize_edges(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"u\" not in df.columns or \"v\" not in df.columns:\n",
    "        df = df.rename(columns={df.columns[0]: \"u\", df.columns[1]: \"v\"})\n",
    "    df[\"u\"] = normalize_user_id(df[\"u\"])\n",
    "    df[\"v\"] = normalize_user_id(df[\"v\"])\n",
    "    df = df.dropna(subset=[\"u\", \"v\"])\n",
    "    return df\n",
    "\n",
    "def normalize_users(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    if \"user_id\" not in df.columns:\n",
    "        df = df.rename(columns={df.columns[0]: \"user_id\"})\n",
    "    df[\"user_id\"] = normalize_user_id(df[\"user_id\"])\n",
    "    df = df.dropna(subset=[\"user_id\"])\n",
    "    return df\n",
    "\n",
    "def normalize_checkins(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"user_id\"] = normalize_user_id(df[\"user_id\"])\n",
    "    df = df.dropna(subset=[\"user_id\"])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cddf454f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def load_comm_df(run_dir: Path) -> pd.DataFrame:\n",
    "    p_rep = run_dir / \"comm_df.repaired.parquet\"\n",
    "    p_plain = run_dir / \"comm_df.parquet\"\n",
    "    if p_rep.exists():\n",
    "        df = pd.read_parquet(p_rep)\n",
    "    elif p_plain.exists():\n",
    "        df = pd.read_parquet(p_plain)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Missing comm_df(.repaired) in {run_dir}\")\n",
    "\n",
    "    if \"user_id\" not in df.columns:\n",
    "        raise ValueError(\"comm_df missing user_id\")\n",
    "\n",
    "    if \"community_id\" not in df.columns:\n",
    "        for alt in [\"community\", \"comm_id\", \"cluster_id\"]:\n",
    "            if alt in df.columns:\n",
    "                df = df.rename(columns={alt: \"community_id\"})\n",
    "                break\n",
    "    if \"community_id\" not in df.columns:\n",
    "        raise ValueError(\"comm_df missing community_id (or alt)\")\n",
    "\n",
    "    df = df[[\"user_id\", \"community_id\"]].copy()\n",
    "    df[\"user_id\"] = normalize_user_id(df[\"user_id\"])\n",
    "    df = df.dropna(subset=[\"user_id\"])\n",
    "    df[\"community_id\"] = df[\"community_id\"].astype(int)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4967cc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded bundles: brightkite lbsn2vec\n",
      "A: (15092, 1) (116506, 2) (3656191, 5) (8556, 2)\n",
      "B: (47389, 1) (279816, 2) (10328914, 5) (11829, 2)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "REQUIRED_CLEARED = [\"edges_final.parquet\", \"users_final.parquet\", \"checkins_final.parquet\"]\n",
    "\n",
    "def load_step8_bundle(ds: str) -> dict:\n",
    "    info = dataset_paths(ds)\n",
    "    if not info[\"cleared_dir\"].exists():\n",
    "        raise FileNotFoundError(info[\"cleared_dir\"])\n",
    "    if info[\"run_dir\"] is None or not info[\"run_dir\"].exists():\n",
    "        raise FileNotFoundError(info[\"run_dir\"])\n",
    "\n",
    "    for fn in REQUIRED_CLEARED:\n",
    "        p = info[\"cleared_dir\"] / fn\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(p)\n",
    "\n",
    "    edges = read_parquet_safe(info[\"cleared_dir\"] / \"edges_final.parquet\")\n",
    "    users = read_parquet_safe(info[\"cleared_dir\"] / \"users_final.parquet\")\n",
    "    checkins = read_parquet_safe(info[\"cleared_dir\"] / \"checkins_final.parquet\")\n",
    "    if edges is None or users is None or checkins is None:\n",
    "        raise RuntimeError(\"Failed to load cleared parquet(s)\")\n",
    "\n",
    "    edges = normalize_edges(edges)\n",
    "    users = normalize_users(users)\n",
    "    checkins = normalize_checkins(checkins)\n",
    "\n",
    "    comm_df = load_comm_df(info[\"run_dir\"])\n",
    "\n",
    "    metrics_global = read_json_safe(info[\"run_dir\"] / \"metrics_global.json\")\n",
    "    run_config = read_json_safe(info[\"run_config\"]) if info[\"run_config\"] and info[\"run_config\"].exists() else None\n",
    "\n",
    "    return dict(\n",
    "        dataset=ds,\n",
    "        edges_final=edges,\n",
    "        users_final=users,\n",
    "        checkins_final=checkins,\n",
    "        comm_df=comm_df,\n",
    "        metrics_global_cache=metrics_global,\n",
    "        run_config=run_config,\n",
    "        paths=info,\n",
    "    )\n",
    "\n",
    "data_A = load_step8_bundle(COMPARE_PAIR[0])\n",
    "data_B = load_step8_bundle(COMPARE_PAIR[1])\n",
    "\n",
    "print(\"Loaded bundles:\", data_A[\"dataset\"], data_B[\"dataset\"])\n",
    "print(\"A:\", data_A[\"users_final\"].shape, data_A[\"edges_final\"].shape, data_A[\"checkins_final\"].shape, data_A[\"comm_df\"].shape)\n",
    "print(\"B:\", data_B[\"users_final\"].shape, data_B[\"edges_final\"].shape, data_B[\"checkins_final\"].shape, data_B[\"comm_df\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fcf8c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A X: (15092, 43) users: 15092\n",
      "B X: (47389, 43) users: 47389\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def load_X_users_from_cleared(cleared_dir: Path) -> np.ndarray:\n",
    "    p = cleared_dir / \"X_users.npy\"\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(p)\n",
    "    X = np.load(p)\n",
    "    if X.ndim != 2:\n",
    "        raise ValueError(f\"X_users must be 2D, got {X.shape}\")\n",
    "    return X\n",
    "\n",
    "def attach_features(bundle: dict) -> dict:\n",
    "    X = load_X_users_from_cleared(bundle[\"paths\"][\"cleared_dir\"])\n",
    "    user_ids = bundle[\"users_final\"][\"user_id\"].astype(str).to_numpy()\n",
    "    if len(user_ids) != X.shape[0]:\n",
    "        raise RuntimeError(f\"Mismatch users_final vs X_users: {len(user_ids)} vs {X.shape[0]}\")\n",
    "    out = bundle.copy()\n",
    "    out[\"user_ids_full\"] = user_ids\n",
    "    out[\"X_users\"] = X.astype(np.float32, copy=False)\n",
    "    return out\n",
    "\n",
    "data_A = attach_features(data_A)\n",
    "data_B = attach_features(data_B)\n",
    "print(\"A X:\", data_A[\"X_users\"].shape, \"users:\", len(data_A[\"user_ids_full\"]))\n",
    "print(\"B X:\", data_B[\"X_users\"].shape, \"users:\", len(data_B[\"user_ids_full\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5910a2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage A: {'users_total': 15092, 'users_labeled': 5346, 'label_coverage': 0.35422740524781343, 'edges_total': 116506, 'edges_labeled': 49536, 'edge_labeled_ratio': 0.42517981906511254, 'checkins_total': 3656191, 'checkins_labeled': 2068469, 'checkins_labeled_ratio': 0.565744240385691}\n",
      "Coverage B: {'users_total': 47389, 'users_labeled': 771, 'label_coverage': 0.016269598430015405, 'edges_total': 279816, 'edges_labeled': 1227, 'edge_labeled_ratio': 0.004385024444635046, 'checkins_total': 10328914, 'checkins_labeled': 160359, 'checkins_labeled_ratio': 0.015525252703236759}\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def make_labeled_views(bundle: dict) -> dict:\n",
    "    users_all = set(bundle[\"users_final\"][\"user_id\"].unique())\n",
    "\n",
    "    comm_df = bundle[\"comm_df\"].copy()\n",
    "    comm_df = comm_df[comm_df[\"user_id\"].isin(users_all)].copy()\n",
    "    labeled_users = set(comm_df[\"user_id\"].unique())\n",
    "\n",
    "    edges = bundle[\"edges_final\"].copy()\n",
    "    edges_labeled = edges[edges[\"u\"].isin(labeled_users) & edges[\"v\"].isin(labeled_users)].copy()\n",
    "\n",
    "    checkins = bundle[\"checkins_final\"].copy()\n",
    "    checkins_labeled = checkins[checkins[\"user_id\"].isin(labeled_users)].copy()\n",
    "\n",
    "    cov = {\n",
    "        \"users_total\": int(len(users_all)),\n",
    "        \"users_labeled\": int(len(labeled_users)),\n",
    "        \"label_coverage\": float(len(labeled_users) / max(1, len(users_all))),\n",
    "\n",
    "        \"edges_total\": int(len(edges)),\n",
    "        \"edges_labeled\": int(len(edges_labeled)),\n",
    "        \"edge_labeled_ratio\": float(len(edges_labeled) / max(1, len(edges))),\n",
    "\n",
    "        \"checkins_total\": int(len(checkins)),\n",
    "        \"checkins_labeled\": int(len(checkins_labeled)),\n",
    "        \"checkins_labeled_ratio\": float(len(checkins_labeled) / max(1, len(checkins))),\n",
    "    }\n",
    "\n",
    "    out = bundle.copy()\n",
    "    out[\"comm_df_labeled\"] = comm_df\n",
    "    out[\"edges_labeled\"] = edges_labeled\n",
    "    out[\"checkins_labeled\"] = checkins_labeled\n",
    "    out[\"coverage\"] = cov\n",
    "    return out\n",
    "\n",
    "data_A = make_labeled_views(data_A)\n",
    "data_B = make_labeled_views(data_B)\n",
    "\n",
    "print(\"Coverage A:\", data_A[\"coverage\"])\n",
    "print(\"Coverage B:\", data_B[\"coverage\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c349db38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aug labels A: (15092, 3) coverage = 1.0\n",
      "Aug labels B: (47389, 3) coverage = 1.0\n",
      "label_source\n",
      "propagated    9746\n",
      "original      5346\n",
      "Name: count, dtype: int64\n",
      "label_source\n",
      "propagated    46618\n",
      "original        771\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def l2_normalize_rows(X: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    nrm = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    return X / (nrm + eps)\n",
    "\n",
    "def augment_labels_knn_majority(bundle: dict, k: int = 15, seed: int = 42) -> pd.DataFrame:\n",
    "\n",
    "    user_ids = bundle[\"user_ids_full\"]\n",
    "    X = bundle[\"X_users\"]\n",
    "    comm = bundle[\"comm_df_labeled\"][[\"user_id\", \"community_id\"]].copy()\n",
    "\n",
    "    labeled_set = set(comm[\"user_id\"])\n",
    "    user_to_idx = {u: i for i, u in enumerate(user_ids.tolist())}\n",
    "\n",
    "    labeled_idx = np.array([user_to_idx[u] for u in comm[\"user_id\"].tolist() if u in user_to_idx], dtype=int)\n",
    "    labeled_comm = comm.set_index(\"user_id\")[\"community_id\"].to_dict()\n",
    "\n",
    "    # normalize for cosine\n",
    "    Xn = l2_normalize_rows(X.astype(np.float32, copy=False))\n",
    "\n",
    "    # split indices\n",
    "    all_idx = np.arange(len(user_ids))\n",
    "    unlabeled_mask = np.array([u not in labeled_set for u in user_ids], dtype=bool)\n",
    "    unlabeled_idx = all_idx[unlabeled_mask]\n",
    "\n",
    "    # if no unlabeled or no labeled -> return original\n",
    "    if len(unlabeled_idx) == 0 or len(labeled_idx) == 0:\n",
    "        out = comm.copy()\n",
    "        out[\"label_source\"] = \"original\"\n",
    "        return out\n",
    "\n",
    "    # nearest neighbors (prefer sklearn)\n",
    "    try:\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "        nn = NearestNeighbors(n_neighbors=min(k, len(labeled_idx)), metric=\"cosine\")\n",
    "        nn.fit(Xn[labeled_idx])\n",
    "        dist, nbr = nn.kneighbors(Xn[unlabeled_idx], return_distance=True)\n",
    "        # cosine sim = 1 - dist\n",
    "        sim = 1.0 - dist\n",
    "    except Exception:\n",
    "        # fallback: brute force (OK for smaller, but can be slower)\n",
    "        # compute in blocks to avoid RAM blow\n",
    "        rng = np.random.default_rng(seed)\n",
    "        k2 = min(k, len(labeled_idx))\n",
    "        sim = np.empty((len(unlabeled_idx), k2), dtype=np.float32)\n",
    "        nbr = np.empty((len(unlabeled_idx), k2), dtype=np.int32)\n",
    "\n",
    "        L = Xn[labeled_idx]\n",
    "        for i, ui in enumerate(unlabeled_idx):\n",
    "            s = L @ Xn[ui]\n",
    "            top = np.argpartition(-s, k2-1)[:k2]\n",
    "            top = top[np.argsort(-s[top])]\n",
    "            sim[i] = s[top]\n",
    "            nbr[i] = top\n",
    "\n",
    "    labeled_users_list = user_ids[labeled_idx].tolist()\n",
    "\n",
    "    preds = []\n",
    "    for i, ui in enumerate(unlabeled_idx):\n",
    "        neigh_pos = nbr[i].tolist()\n",
    "        neigh_users = [labeled_users_list[j] for j in neigh_pos]\n",
    "        neigh_labels = [labeled_comm.get(u, None) for u in neigh_users]\n",
    "\n",
    "        # weighted vote by similarity\n",
    "        weights = sim[i].astype(float)\n",
    "        vote = {}\n",
    "        for lab, w in zip(neigh_labels, weights):\n",
    "            if lab is None:\n",
    "                continue\n",
    "            vote[lab] = vote.get(lab, 0.0) + float(w)\n",
    "\n",
    "        if len(vote) == 0:\n",
    "            # fallback: leave unlabeled\n",
    "            continue\n",
    "\n",
    "        pred_lab = max(vote.items(), key=lambda x: x[1])[0]\n",
    "        preds.append((user_ids[ui], int(pred_lab)))\n",
    "\n",
    "    aug = pd.DataFrame(preds, columns=[\"user_id\", \"community_id\"])\n",
    "    aug[\"label_source\"] = \"propagated\"\n",
    "\n",
    "    base = comm.copy()\n",
    "    base[\"label_source\"] = \"original\"\n",
    "\n",
    "    comm_all = pd.concat([base, aug], ignore_index=True)\n",
    "    comm_all = comm_all.drop_duplicates(subset=[\"user_id\"], keep=\"first\")\n",
    "    comm_all[\"community_id\"] = comm_all[\"community_id\"].astype(int)\n",
    "    return comm_all\n",
    "\n",
    "commA_all = augment_labels_knn_majority(data_A, k=15)\n",
    "commB_all = augment_labels_knn_majority(data_B, k=15)\n",
    "\n",
    "print(\"Aug labels A:\", commA_all.shape, \"coverage =\", commA_all[\"user_id\"].nunique() / data_A[\"users_final\"][\"user_id\"].nunique())\n",
    "print(\"Aug labels B:\", commB_all.shape, \"coverage =\", commB_all[\"user_id\"].nunique() / data_B[\"users_final\"][\"user_id\"].nunique())\n",
    "print(commA_all[\"label_source\"].value_counts())\n",
    "print(commB_all[\"label_source\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c04447a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN A: (68426, 3) kNN B: (217074, 3)\n",
      "kNN A cols: ['u', 'v', 'w']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def l2_normalize_rows(X: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    n = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    return X / (n + eps)\n",
    "\n",
    "def build_knn_graph_from_X(bundle: dict, k: int = 20, mutual: bool = True) -> pd.DataFrame:\n",
    "    user_ids = bundle[\"user_ids_full\"]\n",
    "    X = l2_normalize_rows(bundle[\"X_users\"])\n",
    "\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    nn = NearestNeighbors(n_neighbors=min(k + 1, len(user_ids)), metric=\"cosine\")\n",
    "    nn.fit(X)\n",
    "    dist, nbr = nn.kneighbors(X, return_distance=True)\n",
    "    sim = 1.0 - dist\n",
    "\n",
    "    rows = []\n",
    "    for i in range(len(user_ids)):\n",
    "        ui = user_ids[i]\n",
    "        for jpos in range(1, nbr.shape[1]):  # skip self\n",
    "            j = int(nbr[i, jpos])\n",
    "            vj = user_ids[j]\n",
    "            w = float(sim[i, jpos])\n",
    "            rows.append((ui, vj, w))\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=[\"u\", \"v\", \"w\"])\n",
    "\n",
    "    if mutual:\n",
    "        key = (df[\"u\"].astype(str) + \"||\" + df[\"v\"].astype(str))\n",
    "        inv = (df[\"v\"].astype(str) + \"||\" + df[\"u\"].astype(str))\n",
    "        df = df[key.isin(set(inv))].copy()\n",
    "\n",
    "    # undirected canonical + keep max weight\n",
    "    umin = df[[\"u\", \"v\"]].astype(str).min(axis=1)\n",
    "    vmax = df[[\"u\", \"v\"]].astype(str).max(axis=1)\n",
    "    df[\"u\"] = umin\n",
    "    df[\"v\"] = vmax\n",
    "    df = df.sort_values(\"w\", ascending=False).drop_duplicates(subset=[\"u\", \"v\"], keep=\"first\")\n",
    "    return df[[\"u\", \"v\", \"w\"]].reset_index(drop=True)\n",
    "\n",
    "knnA = build_knn_graph_from_X(data_A, k=20, mutual=True)\n",
    "knnB = build_knn_graph_from_X(data_B, k=20, mutual=True)\n",
    "\n",
    "print(\"kNN A:\", knnA.shape, \"kNN B:\", knnB.shape)\n",
    "print(\"kNN A cols:\", knnA.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dc9e592d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aug A: (15092, 2) coverage: 1.0\n",
      "Aug B: (47389, 2) coverage: 1.0\n",
      "A #communities: 262\n",
      "B #communities: 651\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def leiden_on_knn(knn_edges: pd.DataFrame, user_ids_full: np.ndarray, resolution: float = 1.0) -> pd.DataFrame:\n",
    "    try:\n",
    "        import igraph as ig\n",
    "        import leidenalg\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\"Need igraph + leidenalg installed for Leiden-on-kNN.\") from e\n",
    "\n",
    "    nodes = pd.Index(user_ids_full.astype(str))\n",
    "    node2idx = {u: i for i, u in enumerate(nodes.tolist())}\n",
    "\n",
    "    u_idx = knn_edges[\"u\"].astype(str).map(node2idx).to_numpy()\n",
    "    v_idx = knn_edges[\"v\"].astype(str).map(node2idx).to_numpy()\n",
    "    w = knn_edges[\"w\"].astype(float).to_numpy()\n",
    "\n",
    "    # drop any NaN mapping (shouldn't happen if kNN built from same users)\n",
    "    mask = (~pd.isna(u_idx)) & (~pd.isna(v_idx))\n",
    "    u_idx = u_idx[mask].astype(int)\n",
    "    v_idx = v_idx[mask].astype(int)\n",
    "    w = w[mask]\n",
    "\n",
    "    g = ig.Graph(n=len(nodes), edges=list(zip(u_idx, v_idx)), directed=False)\n",
    "    g.es[\"weight\"] = w\n",
    "    g.simplify(multiple=True, loops=True, combine_edges=\"max\")\n",
    "\n",
    "    part = leidenalg.find_partition(\n",
    "        g,\n",
    "        leidenalg.RBConfigurationVertexPartition,\n",
    "        weights=\"weight\",\n",
    "        resolution_parameter=float(resolution),\n",
    "    )\n",
    "    membership = np.array(part.membership, dtype=int)\n",
    "    return pd.DataFrame({\"user_id\": nodes.astype(str), \"community_id\": membership})\n",
    "\n",
    "commA_aug = leiden_on_knn(knnA, data_A[\"user_ids_full\"], resolution=1.0)\n",
    "commB_aug = leiden_on_knn(knnB, data_B[\"user_ids_full\"], resolution=1.0)\n",
    "\n",
    "print(\"Aug A:\", commA_aug.shape, \"coverage:\", commA_aug[\"user_id\"].nunique()/len(data_A[\"user_ids_full\"]))\n",
    "print(\"Aug B:\", commB_aug.shape, \"coverage:\", commB_aug[\"user_id\"].nunique()/len(data_B[\"user_ids_full\"]))\n",
    "print(\"A #communities:\", commA_aug[\"community_id\"].nunique())\n",
    "print(\"B #communities:\", commB_aug[\"community_id\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cba97eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def try_compute_modularity_igraph(edges_df: pd.DataFrame, comm_df: pd.DataFrame) -> float | None:\n",
    "    try:\n",
    "        import igraph as ig\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    nodes = pd.Index(pd.unique(pd.concat([edges_df[\"u\"], edges_df[\"v\"]], ignore_index=True)))\n",
    "    node2idx = {u: i for i, u in enumerate(nodes.tolist())}\n",
    "    edgelist = list(zip(edges_df[\"u\"].map(node2idx), edges_df[\"v\"].map(node2idx)))\n",
    "\n",
    "    g = ig.Graph(n=len(nodes), edges=edgelist, directed=False)\n",
    "    g.simplify(multiple=True, loops=True)\n",
    "\n",
    "    comm_map = comm_df.set_index(\"user_id\")[\"community_id\"].to_dict()\n",
    "    membership = [comm_map.get(u, -1) for u in nodes.tolist()]\n",
    "    valid_idx = [i for i, m in enumerate(membership) if m != -1]\n",
    "\n",
    "    if len(valid_idx) != len(membership):\n",
    "        g = g.induced_subgraph(valid_idx)\n",
    "        membership = [membership[i] for i in valid_idx]\n",
    "\n",
    "    uniq = {c: i for i, c in enumerate(sorted(set(membership)))}\n",
    "    membership = [uniq[c] for c in membership]\n",
    "    return float(g.modularity(membership))\n",
    "\n",
    "def compute_structural_comm_metrics(edges_df: pd.DataFrame, comm_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    u2c = comm_df.set_index(\"user_id\")[\"community_id\"]\n",
    "\n",
    "    df = edges_df.copy()\n",
    "    df[\"cu\"] = df[\"u\"].map(u2c)\n",
    "    df[\"cv\"] = df[\"v\"].map(u2c)\n",
    "    df = df.dropna(subset=[\"cu\", \"cv\"])\n",
    "    df[\"cu\"] = df[\"cu\"].astype(int)\n",
    "    df[\"cv\"] = df[\"cv\"].astype(int)\n",
    "\n",
    "    deg = pd.concat([edges_df[\"u\"], edges_df[\"v\"]], ignore_index=True).value_counts()\n",
    "    deg = deg.rename_axis(\"user_id\").reset_index(name=\"deg\")\n",
    "    deg[\"user_id\"] = deg[\"user_id\"].astype(str)\n",
    "    deg = deg.merge(comm_df[[\"user_id\",\"community_id\"]], on=\"user_id\", how=\"inner\")\n",
    "\n",
    "    vol_by_comm = deg.groupby(\"community_id\")[\"deg\"].sum().rename(\"volume\")\n",
    "    total_vol = float(vol_by_comm.sum())\n",
    "\n",
    "    internal = (\n",
    "        df[df[\"cu\"] == df[\"cv\"]]\n",
    "        .groupby(\"cu\").size()\n",
    "        .rename(\"internal_edges\")\n",
    "        .rename_axis(\"community_id\")\n",
    "    )\n",
    "\n",
    "    cut_df = df[df[\"cu\"] != df[\"cv\"]][[\"cu\", \"cv\"]]\n",
    "    cut_counts = (\n",
    "        pd.concat([cut_df[\"cu\"].value_counts(), cut_df[\"cv\"].value_counts()], axis=0)\n",
    "        .groupby(level=0).sum()\n",
    "        .rename(\"cut_edges\")\n",
    "        .rename_axis(\"community_id\")\n",
    "    )\n",
    "\n",
    "    sizes = comm_df.groupby(\"community_id\")[\"user_id\"].nunique().rename(\"n_nodes\").rename_axis(\"community_id\")\n",
    "\n",
    "    out = pd.concat([sizes, vol_by_comm, internal, cut_counts], axis=1).fillna(0).reset_index()\n",
    "\n",
    "    denom = np.minimum(out[\"volume\"].values, total_vol - out[\"volume\"].values)\n",
    "    denom = np.where(denom <= 0, np.nan, denom)\n",
    "    out[\"conductance\"] = out[\"cut_edges\"].values / denom\n",
    "\n",
    "    n = out[\"n_nodes\"].values.astype(float)\n",
    "    denom_d = n * (n - 1.0) / 2.0\n",
    "    denom_d = np.where(denom_d <= 0, np.nan, denom_d)\n",
    "    out[\"density\"] = out[\"internal_edges\"].values / denom_d\n",
    "\n",
    "    return out\n",
    "\n",
    "def summarize_structural(comm_metrics: pd.DataFrame, modularity: float | None) -> dict:\n",
    "    s = {\"modularity\": modularity}\n",
    "    for col in [\"conductance\", \"density\"]:\n",
    "        vals = comm_metrics[col].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        s[f\"{col}_median\"] = float(vals.median()) if len(vals) else None\n",
    "        s[f\"{col}_iqr\"] = float(vals.quantile(0.75) - vals.quantile(0.25)) if len(vals) else None\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62fbaed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1 = np.radians(lat1); lon1 = np.radians(lon1)\n",
    "    lat2 = np.radians(lat2); lon2 = np.radians(lon2)\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1)*np.cos(lat2)*np.sin(dlon/2)**2\n",
    "    c = 2*np.arcsin(np.sqrt(a))\n",
    "    return R*c\n",
    "\n",
    "def compute_user_centroids(checkins_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return checkins_df.groupby(\"user_id\", as_index=False)[[\"lat\", \"lon\"]].mean()\n",
    "\n",
    "def compute_spatial_comm_metrics(user_centroids: pd.DataFrame, comm_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    tmp = user_centroids.merge(comm_df[[\"user_id\",\"community_id\"]], on=\"user_id\", how=\"inner\")\n",
    "    sizes = tmp.groupby(\"community_id\")[\"user_id\"].nunique().rename(\"n_nodes\")\n",
    "    comm_cent = tmp.groupby(\"community_id\")[[\"lat\", \"lon\"]].mean().rename(columns={\"lat\": \"comm_lat\", \"lon\": \"comm_lon\"})\n",
    "    tmp = tmp.merge(comm_cent, on=\"community_id\", how=\"left\")\n",
    "\n",
    "    dist = haversine_km(tmp[\"lat\"].values, tmp[\"lon\"].values, tmp[\"comm_lat\"].values, tmp[\"comm_lon\"].values)\n",
    "    tmp[\"dist_km\"] = dist\n",
    "\n",
    "    spatial = tmp.groupby(\"community_id\")[\"dist_km\"].median().rename(\"spatial_median_km\")\n",
    "    out = pd.concat([sizes, comm_cent, spatial], axis=1).reset_index()\n",
    "    return out\n",
    "\n",
    "def spatial_global_stat(comm_spatial: pd.DataFrame) -> float:\n",
    "    vals = comm_spatial[\"spatial_median_km\"].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    return float(vals.median()) if len(vals) else float(\"nan\")\n",
    "\n",
    "def spatial_baseline_zscore(user_centroids: pd.DataFrame,\n",
    "                           comm_df: pd.DataFrame,\n",
    "                           n_shuffles: int = 150,\n",
    "                           seed: int = 42,\n",
    "                           min_comm_size: int = 20,\n",
    "                           sample_users: int = 200000) -> dict:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    tmp = user_centroids.merge(comm_df[[\"user_id\",\"community_id\"]], on=\"user_id\", how=\"inner\")\n",
    "    if len(tmp) == 0:\n",
    "        return dict(observed=np.nan, mu=np.nan, sd=np.nan, z=np.nan, used_users=0, min_comm_size_used=None)\n",
    "\n",
    "    # filter by comm size\n",
    "    comm_sizes = tmp[\"community_id\"].value_counts()\n",
    "    keep = comm_sizes[comm_sizes >= min_comm_size].index.values\n",
    "    if len(keep) == 0:\n",
    "        # relax\n",
    "        min_comm_size = max(5, int(comm_sizes.max())) if len(comm_sizes) else 2\n",
    "        keep = comm_sizes[comm_sizes >= min_comm_size].index.values\n",
    "\n",
    "    tmp = tmp[tmp[\"community_id\"].isin(keep)].copy()\n",
    "    if len(tmp) > sample_users:\n",
    "        tmp = tmp.sample(n=sample_users, random_state=seed)\n",
    "\n",
    "    obs_comm = compute_spatial_comm_metrics(tmp[[\"user_id\",\"lat\",\"lon\"]], tmp[[\"user_id\",\"community_id\"]])\n",
    "    observed = spatial_global_stat(obs_comm)\n",
    "\n",
    "    # size-preserving shuffle\n",
    "    comm_sizes2 = tmp[\"community_id\"].value_counts().sort_index()\n",
    "    comm_ids = comm_sizes2.index.values.astype(int)\n",
    "    sizes = comm_sizes2.values.astype(int)\n",
    "    labels = np.repeat(comm_ids, sizes)\n",
    "\n",
    "    user_lat = tmp[\"lat\"].values\n",
    "    user_lon = tmp[\"lon\"].values\n",
    "\n",
    "    baseline_vals = []\n",
    "    for _ in range(n_shuffles):\n",
    "        rng.shuffle(labels)\n",
    "        tcoords = pd.DataFrame({\"community_id\": labels, \"lat\": user_lat, \"lon\": user_lon})\n",
    "        cc = tcoords.groupby(\"community_id\")[[\"lat\",\"lon\"]].mean().rename(columns={\"lat\":\"comm_lat\",\"lon\":\"comm_lon\"})\n",
    "        comm_lat = pd.Series(labels).map(cc[\"comm_lat\"]).values\n",
    "        comm_lon = pd.Series(labels).map(cc[\"comm_lon\"]).values\n",
    "        dist = haversine_km(user_lat, user_lon, comm_lat, comm_lon)\n",
    "        tcoords[\"dist_km\"] = dist\n",
    "        cm = tcoords.groupby(\"community_id\")[\"dist_km\"].median()\n",
    "        baseline_vals.append(float(cm.median()))\n",
    "\n",
    "    baseline_vals = np.array(baseline_vals, dtype=float)\n",
    "    mu = float(np.mean(baseline_vals))\n",
    "    sd = float(np.std(baseline_vals) + 1e-12)\n",
    "    z = (observed - mu) / sd\n",
    "\n",
    "    return dict(observed=observed, mu=mu, sd=sd, z=float(z),\n",
    "                used_users=int(len(tmp)), min_comm_size_used=int(min_comm_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89d7c37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def _take_first_col_as_series(df: pd.DataFrame, col: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    If df has duplicate column labels, df[col] returns a DataFrame.\n",
    "    This helper always returns a 1D Series (take the first occurrence).\n",
    "    \"\"\"\n",
    "    x = df.loc[:, col]\n",
    "    if isinstance(x, pd.DataFrame):\n",
    "        return x.iloc[:, 0]\n",
    "    return x\n",
    "\n",
    "def try_compute_modularity_igraph(edges_df: pd.DataFrame, comm_df: pd.DataFrame) -> float | None:\n",
    "    try:\n",
    "        import igraph as ig\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    # robust u/v extraction\n",
    "    u = _take_first_col_as_series(edges_df, \"u\").astype(str)\n",
    "    v = _take_first_col_as_series(edges_df, \"v\").astype(str)\n",
    "\n",
    "    nodes = pd.Index(pd.unique(pd.concat([u, v], ignore_index=True)))\n",
    "    node2idx = {node: i for i, node in enumerate(nodes.tolist())}\n",
    "    edgelist = list(zip(u.map(node2idx), v.map(node2idx)))\n",
    "\n",
    "    g = ig.Graph(n=len(nodes), edges=edgelist, directed=False)\n",
    "    g.simplify(multiple=True, loops=True)\n",
    "\n",
    "    comm_map = comm_df.set_index(\"user_id\")[\"community_id\"].to_dict()\n",
    "    membership = [comm_map.get(node, -1) for node in nodes.tolist()]\n",
    "    valid_idx = [i for i, m in enumerate(membership) if m != -1]\n",
    "\n",
    "    if len(valid_idx) != len(membership):\n",
    "        g = g.induced_subgraph(valid_idx)\n",
    "        membership = [membership[i] for i in valid_idx]\n",
    "\n",
    "    uniq = {c: i for i, c in enumerate(sorted(set(membership)))}\n",
    "    membership = [uniq[c] for c in membership]\n",
    "    return float(g.modularity(membership))\n",
    "\n",
    "\n",
    "def compute_metrics(bundle: dict, comm_all: pd.DataFrame, knn_edges: pd.DataFrame,\n",
    "                    baseline_shuffles: int = 150, seed: int = 42) -> dict:\n",
    "    ds = bundle[\"dataset\"]\n",
    "\n",
    "    # coverage original vs augmented\n",
    "    cov0 = bundle[\"coverage\"]\n",
    "    users_total = bundle[\"users_final\"][\"user_id\"].nunique()\n",
    "    cov1 = {\n",
    "        \"users_total\": int(users_total),\n",
    "        \"users_labeled\": int(comm_all[\"user_id\"].nunique()),\n",
    "        \"label_coverage\": float(comm_all[\"user_id\"].nunique() / max(1, users_total)),\n",
    "    }\n",
    "\n",
    "    labeled_set = set(comm_all[\"user_id\"].astype(str))\n",
    "\n",
    "    # ---- Structural-A: friendship graph ----\n",
    "    edgesA = bundle[\"edges_final\"][[\"u\", \"v\"]].copy()\n",
    "    edgesA[\"u\"] = edgesA[\"u\"].astype(str)\n",
    "    edgesA[\"v\"] = edgesA[\"v\"].astype(str)\n",
    "    edgesA = edgesA[edgesA[\"u\"].isin(labeled_set) & edgesA[\"v\"].isin(labeled_set)].copy()\n",
    "\n",
    "    modularity_A = try_compute_modularity_igraph(edgesA, comm_all) if len(edgesA) else None\n",
    "    comm_struct_A = compute_structural_comm_metrics(edgesA, comm_all) if len(edgesA) else pd.DataFrame()\n",
    "    sum_A = summarize_structural(comm_struct_A, modularity_A)\n",
    "\n",
    "    # ---- Structural-B: kNN graph (from X_users) ----\n",
    "    edgesB = knn_edges[[\"u\", \"v\"]].copy()\n",
    "    edgesB[\"u\"] = edgesB[\"u\"].astype(str)\n",
    "    edgesB[\"v\"] = edgesB[\"v\"].astype(str)\n",
    "    edgesB = edgesB[edgesB[\"u\"].isin(labeled_set) & edgesB[\"v\"].isin(labeled_set)].copy()\n",
    "\n",
    "    modularity_B = try_compute_modularity_igraph(edgesB, comm_all) if len(edgesB) else None\n",
    "    comm_struct_B = compute_structural_comm_metrics(edgesB, comm_all) if len(edgesB) else pd.DataFrame()\n",
    "    sum_B = summarize_structural(comm_struct_B, modularity_B)\n",
    "\n",
    "    # ---- Spatial (augmented labels) ----\n",
    "    chk = bundle[\"checkins_final\"][[\"user_id\", \"lat\", \"lon\"]].copy()\n",
    "    chk[\"user_id\"] = chk[\"user_id\"].astype(str)\n",
    "    chk = chk[chk[\"user_id\"].isin(labeled_set)].copy()\n",
    "\n",
    "    if len(chk):\n",
    "        user_cent = compute_user_centroids(chk)\n",
    "        comm_spatial = compute_spatial_comm_metrics(user_cent, comm_all)\n",
    "        base = spatial_baseline_zscore(\n",
    "            user_centroids=user_cent,\n",
    "            comm_df=comm_all,\n",
    "            n_shuffles=baseline_shuffles,\n",
    "            seed=seed,\n",
    "            min_comm_size=20,\n",
    "            sample_users=200000,\n",
    "        )\n",
    "        spatial_median_km = float(base[\"observed\"])\n",
    "        baseline_mean_km = float(base[\"mu\"])\n",
    "        baseline_std_km = float(base[\"sd\"])\n",
    "        spatial_z = float(base[\"z\"])\n",
    "        baseline_sample_users = int(base[\"used_users\"])\n",
    "        baseline_min_comm_size_used = base[\"min_comm_size_used\"]\n",
    "    else:\n",
    "        comm_spatial = pd.DataFrame()\n",
    "        spatial_median_km = float(\"nan\")\n",
    "        baseline_mean_km = float(\"nan\")\n",
    "        baseline_std_km = float(\"nan\")\n",
    "        spatial_z = float(\"nan\")\n",
    "        baseline_sample_users = 0\n",
    "        baseline_min_comm_size_used = None\n",
    "\n",
    "    # community size stats\n",
    "    comm_sizes = comm_all[\"community_id\"].value_counts()\n",
    "    largest_comm = int(comm_sizes.max()) if len(comm_sizes) else 0\n",
    "    median_comm = float(comm_sizes.median()) if len(comm_sizes) else float(\"nan\")\n",
    "\n",
    "    return dict(\n",
    "        dataset=ds,\n",
    "        coverage_original=cov0,\n",
    "        coverage_augmented=cov1,\n",
    "\n",
    "        communities_augmented=int(comm_all[\"community_id\"].nunique()),\n",
    "        largest_comm=largest_comm,\n",
    "        median_comm=median_comm,\n",
    "\n",
    "        # Structural-A\n",
    "        structuralA_modularity=sum_A[\"modularity\"],\n",
    "        structuralA_conductance_median=sum_A[\"conductance_median\"],\n",
    "        structuralA_conductance_iqr=sum_A[\"conductance_iqr\"],\n",
    "        structuralA_density_median=sum_A[\"density_median\"],\n",
    "        structuralA_density_iqr=sum_A[\"density_iqr\"],\n",
    "\n",
    "        # Structural-B\n",
    "        structuralB_modularity=sum_B[\"modularity\"],\n",
    "        structuralB_conductance_median=sum_B[\"conductance_median\"],\n",
    "        structuralB_conductance_iqr=sum_B[\"conductance_iqr\"],\n",
    "        structuralB_density_median=sum_B[\"density_median\"],\n",
    "        structuralB_density_iqr=sum_B[\"density_iqr\"],\n",
    "\n",
    "        # Spatial\n",
    "        spatial_median_km=spatial_median_km,\n",
    "        baseline_mean_km=baseline_mean_km,\n",
    "        baseline_std_km=baseline_std_km,\n",
    "        spatial_z=spatial_z,\n",
    "        baseline_shuffles=int(baseline_shuffles),\n",
    "        baseline_sample_users=baseline_sample_users,\n",
    "        baseline_min_comm_size_used=baseline_min_comm_size_used,\n",
    "\n",
    "        # details\n",
    "        comm_all=comm_all,\n",
    "        comm_struct_A=comm_struct_A,\n",
    "        comm_struct_B=comm_struct_B,\n",
    "        comm_spatial=comm_spatial,\n",
    "        run_config=bundle.get(\"run_config\"),\n",
    "        metrics_global_cache=bundle.get(\"metrics_global_cache\"),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "948c6f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] metrics_A keys: ['dataset', 'coverage_original', 'coverage_augmented', 'communities_augmented', 'largest_comm', 'median_comm', 'structuralA_modularity', 'structuralA_conductance_median', 'structuralA_conductance_iqr', 'structuralA_density_median'] ...\n",
      "[OK] metrics_B keys: ['dataset', 'coverage_original', 'coverage_augmented', 'communities_augmented', 'largest_comm', 'median_comm', 'structuralA_modularity', 'structuralA_conductance_median', 'structuralA_conductance_iqr', 'structuralA_density_median'] ...\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Guard: ensure metrics_A / metrics_B exist before building table1\n",
    "\n",
    "need = []\n",
    "for var in [\"metrics_A\", \"metrics_B\"]:\n",
    "    if var not in globals() or globals()[var] is None:\n",
    "        need.append(var)\n",
    "\n",
    "if need:\n",
    "    print(\"[INFO] Missing:\", need, \"-> recomputing metrics now...\")\n",
    "\n",
    "    # sanity checks (must exist from previous cells)\n",
    "    required = [\"data_A\", \"data_B\", \"commA_all\", \"commB_all\", \"knnA\", \"knnB\"]\n",
    "    missing2 = [x for x in required if x not in globals()]\n",
    "    if missing2:\n",
    "        raise RuntimeError(f\"Cannot compute metrics because missing objects: {missing2}. \"\n",
    "                           f\"Run earlier cells that create: {missing2}\")\n",
    "\n",
    "    metrics_A = compute_metrics(data_A, commA_all, knnA)\n",
    "    metrics_B = compute_metrics(data_B, commB_all, knnB)\n",
    "\n",
    "print(\"[OK] metrics_A keys:\", list(metrics_A.keys())[:10], \"...\")\n",
    "print(\"[OK] metrics_B keys:\", list(metrics_B.keys())[:10], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b0d1766c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>users_total</th>\n",
       "      <th>users_labeled_original</th>\n",
       "      <th>label_coverage_original</th>\n",
       "      <th>edges_total</th>\n",
       "      <th>edges_labeled_original</th>\n",
       "      <th>edge_labeled_ratio_original</th>\n",
       "      <th>checkins_total</th>\n",
       "      <th>checkins_labeled_original</th>\n",
       "      <th>checkins_labeled_ratio_original</th>\n",
       "      <th>...</th>\n",
       "      <th>B_conductance_iqr</th>\n",
       "      <th>B_density_median</th>\n",
       "      <th>B_density_iqr</th>\n",
       "      <th>spatial_median_km</th>\n",
       "      <th>baseline_mean_km</th>\n",
       "      <th>baseline_std_km</th>\n",
       "      <th>spatial_z</th>\n",
       "      <th>baseline_shuffles</th>\n",
       "      <th>baseline_sample_users</th>\n",
       "      <th>baseline_min_comm_size_used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brightkite</td>\n",
       "      <td>15092</td>\n",
       "      <td>5346</td>\n",
       "      <td>0.354227</td>\n",
       "      <td>116506</td>\n",
       "      <td>49536</td>\n",
       "      <td>0.425180</td>\n",
       "      <td>3656191</td>\n",
       "      <td>2068469</td>\n",
       "      <td>0.565744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>2776.853813</td>\n",
       "      <td>9190.566185</td>\n",
       "      <td>227.701613</td>\n",
       "      <td>-28.167180</td>\n",
       "      <td>150</td>\n",
       "      <td>1291</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lbsn2vec</td>\n",
       "      <td>47389</td>\n",
       "      <td>771</td>\n",
       "      <td>0.016270</td>\n",
       "      <td>279816</td>\n",
       "      <td>1227</td>\n",
       "      <td>0.004385</td>\n",
       "      <td>10328914</td>\n",
       "      <td>160359</td>\n",
       "      <td>0.015525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115058</td>\n",
       "      <td>0.036635</td>\n",
       "      <td>0.034977</td>\n",
       "      <td>1929.754706</td>\n",
       "      <td>9523.219344</td>\n",
       "      <td>22.845851</td>\n",
       "      <td>-332.378279</td>\n",
       "      <td>150</td>\n",
       "      <td>45476</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      dataset  users_total  users_labeled_original  label_coverage_original  \\\n",
       "0  brightkite        15092                    5346                 0.354227   \n",
       "1    lbsn2vec        47389                     771                 0.016270   \n",
       "\n",
       "   edges_total  edges_labeled_original  edge_labeled_ratio_original  \\\n",
       "0       116506                   49536                     0.425180   \n",
       "1       279816                    1227                     0.004385   \n",
       "\n",
       "   checkins_total  checkins_labeled_original  checkins_labeled_ratio_original  \\\n",
       "0         3656191                    2068469                         0.565744   \n",
       "1        10328914                     160359                         0.015525   \n",
       "\n",
       "   ...  B_conductance_iqr  B_density_median  B_density_iqr  spatial_median_km  \\\n",
       "0  ...           0.153846          0.500000       0.733333        2776.853813   \n",
       "1  ...           0.115058          0.036635       0.034977        1929.754706   \n",
       "\n",
       "   baseline_mean_km  baseline_std_km   spatial_z  baseline_shuffles  \\\n",
       "0       9190.566185       227.701613  -28.167180                150   \n",
       "1       9523.219344        22.845851 -332.378279                150   \n",
       "\n",
       "   baseline_sample_users  baseline_min_comm_size_used  \n",
       "0                   1291                           20  \n",
       "1                  45476                           20  \n",
       "\n",
       "[2 rows x 32 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "def build_table1_row(m: dict) -> dict:\n",
    "    cov0 = m[\"coverage_original\"]\n",
    "    cov1 = m[\"coverage_augmented\"]\n",
    "\n",
    "    return {\n",
    "        \"dataset\": m[\"dataset\"],\n",
    "\n",
    "        # original coverage\n",
    "        \"users_total\": cov0[\"users_total\"],\n",
    "        \"users_labeled_original\": cov0[\"users_labeled\"],\n",
    "        \"label_coverage_original\": cov0[\"label_coverage\"],\n",
    "        \"edges_total\": cov0[\"edges_total\"],\n",
    "        \"edges_labeled_original\": cov0[\"edges_labeled\"],\n",
    "        \"edge_labeled_ratio_original\": cov0[\"edge_labeled_ratio\"],\n",
    "        \"checkins_total\": cov0[\"checkins_total\"],\n",
    "        \"checkins_labeled_original\": cov0[\"checkins_labeled\"],\n",
    "        \"checkins_labeled_ratio_original\": cov0[\"checkins_labeled_ratio\"],\n",
    "\n",
    "        # augmented coverage\n",
    "        \"users_labeled_aug\": cov1[\"users_labeled\"],\n",
    "        \"label_coverage_aug\": cov1[\"label_coverage\"],\n",
    "\n",
    "        \"communities_aug\": m[\"communities_augmented\"],\n",
    "        \"largest_comm\": m[\"largest_comm\"],\n",
    "        \"median_comm\": m[\"median_comm\"],\n",
    "\n",
    "        # Structural-A\n",
    "        \"A_modularity\": m[\"structuralA_modularity\"],\n",
    "        \"A_conductance_median\": m[\"structuralA_conductance_median\"],\n",
    "        \"A_conductance_iqr\": m[\"structuralA_conductance_iqr\"],\n",
    "        \"A_density_median\": m[\"structuralA_density_median\"],\n",
    "        \"A_density_iqr\": m[\"structuralA_density_iqr\"],\n",
    "\n",
    "        # Structural-B\n",
    "        \"B_modularity\": m[\"structuralB_modularity\"],\n",
    "        \"B_conductance_median\": m[\"structuralB_conductance_median\"],\n",
    "        \"B_conductance_iqr\": m[\"structuralB_conductance_iqr\"],\n",
    "        \"B_density_median\": m[\"structuralB_density_median\"],\n",
    "        \"B_density_iqr\": m[\"structuralB_density_iqr\"],\n",
    "\n",
    "        # Spatial\n",
    "        \"spatial_median_km\": m[\"spatial_median_km\"],\n",
    "        \"baseline_mean_km\": m[\"baseline_mean_km\"],\n",
    "        \"baseline_std_km\": m[\"baseline_std_km\"],\n",
    "        \"spatial_z\": m[\"spatial_z\"],\n",
    "        \"baseline_shuffles\": m[\"baseline_shuffles\"],\n",
    "        \"baseline_sample_users\": m[\"baseline_sample_users\"],\n",
    "        \"baseline_min_comm_size_used\": m[\"baseline_min_comm_size_used\"],\n",
    "    }\n",
    "\n",
    "table1 = pd.DataFrame([build_table1_row(metrics_A), build_table1_row(metrics_B)])\n",
    "table1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89d0a282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>community_id</th>\n",
       "      <th>n_nodes</th>\n",
       "      <th>spatial_median_km</th>\n",
       "      <th>comm_lat</th>\n",
       "      <th>comm_lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>brightkite</td>\n",
       "      <td>1</td>\n",
       "      <td>136</td>\n",
       "      <td>6088.049378</td>\n",
       "      <td>29.947662</td>\n",
       "      <td>-27.186163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brightkite</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>4710.477367</td>\n",
       "      <td>38.353088</td>\n",
       "      <td>-48.468878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>brightkite</td>\n",
       "      <td>2879</td>\n",
       "      <td>18</td>\n",
       "      <td>80.082683</td>\n",
       "      <td>35.695438</td>\n",
       "      <td>138.845531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>brightkite</td>\n",
       "      <td>3055</td>\n",
       "      <td>37</td>\n",
       "      <td>147.932686</td>\n",
       "      <td>35.980826</td>\n",
       "      <td>138.091714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lbsn2vec</td>\n",
       "      <td>10767</td>\n",
       "      <td>533</td>\n",
       "      <td>1547.397299</td>\n",
       "      <td>14.393729</td>\n",
       "      <td>109.566631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lbsn2vec</td>\n",
       "      <td>11</td>\n",
       "      <td>466</td>\n",
       "      <td>3644.573385</td>\n",
       "      <td>-8.538571</td>\n",
       "      <td>-40.065792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lbsn2vec</td>\n",
       "      <td>1684</td>\n",
       "      <td>208</td>\n",
       "      <td>181.077759</td>\n",
       "      <td>40.988958</td>\n",
       "      <td>28.405977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lbsn2vec</td>\n",
       "      <td>1740</td>\n",
       "      <td>212</td>\n",
       "      <td>333.133407</td>\n",
       "      <td>41.007806</td>\n",
       "      <td>28.751232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dataset  community_id  n_nodes  spatial_median_km   comm_lat    comm_lon\n",
       "0  brightkite             1      136        6088.049378  29.947662  -27.186163\n",
       "1  brightkite             0      118        4710.477367  38.353088  -48.468878\n",
       "2  brightkite          2879       18          80.082683  35.695438  138.845531\n",
       "3  brightkite          3055       37         147.932686  35.980826  138.091714\n",
       "4    lbsn2vec         10767      533        1547.397299  14.393729  109.566631\n",
       "5    lbsn2vec            11      466        3644.573385  -8.538571  -40.065792\n",
       "6    lbsn2vec          1684      208         181.077759  40.988958   28.405977\n",
       "7    lbsn2vec          1740      212         333.133407  41.007806   28.751232"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "def build_case_studies(m: dict) -> pd.DataFrame:\n",
    "    comm_spatial = m[\"comm_spatial\"].copy()\n",
    "    if len(comm_spatial) == 0:\n",
    "        return pd.DataFrame(columns=[\"dataset\",\"community_id\",\"n_nodes\",\"spatial_median_km\",\"comm_lat\",\"comm_lon\"])\n",
    "\n",
    "    comm_spatial[\"dataset\"] = m[\"dataset\"]\n",
    "\n",
    "    # top 2 by size\n",
    "    top_size = comm_spatial.sort_values(\"n_nodes\", ascending=False).head(2)\n",
    "\n",
    "    # top 2 by spatial compactness (exclude too small)\n",
    "    min_size = max(10, int(np.nanmedian(comm_spatial[\"n_nodes\"].values)))\n",
    "    cand = comm_spatial[comm_spatial[\"n_nodes\"] >= min_size].copy()\n",
    "    if len(cand) == 0:\n",
    "        cand = comm_spatial.copy()\n",
    "\n",
    "    top_spatial = cand.sort_values(\"spatial_median_km\", ascending=True).head(2)\n",
    "\n",
    "    picked = pd.concat([top_size, top_spatial], ignore_index=True)\n",
    "    picked = picked.drop_duplicates(subset=[\"community_id\"]).reset_index(drop=True)\n",
    "\n",
    "    cols = [\"dataset\",\"community_id\",\"n_nodes\",\"spatial_median_km\",\"comm_lat\",\"comm_lon\"]\n",
    "    return picked[cols]\n",
    "\n",
    "table2 = pd.concat([build_case_studies(metrics_A), build_case_studies(metrics_B)], ignore_index=True)\n",
    "table2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7bfd38ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structural-A (friendship graph) â€” social ties cohesion\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>modularity</th>\n",
       "      <th>conductance_median</th>\n",
       "      <th>conductance_iqr</th>\n",
       "      <th>density_median</th>\n",
       "      <th>density_iqr</th>\n",
       "      <th>labels_used</th>\n",
       "      <th>social_cohesion_score</th>\n",
       "      <th>winner_by_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lbsn2vec</td>\n",
       "      <td>0.012983</td>\n",
       "      <td>0.995381</td>\n",
       "      <td>0.011864</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.001932</td>\n",
       "      <td>original/augmented (depends on your compute_me...</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>lbsn2vec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brightkite</td>\n",
       "      <td>0.003149</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>original/augmented (depends on your compute_me...</td>\n",
       "      <td>-1.414214</td>\n",
       "      <td>lbsn2vec</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dataset  modularity  conductance_median  conductance_iqr  \\\n",
       "0    lbsn2vec    0.012983            0.995381         0.011864   \n",
       "1  brightkite    0.003149            1.000000         0.000000   \n",
       "\n",
       "   density_median  density_iqr  \\\n",
       "0        0.000699     0.001932   \n",
       "1        0.000000     0.000000   \n",
       "\n",
       "                                         labels_used  social_cohesion_score  \\\n",
       "0  original/augmented (depends on your compute_me...               1.414214   \n",
       "1  original/augmented (depends on your compute_me...              -1.414214   \n",
       "\n",
       "  winner_by_score  \n",
       "0        lbsn2vec  \n",
       "1        lbsn2vec  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Structural-B (kNN/embedding graph) â€” behavioral similarity cohesion\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>modularity</th>\n",
       "      <th>conductance_median</th>\n",
       "      <th>conductance_iqr</th>\n",
       "      <th>density_median</th>\n",
       "      <th>density_iqr</th>\n",
       "      <th>labels_used</th>\n",
       "      <th>social_cohesion_score</th>\n",
       "      <th>winner_by_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lbsn2vec</td>\n",
       "      <td>0.270016</td>\n",
       "      <td>0.800289</td>\n",
       "      <td>0.115058</td>\n",
       "      <td>0.036635</td>\n",
       "      <td>0.034977</td>\n",
       "      <td>augmented if Leiden-on-kNN-full was used</td>\n",
       "      <td>1.414214</td>\n",
       "      <td>lbsn2vec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brightkite</td>\n",
       "      <td>0.142777</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>augmented if Leiden-on-kNN-full was used</td>\n",
       "      <td>-1.414214</td>\n",
       "      <td>lbsn2vec</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      dataset  modularity  conductance_median  conductance_iqr  \\\n",
       "0    lbsn2vec    0.270016            0.800289         0.115058   \n",
       "1  brightkite    0.142777            0.920000         0.153846   \n",
       "\n",
       "   density_median  density_iqr                               labels_used  \\\n",
       "0        0.036635     0.034977  augmented if Leiden-on-kNN-full was used   \n",
       "1        0.500000     0.733333  augmented if Leiden-on-kNN-full was used   \n",
       "\n",
       "   social_cohesion_score winner_by_score  \n",
       "0               1.414214        lbsn2vec  \n",
       "1              -1.414214        lbsn2vec  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%\n",
    "def build_structural_tables(metrics_A: dict, metrics_B: dict) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    rows_A = []\n",
    "    rows_B = []\n",
    "\n",
    "    for m in [metrics_A, metrics_B]:\n",
    "        ds = m[\"dataset\"]\n",
    "\n",
    "        # Structural-A: social graph (friendship edges_final)\n",
    "        rows_A.append({\n",
    "            \"dataset\": ds,\n",
    "            \"modularity\": m.get(\"A_modularity\", m.get(\"structuralA_modularity\")),\n",
    "            \"conductance_median\": m.get(\"A_conductance_median\", m.get(\"structuralA_conductance_median\")),\n",
    "            \"conductance_iqr\": m.get(\"A_conductance_iqr\", m.get(\"structuralA_conductance_iqr\")),\n",
    "            \"density_median\": m.get(\"A_density_median\", m.get(\"structuralA_density_median\")),\n",
    "            \"density_iqr\": m.get(\"A_density_iqr\", m.get(\"structuralA_density_iqr\")),\n",
    "            \"labels_used\": \"original/augmented (depends on your compute_metrics)\",\n",
    "        })\n",
    "\n",
    "        # Structural-B: embedding graph (kNN graph)\n",
    "        rows_B.append({\n",
    "            \"dataset\": ds,\n",
    "            \"modularity\": m.get(\"B_modularity\", m.get(\"structuralB_modularity\")),\n",
    "            \"conductance_median\": m.get(\"B_conductance_median\", m.get(\"structuralB_conductance_median\")),\n",
    "            \"conductance_iqr\": m.get(\"B_conductance_iqr\", m.get(\"structuralB_conductance_iqr\")),\n",
    "            \"density_median\": m.get(\"B_density_median\", m.get(\"structuralB_density_median\")),\n",
    "            \"density_iqr\": m.get(\"B_density_iqr\", m.get(\"structuralB_density_iqr\")),\n",
    "            \"labels_used\": \"augmented if Leiden-on-kNN-full was used\",\n",
    "        })\n",
    "\n",
    "    structural_A = pd.DataFrame(rows_A)\n",
    "    structural_B = pd.DataFrame(rows_B)\n",
    "\n",
    "    # helper: highlight â€œÄ‘áº­m social hÆ¡nâ€ = higher modularity + lower conductance\n",
    "    def _winner(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        # convert to numeric safely\n",
    "        for c in [\"modularity\",\"conductance_median\",\"density_median\",\"conductance_iqr\",\"density_iqr\"]:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "        # score: z(modularity) - z(conductance_median)\n",
    "        z_mod = (df[\"modularity\"] - df[\"modularity\"].mean()) / (df[\"modularity\"].std() + 1e-12)\n",
    "        z_con = (df[\"conductance_median\"] - df[\"conductance_median\"].mean()) / (df[\"conductance_median\"].std() + 1e-12)\n",
    "        df[\"social_cohesion_score\"] = z_mod - z_con\n",
    "\n",
    "        best = df.loc[df[\"social_cohesion_score\"].idxmax(), \"dataset\"]\n",
    "        df[\"winner_by_score\"] = best\n",
    "        return df.sort_values(\"social_cohesion_score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    return _winner(structural_A), _winner(structural_B)\n",
    "\n",
    "structural_A, structural_B = build_structural_tables(metrics_A, metrics_B)\n",
    "\n",
    "print(\"Structural-A (friendship graph) â€” social ties cohesion\")\n",
    "display(structural_A)\n",
    "\n",
    "print(\"\\nStructural-B (kNN/embedding graph) â€” behavioral similarity cohesion\")\n",
    "display(structural_B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77026c4b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73171b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>structural_view</th>\n",
       "      <th>brightkite_modularity</th>\n",
       "      <th>lbsn2vec_modularity</th>\n",
       "      <th>winner_modularity(higher=better)</th>\n",
       "      <th>brightkite_conductance_median</th>\n",
       "      <th>lbsn2vec_conductance_median</th>\n",
       "      <th>winner_conductance(lower=better)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Structural-A (friendship graph)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Structural-B (mutual-kNN embedding graph)</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             structural_view brightkite_modularity  \\\n",
       "0            Structural-A (friendship graph)                  None   \n",
       "1  Structural-B (mutual-kNN embedding graph)                  None   \n",
       "\n",
       "  lbsn2vec_modularity winner_modularity(higher=better)  \\\n",
       "0                None                             None   \n",
       "1                None                             None   \n",
       "\n",
       "  brightkite_conductance_median lbsn2vec_conductance_median  \\\n",
       "0                          None                        None   \n",
       "1                          None                        None   \n",
       "\n",
       "  winner_conductance(lower=better)  \n",
       "0                             None  \n",
       "1                             None  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /mnt/d/community-detection/data/processed/_runs/20251218_013727/step8_compare/step8_structural_compare.csv\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _winner_higher(a, b):\n",
    "    if pd.isna(a) or pd.isna(b): \n",
    "        return None\n",
    "    return \"brightkite\" if a > b else (\"lbsn2vec\" if b > a else \"tie\")\n",
    "\n",
    "def _winner_lower(a, b):\n",
    "    if pd.isna(a) or pd.isna(b): \n",
    "        return None\n",
    "    return \"brightkite\" if a < b else (\"lbsn2vec\" if b < a else \"tie\")\n",
    "\n",
    "def build_structural_compare_table(metrics_A: dict, metrics_B: dict) -> pd.DataFrame:\n",
    "    rows = []\n",
    "\n",
    "    # Structural-A (friendship graph)\n",
    "    A_mod_A = metrics_A.get(\"A_modularity\")\n",
    "    A_mod_B = metrics_B.get(\"A_modularity\")\n",
    "    A_cond_A = metrics_A.get(\"A_conductance_median\")\n",
    "    A_cond_B = metrics_B.get(\"A_conductance_median\")\n",
    "\n",
    "    rows.append({\n",
    "        \"structural_view\": \"Structural-A (friendship graph)\",\n",
    "        \"brightkite_modularity\": A_mod_A,\n",
    "        \"lbsn2vec_modularity\": A_mod_B,\n",
    "        \"winner_modularity(higher=better)\": _winner_higher(A_mod_A, A_mod_B),\n",
    "        \"brightkite_conductance_median\": A_cond_A,\n",
    "        \"lbsn2vec_conductance_median\": A_cond_B,\n",
    "        \"winner_conductance(lower=better)\": _winner_lower(A_cond_A, A_cond_B),\n",
    "    })\n",
    "\n",
    "    # Structural-B (mutual-kNN embedding graph)\n",
    "    B_mod_A = metrics_A.get(\"B_modularity\")\n",
    "    B_mod_B = metrics_B.get(\"B_modularity\")\n",
    "    B_cond_A = metrics_A.get(\"B_conductance_median\")\n",
    "    B_cond_B = metrics_B.get(\"B_conductance_median\")\n",
    "\n",
    "    rows.append({\n",
    "        \"structural_view\": \"Structural-B (mutual-kNN embedding graph)\",\n",
    "        \"brightkite_modularity\": B_mod_A,\n",
    "        \"lbsn2vec_modularity\": B_mod_B,\n",
    "        \"winner_modularity(higher=better)\": _winner_higher(B_mod_A, B_mod_B),\n",
    "        \"brightkite_conductance_median\": B_cond_A,\n",
    "        \"lbsn2vec_conductance_median\": B_cond_B,\n",
    "        \"winner_conductance(lower=better)\": _winner_lower(B_cond_A, B_cond_B),\n",
    "    })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# --- build now (requires metrics_A, metrics_B already computed) ---\n",
    "struct_table = build_structural_compare_table(metrics_A, metrics_B)\n",
    "display(struct_table)\n",
    "\n",
    "# --- save ---\n",
    "(struct_table\n",
    " .to_csv(STEP8_OUT / \"step8_structural_compare.csv\", index=False))\n",
    "(struct_table\n",
    " .to_parquet(STEP8_OUT / \"step8_structural_compare.parquet\", index=False))\n",
    "print(\"Saved:\", STEP8_OUT / \"step8_structural_compare.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
